{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agroimpacts/adleo/blob/main/assignments/assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmaQLda6ksLG"
      },
      "source": [
        "# Assignment 5 - Fine-tuning and Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 50/50 points on A 5</font>**"
      ],
      "metadata": {
        "id": "AxMMoBLgs2qG"
      }
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "eF-eyCODu2PR"
      },
      "source": [
        "---\n",
        "toc: true\n",
        "toc-depth: 6\n",
        "number-sections: true\n",
        "number-depth: 6\n",
        "execute:\n",
        "  eval: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdWJ1uMOkzku"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Please follow the [instructions](assignments-setup.qmd) for setting up, completing, and submitting your assignments.\n",
        "\n",
        "<font color='red'>\n",
        "\n",
        "**Important Note**:\n",
        "\n",
        "- If you are using ChatGPT or any other AI based services to correct your writings or errors in code, you are required to provide a citation link.\n",
        "- For answers to theoretical questions you should provide references for the source of your answers.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding assignments\n",
        "The coding work here focus on continuing and complete our model pipeline, including assignments focused on:\n",
        "\n",
        "\n",
        "1.   Adding the imagery onto which we are predicting into the pipeline;\n",
        "\n",
        "2.   Adding code for running predictions;\n",
        "\n",
        "3.   Fine-tuning the previously trained model\n",
        "\n",
        "The code to undertake this is mostly provided in the sections that follow under Key Code, including a U-Net model (note: you can replace that with the one you developed in your previous assignments, if you prefer).\n",
        "\n",
        "The specific assignments are below the Key Code section."
      ],
      "metadata": {
        "id": "XzaJjDPcrCf_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Ks0Gqcu2PS"
      },
      "source": [
        "### Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csfWTTctkunl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f868b2a-cc30-4c54-9f9d-1a6348b17078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbhJLDyAkjZ5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRb1Rvq_lc7I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image\n",
        "\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6aYx27Vm44Z"
      },
      "source": [
        "### Key Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0ooH42u2PU"
      },
      "source": [
        "#### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aQwho5Mm-ra"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path, is_label=False, apply_normalization=False,\n",
        "              dtype=np.float32, verbose=False):\n",
        "    r\"\"\"\n",
        "    Open data using gdal, read it as an array and normalize it.\n",
        "    Arguments:\n",
        "            data_path (string) -- Full path including filename of the data\n",
        "                source we wish to load.\n",
        "            is_label (binary) -- If True then the layer is a ground truth\n",
        "                (category index) and if set to False the layer is a reflectance\n",
        "                band.\n",
        "            apply_normalization (binary) -- If true min/max normalization will b\n",
        "                e applied on each band.\n",
        "            dtype (np.dtype) -- Data type of the output image chips.\n",
        "            verbose (binary) -- if set to true, print a screen statement on the\n",
        "                loaded band.\n",
        "    Returns:\n",
        "            image -- Returns the loaded image as a 32-bit float numpy ndarray\n",
        "                with shape (height, width, num_bands).\n",
        "    \"\"\"\n",
        "\n",
        "    # Inform user of the file names being loaded from the Dataset.\n",
        "    if verbose:\n",
        "        print('loading file:{}'.format(data_path))\n",
        "\n",
        "    # open dataset using rasterio library.\n",
        "    with rasterio.open(data_path, \"r\") as src:\n",
        "\n",
        "        if is_label:\n",
        "            if src.count != 1:\n",
        "                raise ValueError(\"Expected Label to have exactly one channel.\")\n",
        "            img = src.read(1)\n",
        "\n",
        "        else:\n",
        "            if apply_normalization:\n",
        "                img = min_max_normalize_image(src.read())\n",
        "                img = img.astype(dtype)\n",
        "            else:\n",
        "                img = src.read()\n",
        "                img = img.astype(dtype)\n",
        "\n",
        "    return img\n",
        "\n",
        "#########################\n",
        "\n",
        "def get_meta_from_bounds(image, overlap=None):\n",
        "    with rasterio.open(image, \"r\") as src:\n",
        "        meta = src.meta\n",
        "\n",
        "    if overlap:\n",
        "        dst_width = src.width - 2 * overlap\n",
        "        dst_height = src.height - 2 * overlap\n",
        "        window = Window(overlap, overlap, dst_width, dst_height)\n",
        "        win_transform = src.window_transform(window)\n",
        "        meta.update({\n",
        "            'width': dst_width,\n",
        "            'height': dst_height,\n",
        "            'transform': win_transform,\n",
        "            'count': 1,\n",
        "            'dtype': 'int8'\n",
        "        })\n",
        "\n",
        "    return meta\n",
        "\n",
        "#########################\n",
        "\n",
        "def load_params(params_dir, model, freeze_params=None):\n",
        "    \"\"\"Load a set of parameters into a PyTorch model.\n",
        "    Args:\n",
        "        params_dir (str): Path to the \".pth\" or \".pt\" file containing the input\n",
        "            parameters.\n",
        "        model (pytorch nn object): Initialized model.\n",
        "        freeze_params (list[int] or None, optional): List of parameter indices\n",
        "            to freeze (i.e., set to require no gradients). Default is None,\n",
        "            which means that all parameters are trainable.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified file does not exist.\n",
        "\n",
        "    Example:\n",
        "        # Load parameters from file and freeze the first layer\n",
        "        load_params(\"params.pth\", freeze_params=[0])\n",
        "    \"\"\"\n",
        "    # Load input parameters from file\n",
        "    input_params = torch.load(params_dir)\n",
        "\n",
        "    # Filter out parameters that are not in the model\n",
        "    model_dict = model.state_dict()\n",
        "    if \"module\" in list(input_params.keys())[0]:\n",
        "        input_params_filter = {k[7:]: v.cpu() for k, v in input_params.items()\n",
        "                               if k[7:] in model_dict}\n",
        "    else:\n",
        "        input_params_filter = {k: v.cpu() for k, v in input_params.items()\n",
        "                               if k in model_dict}\n",
        "\n",
        "    # Update the model parameters with the input parameters\n",
        "    model_dict.update(input_params_filter)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # free some layers\n",
        "    if freeze_params is not None:\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            if i in freeze_params:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HgcYTNIloOC"
      },
      "source": [
        "#### Pre-processing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNQhJitu2PU"
      },
      "source": [
        "##### Input normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJybOajXl0dZ"
      },
      "outputs": [],
      "source": [
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is\n",
        "    \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVx9Lxvuu2PV"
      },
      "source": [
        "##### Image augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIkeH2ful8fW"
      },
      "outputs": [],
      "source": [
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either\n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "\n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "\n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "\n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "\n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle\n",
        "            (in degrees) respectively. If number of elements are bigger than 2,\n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "\n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2],\n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2],\n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FBL7jIZu2PV"
      },
      "source": [
        "##### Get center index of each smaller chip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9eOqlS75wjT"
      },
      "outputs": [],
      "source": [
        "def patch_center_index(cropping_ref, patch_size, overlap, usage,\n",
        "                       positive_class_threshold=None, verbose=True):\n",
        "    r\"\"\"\n",
        "    Generate index to divide the scene into small chips.\n",
        "    Each index marks the location of corresponding chip center.\n",
        "    Arguments:\n",
        "        cropping_ref (list) : Reference raster layers, to be used to generate\n",
        "            the index. In our case, it is study area binary mask and label mask.\n",
        "        patch_size (int) : Size of each clipped patches.\n",
        "        overlap (int) : amount of overlap between the extracted chips.\n",
        "        usage (str) : Either 'train', 'val'. Chipping strategy is different for\n",
        "            different usage.\n",
        "        positive_class_threshold (float) : A real value as a threshold for the\n",
        "            proportion of positive class to the total areal of the chip. Used to\n",
        "            decide if the chip should be considered as a positive chip in the\n",
        "            sampling process.\n",
        "    verbose (binary) : If set to True prints on screen the detailed list of\n",
        "            center coordinates of the sampled chips.\n",
        "    Returns:\n",
        "        proportional_patch_index : A list of index recording the center of\n",
        "        patches to extract from the input\n",
        "    \"\"\"\n",
        "\n",
        "    assert usage in [\"train\", \"validation\", \"inference\"]\n",
        "\n",
        "    if usage == \"inference\":\n",
        "        mask = cropping_ref\n",
        "    else:\n",
        "        mask, label = cropping_ref\n",
        "\n",
        "    half_size = patch_size // 2\n",
        "    step_size = patch_size - 2 * overlap\n",
        "\n",
        "    proportional_patch_index = []\n",
        "    non_proportional_patch_index = []\n",
        "    neg_patch_index = []\n",
        "\n",
        "    # Get the index of all the non-zero elements in the mask.\n",
        "    x = np.argwhere(mask)\n",
        "\n",
        "    # First col of x shows the row indices (height) of the mask layer\n",
        "    # (iterate over the y axis or latitude).\n",
        "    x_min = min(x[:, 0]) + half_size\n",
        "    x_max = max(x[:, 0]) - half_size\n",
        "    # Second col of x shows the column indices (width) of the mask layer\n",
        "    # (iterate over the x axis or longitude).\n",
        "    y_min = min(x[:, 1]) + half_size\n",
        "    y_max = max(x[:, 1]) - half_size\n",
        "\n",
        "    # Generate index for the center of each patch considering the proportion of\n",
        "    # each category falling into each patch.\n",
        "    for j in range(y_min, y_max + 1, step_size):\n",
        "\n",
        "        for i in range(x_min, x_max + 1, step_size):\n",
        "\n",
        "            # Split the mask and label layers into patches based on the index of\n",
        "            # the center of the patch\n",
        "            mask_ref = mask[i - half_size: i + half_size,\n",
        "                            j - half_size: j + half_size]\n",
        "            if usage != \"inference\":\n",
        "                label_ref = label[i - half_size: i + half_size,\n",
        "                                  j - half_size: j + half_size]\n",
        "\n",
        "            if (usage == \"train\") and mask_ref.all():\n",
        "\n",
        "                if label_ref.any() != 0:\n",
        "                    pond_ratio = np.sum(label_ref == 1) / label_ref.size\n",
        "                    if pond_ratio >= positive_class_threshold:\n",
        "                        proportional_patch_index.append([i, j])\n",
        "                else:\n",
        "                    neg_patch_index.append([i, j])\n",
        "\n",
        "            if (usage == \"validation\") and (label_ref.any() != 0) \\\n",
        "                and mask_ref.all():\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "\n",
        "            if (usage == \"inference\") and (mask_ref.any() != 0):\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "\n",
        "    if usage == \"train\":\n",
        "\n",
        "        num_negative_samples = min(\n",
        "            math.ceil(0.2 * len(proportional_patch_index)), 15\n",
        "        )\n",
        "        neg_samples = random.sample(neg_patch_index, num_negative_samples)\n",
        "\n",
        "        proportional_patch_index.extend(neg_samples)\n",
        "\n",
        "    # For test set use the indices generated from mask without\n",
        "    # considering the class proportions.\n",
        "    if usage in [\"validation\", \"inference\"]:\n",
        "        proportional_patch_index = non_proportional_patch_index\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of patches:\", len(proportional_patch_index))\n",
        "        print(\"Patched from:\\n{}\".format(proportional_patch_index))\n",
        "\n",
        "    return proportional_patch_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXnPz7npu2PV"
      },
      "source": [
        "#### Model, training, and evaluation\n",
        "\n",
        "You can add the U-Net model you designed in Assignment 3 (or 4), or use the one provided below (note, you will learn more if you use your own)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ9MB_rru2PV"
      },
      "source": [
        "##### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjb2pcAou2PW"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    r\"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int)-- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        kernel_size (int) -- Size of convolution kernel.\n",
        "        stride (int) -- decides how jumpy kernel moves along the spatial\n",
        "            dimensions.\n",
        "        padding (int) -- how much the input should be padded on the borders with\n",
        "            zero.\n",
        "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
        "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float) -- dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation,\n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        if num_conv_layers > 1:\n",
        "            if drop_rate > 0:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(drop_rate),\n",
        "                ] * (num_conv_layers - 1)\n",
        "            else:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                ] * (num_conv_layers - 1)\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class UpconvBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Decoder layer decodes the features along the expansive path.\n",
        "    Args:\n",
        "        in_channels (int) -- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        upmode (str) -- Upsampling type.\n",
        "            If \"fixed\" then a linear upsampling with scale factor of two will be\n",
        "            applied using bi-linear as interpolation method. If deconv_1 is\n",
        "            chosen then a non-overlapping transposed convolution will be applied\n",
        "            to upsample the feature maps. If deconv_1 is chosen then an\n",
        "            overlapping transposed convolution will be applied to upsample the\n",
        "            feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
        "        super(UpconvBlock, self).__init__()\n",
        "\n",
        "        if upmode == \"fixed\":\n",
        "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\",\n",
        "                                  align_corners=True), ]\n",
        "            layers += [nn.BatchNorm2d(in_channels),\n",
        "                       nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                 stride=1, padding=0, bias=False), ]\n",
        "\n",
        "        elif upmode == \"deconv_1\":\n",
        "            layers = [\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2,\n",
        "                                   stride=2, padding=0, dilation=1),\n",
        "            ]\n",
        "\n",
        "        elif upmode == \"deconv_2\":\n",
        "            layers = [\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n",
        "                                   stride=2, padding=1, dilation=1),\n",
        "            ]\n",
        "\n",
        "        # Dense Upscaling Convolution\n",
        "        elif upmode == \"DUC\":\n",
        "            up_factor = 2\n",
        "            upsample_dim = (up_factor ** 2) * out_channels\n",
        "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3,\n",
        "                                padding=1),\n",
        "                      nn.BatchNorm2d(upsample_dim),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.PixelShuffle(up_factor), ]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.block(inputs)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=None,\n",
        "                 dropout_rate=0):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if not filter_config:\n",
        "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
        "\n",
        "        assert len(filter_config) == 6\n",
        "\n",
        "        # Contraction Path\n",
        "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 64x256x256\n",
        "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 128x128x128\n",
        "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 256x64x64\n",
        "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 512x32x32\n",
        "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 1024x16x16\n",
        "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 2048x8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Expansion Path\n",
        "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4],\n",
        "                                     upmode=\"deconv_2\")  # 1024x16x16\n",
        "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3],\n",
        "                                     upmode=\"deconv_2\")  # 512x32x32\n",
        "        self.conv2 = ConvBlock(filter_config[4], filter_config[3],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2],\n",
        "                                     upmode=\"deconv_2\")  # 256x64x64\n",
        "        self.conv3 = ConvBlock(filter_config[3], filter_config[2],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1],\n",
        "                                     upmode=\"deconv_2\")  # 128x128x128\n",
        "        self.conv4 = ConvBlock(filter_config[2], filter_config[1],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0],\n",
        "                                     upmode=\"deconv_2\")  # 64x256x256\n",
        "        self.conv5 = ConvBlock(filter_config[1], filter_config[0],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1,\n",
        "                                    stride=1, padding=0)  # classNumx256x256\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # set_trace()\n",
        "        e1 = self.encoder_1(inputs)  # batch size x 64 x 256 x 256\n",
        "        p1 = self.pool(e1)  # batch size x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.encoder_2(p1)  # batch size x 128 x 128 x 128\n",
        "        p2 = self.pool(e2)  # batch size x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.encoder_3(p2)  # batch size x 256 x 64 x 64\n",
        "        p3 = self.pool(e3)  # batch size x 256 x 32 x 32\n",
        "\n",
        "        e4 = self.encoder_4(p3)  # batch size x 512 x 32 x 32\n",
        "        p4 = self.pool(e4)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        e5 = self.encoder_5(p4)  # batch size x 1024 x 16 x 16\n",
        "        p5 = self.pool(e5)  # batch size x 1024 x 8 x 8\n",
        "\n",
        "        e6 = self.encoder_6(p5)  # batch size x 2048 x 8 x 8\n",
        "\n",
        "        d6 = self.decoder_1(e6)  # batch size x 1024 x 16 x 16\n",
        "        skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 16 x 16\n",
        "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 32 x 32\n",
        "        skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 32 x 32\n",
        "        d5_proper = self.conv2(skip2)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 64 x 64\n",
        "        skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 64 x 64\n",
        "        d4_proper = self.conv3(skip3)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 128 x 128\n",
        "        skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 128 x 128\n",
        "        d3_proper = self.conv4(skip4)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 256 x 256\n",
        "        skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 256 x 256\n",
        "        d2_proper = self.conv5(skip5)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        d1 = self.classifier(d2_proper)  # batch size x classNum x 256 x 256\n",
        "\n",
        "        return d1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMl_cclvu2PW"
      },
      "source": [
        "##### Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3qrwOitu2PW"
      },
      "outputs": [],
      "source": [
        "class BinaryTverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Pytorch versiono of tversky focal loss proposed in paper\n",
        "    'A novel focal Tversky loss function and improved Attention U-Net for lesion\n",
        "    segmentation' (https://arxiv.org/abs/1810.07842)\n",
        "    Arguments:\n",
        "        smooth (float): A float number to smooth loss, and avoid NaN error,\n",
        "            default: 1\n",
        "        alpha (float): Hyperparameters alpha, paired with (1 - alpha) to shift\n",
        "            emphasis to improve recall\n",
        "        gamma (float): Tversky index, default: 1.33\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smooth=1, alpha=0.7, gamma=1.33):\n",
        "        super(BinaryTverskyFocalLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.alpha = alpha\n",
        "        self.beta = 1 - self.alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # shape: [N, C, *]\n",
        "        assert predict.shape[0] == target.shape[0], \\\n",
        "            \"predict & target batch size do not match\"\n",
        "\n",
        "        # no reduction, same as original paper\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        num = (predict * target).sum() + self.smooth\n",
        "        den = (predict * target).sum() + self.alpha * \\\n",
        "            ((1 - predict) * target).sum() + self.beta * \\\n",
        "            (predict * (1 - target)).sum() + self.smooth\n",
        "        loss = torch.pow(1 - num / den, 1 / self.gamma)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Tversky focal loss\n",
        "    Arguments:\n",
        "        weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor with the same shape as predict.\n",
        "        other args pass to BinaryTverskyFocalLoss\n",
        "    Returns:\n",
        "        same as BinaryTverskyFocalLoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
        "        super(TverskyFocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        nclass = predict.shape[1]\n",
        "        if predict.shape == target.shape:\n",
        "            pass\n",
        "        elif len(predict.shape) == 4:\n",
        "            target = F.one_hot(target, num_classes=nclass).permute(0, 3, 1, 2)\\\n",
        "                .contiguous()\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        tversky = BinaryTverskyFocalLoss(**self.kwargs)\n",
        "        total_loss = 0\n",
        "        weight = torch.Tensor([1. / nclass] * nclass).cuda() \\\n",
        "            if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        for i in range(nclass):\n",
        "            if i != self.ignore_index:\n",
        "                tversky_loss = tversky(predict[:, i], target[:, i])\n",
        "                assert weight.shape[0] == nclass, \\\n",
        "                    'Expect weight shape [{}], get[{}]'\\\n",
        "                    .format(nclass, weight.shape[0])\n",
        "                tversky_loss *= weight[i]\n",
        "                total_loss += tversky_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "class BalancedTverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Tversky focal loss weighted by inverse of label frequency\n",
        "    Arguments:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same\n",
        "            shape with predict\n",
        "        other args pass to BinaryTverskyFocalLoss\n",
        "    Returns:\n",
        "        same as TverskyFocalLoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-100, **kwargs):\n",
        "        super(BalancedTverskyFocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
        "        # calculate weight for only valid indices\n",
        "        unique_counts = unique_counts[unique != self.ignore_index]\n",
        "        unique = unique[unique != self.ignore_index]\n",
        "        ratio = unique_counts.float() / torch.numel(target)\n",
        "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
        "\n",
        "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
        "        for i in range(len(unique)):\n",
        "            lossWeight[unique[i]] = weight[i]\n",
        "\n",
        "        # loss\n",
        "        loss = TverskyFocalLoss(weight=lossWeight,\n",
        "                                ignore_index=self.ignore_index, **self.kwargs)\n",
        "\n",
        "        return loss(predict, target)\n",
        "\n",
        "\n",
        "class TverskyFocalCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combination of tversky focal loss and cross entropy loss though summation\n",
        "    Arguments:\n",
        "        loss_weight (tensor): a manual rescaling weight given to each class. If\n",
        "            given, has to be a Tensor of size C\n",
        "        tversky_weight (float): Weight on tversky focal loss for the summation,\n",
        "             while weight on cross entropy loss is (1 - tversky_weight)\n",
        "        tversky_smooth (float): A float number to smooth tversky focal loss,\n",
        "            and avoid NaN error, default: 1\n",
        "        tversky_alpha (float):\n",
        "        tversky_gamma (float):\n",
        "        ignore_index (int): Class index to ignore\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss_weight=None, tversky_weight=0.5, tversky_smooth=1,\n",
        "                 tversky_alpha=0.7, tversky_gamma=0.9, ignore_index=-100):\n",
        "        super(TverskyFocalCELoss, self).__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.tversky_weight = tversky_weight\n",
        "        self.tversky_smooth = tversky_smooth\n",
        "        self.tversky_alpha = tversky_alpha\n",
        "        self.tversky_gamma = tversky_gamma\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \\\n",
        "            \"predict & target batch size do not match\"\n",
        "\n",
        "        tversky = TverskyFocalLoss(weight=self.loss_weight,\n",
        "                                   ignore_index=self.ignore_index,\n",
        "                                   smooth=self.tversky_smooth,\n",
        "                                   alpha=self.tversky_alpha,\n",
        "                                   gamma=self.tversky_gamma)\n",
        "        ce = nn.CrossEntropyLoss(weight=self.loss_weight,\n",
        "                                 ignore_index=self.ignore_index)\n",
        "        loss = self.tversky_weight * tversky(predict, target) +\\\n",
        "            (1 - self.tversky_weight) * ce(predict, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class BalancedTverskyFocalCELoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Combination of tversky focal loss and cross entropy loss weighted by inverse of label frequency\n",
        "    Arguments:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
        "        other args pass to DiceCELoss, excluding loss_weight\n",
        "    Returns:\n",
        "        Same as TverskyFocalCELoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-100, **kwargs):\n",
        "        super(BalancedTverskyFocalCELoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
        "        # calculate weight for only valid indices\n",
        "        unique_counts = unique_counts[unique != self.ignore_index]\n",
        "        unique = unique[unique != self.ignore_index]\n",
        "        ratio = unique_counts.float() / torch.numel(target)\n",
        "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
        "\n",
        "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
        "        for i in range(len(unique)):\n",
        "            lossWeight[unique[i]] = weight[i]\n",
        "\n",
        "        loss = TverskyFocalCELoss(loss_weight=lossWeight, **self.kwargs)\n",
        "\n",
        "        return loss(predict, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwf8TVKwu2PX"
      },
      "source": [
        "##### Model fitting (training + validation)\n",
        "\n",
        "Functions to train the model using the training set, and to do validation on the validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_ePuRk4u2PX"
      },
      "outputs": [],
      "source": [
        "def train(trainData, model, optimizer, criterion, device, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset (AquacultureData).\n",
        "            model -- Choice of segmentation model.\n",
        "            optimizer -- Chosen optimization algorithm to update model\n",
        "                parameters.\n",
        "            criterion -- Chosen function to calculate loss over training\n",
        "                samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- ???????????????????????????\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))\n",
        "\n",
        "##########################################################\n",
        "\n",
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation\n",
        "                samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset.\n",
        "                This allows loss to calculate at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))\n",
        "\n",
        "##########################################################\n",
        "\n",
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "                  initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "\n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        model = model\n",
        "\n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.90)\n",
        "\n",
        "    for t in range(num_epochs):\n",
        "        print(\"Epoch [{}/{}]\".format(t + 1, num_epochs))\n",
        "        start_epoch = datetime.now()\n",
        "\n",
        "        train(trainData, model, optimizer, criterion, device,\n",
        "              train_loss=train_loss)\n",
        "        validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LR: {}\".format(scheduler.get_last_lr()))\n",
        "\n",
        "        writer.add_scalars(\"Loss\",\n",
        "                           {\"train loss\": train_loss[t],\n",
        "                            \"validation loss\": val_loss[t]},\n",
        "                           t + 1)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\"\\\n",
        "          .format(duration_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNQoRIwu2PX"
      },
      "source": [
        "##### Evaluation and accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NidHe0KUu2PX"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) /\\\n",
        "            np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix)\n",
        "                )\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
        "\n",
        "#########################\n",
        "\n",
        "def do_accuracy_evaluation(model, dataloader, num_classes):\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # add batch to evaluator\n",
        "            evaluator.add_batch(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    mean_accuracy = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_IoU = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_IoU = evaluator\\\n",
        "        .Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    return pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtyPvxzWPVrc"
      },
      "source": [
        "### Coding Assignment 1\n",
        "\n",
        "This first task is to complete the pipeline you were developing from [Assignment 3](assignment3.ipynb). To do this, you need to modify the custom dataset (e.g. `ActiveLoadingDataset`) to read and pre-process the prediction dataset to get fed into the model.\n",
        "\n",
        "The process is very similar to the custom dataset you have used in [Assignment 3](assignment3.ipynb). The prediction scenes are organized in a \"csv\" file. Pixel values are atmospherically corrected to ground reflectance (Values are between 0 and 1) if you want you can standardize it or leave it as is.\n",
        "\n",
        "The detailed instructions to complete both this task are provided in the template, which you will modify to complete this assignment.\n",
        "\n",
        "After completing the modification of the `AquaCultureData` class, run through the code below it, which has the complete pipeline for training, validating, and testing model performance. Please find the dataset needed for the assignment in [here](https://drive.google.com/drive/folders/1FjHF1ompyGhR3KEVdkn62SNEdwln6CJr?usp=sharing).\n",
        "\n",
        "\n",
        "\n",
        "(15 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 15/15 points on CA 1</font>**"
      ],
      "metadata": {
        "id": "rsD208zos6di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AquacultureData(torch.utils.data.Dataset):\n",
        "    def __init__(self, src_dir, usage, dataset_name=None,\n",
        "                 apply_normalization=False, transform=None, csv_name=None,\n",
        "                 patch_size=None, overlap=None, catalog_index=None):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing\n",
        "                                 structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to\n",
        "            be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.csv_name = csv_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\", \"inference\"], \\\n",
        "            \"Usage is not recognized.\"\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            assert self.dataset_name is not None\n",
        "            img_dir = Path(src_dir) / self.dataset_name / self.usage / \"bands\"\n",
        "            img_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(img_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            img_fnames.sort()\n",
        "\n",
        "            lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "            lbl_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(lbl_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            lbl_fnames.sort()\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.lbl_chips = []\n",
        "\n",
        "            for img_path, lbl_path in tqdm.tqdm(zip(img_fnames, lbl_fnames),\n",
        "                                                total=len(img_fnames)):\n",
        "                img_chip = load_data(\n",
        "                    img_path, is_label=False,\n",
        "                    apply_normalization=self.apply_normalization\n",
        "                )\n",
        "                img_chip = img_chip.transpose((1, 2, 0))\n",
        "\n",
        "                lbl_chip = load_data(lbl_path, is_label=True)\n",
        "\n",
        "                self.img_chips.append(img_chip)\n",
        "                self.lbl_chips.append(lbl_chip)\n",
        "\n",
        "            print('--------------{} patches cropped--------------'.format(len(self.img_chips)))\n",
        "\n",
        "        # This part handles prediction dataset\n",
        "        else:\n",
        "            assert self.csv_name is not None\n",
        "\n",
        "            self.catalog = pd.read_csv(Path(self.src_dir) / self.csv_name)\n",
        "            self.catalog = self.catalog.iloc[catalog_index]\n",
        "\n",
        "            self.tile = (self.catalog[\"wrs_path\"], self.catalog[\"wrs_row\"])\n",
        "\n",
        "            img_path_ls = [Path(self.src_dir)/self.catalog[\"img_dir\"]]\n",
        "            mask_path_ls = [Path(self.src_dir)/self.catalog[\"mask_dir\"]]\n",
        "\n",
        "            self.meta = get_meta_from_bounds(img_path_ls[0])\n",
        "\n",
        "            for img_path, mask_path in zip(img_path_ls, mask_path_ls):\n",
        "                img = load_data(img_path, is_label=False, apply_normalization=self.apply_normalization)\n",
        "                img = img.transpose((1, 2, 0))\n",
        "                mask = load_data(mask_path, is_label=True)\n",
        "\n",
        "            crop_ref = mask\n",
        "            index = patch_center_index(crop_ref, self.patch_size, self.overlap, self.usage)\n",
        "            half_size = self.patch_size // 2\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.coor = []\n",
        "\n",
        "            for i in range(len(index)):\n",
        "                x = index[i][0]\n",
        "                y = index[i][1]\n",
        "                self.img_chips.append(img[x - half_size: x + half_size, y - half_size: y + half_size, :])\n",
        "                self.coor.append([x, y])\n",
        "\n",
        "            print('--------------{} patches cropped--------------'.format(len(self.img_chips)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            image_chip = self.img_chips[index]\n",
        "            label_chip = self.lbl_chips[index]\n",
        "\n",
        "            if self.usage == \"train\" and self.transform:\n",
        "                trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "                if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                    trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                    image_chip, label_chip = flip_image_and_label(\n",
        "                        image_chip, label_chip, trans_flip\n",
        "                    )\n",
        "\n",
        "                if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                    img_chip, lbl_chip = rotate_image_and_label(\n",
        "                        image_chip, label_chip, angle=[0,90]\n",
        "                    )\n",
        "\n",
        "            image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1))).float()\n",
        "            label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip)).long()\n",
        "\n",
        "            return image_tensor, label_tensor\n",
        "        else:\n",
        "            coor = self.coor[index]\n",
        "            img_chip = self.img_chips[index]\n",
        "            image_tensor = torch.from_numpy(img_chip.transpose((2, 0, 1))).float()\n",
        "\n",
        "            return image_tensor, coor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "GQPZQCfDhl-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import psutil  # For monitoring memory usage\n",
        "import numpy as np\n",
        "\n",
        "# Assuming load_data, flip_image_and_label, rotate_image_and_label are defined elsewhere.\n",
        "# Assuming AquacultureData class is correctly defined.\n",
        "\n",
        "def monitor_memory():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 * 1024)  # in MB\n",
        "    print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "# Modify the AquacultureData class to include memory monitoring\n",
        "class AquacultureDataWithMemoryMonitor(AquacultureData):\n",
        "    def __getitem__(self, index):\n",
        "        print(f\"Processing sample {index}\")\n",
        "        monitor_memory() # check memory before loading data\n",
        "        image_tensor, label_tensor = super().__getitem__(index)\n",
        "        monitor_memory() # check memory after loading data\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "# Test the modified class\n",
        "src_dir = \"/content/gdrive/MyDrive/adleo/A5/Global-20250403T014737Z-001\" # change as needed\n",
        "dataset_name = \"train\" #replace with your dataset_name\n",
        "transform = [\"flip_horizontal\", \"rotate\"] #replace with your transforms\n",
        "\n",
        "train_dataset = AquacultureDataWithMemoryMonitor(\n",
        "    src_dir=src_dir,\n",
        "    usage=\"train\",\n",
        "    dataset_name=dataset_name,\n",
        "    apply_normalization=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Example: Iterate through the dataset (stop before the crash)\n",
        "for i in range(462):  # Stop at sample 461\n",
        "    try:\n",
        "        image, label = train_dataset[i]\n",
        "    except Exception as e:\n",
        "        print(f\"Error at sample {i}: {e}\")\n",
        "        break\n",
        "\n",
        "print(\"Testing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwmnmocWXSnV",
        "outputId": "48cdaeac-289c-468f-b3b9-fb7dab7d5621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------0 patches cropped--------------\n",
            "Processing sample 0\n",
            "Memory Usage: 4254.47 MB\n",
            "Error at sample 0: list index out of range\n",
            "Testing complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_-qmChNu2PX"
      },
      "source": [
        "#### Running through the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVelgSMqoKm"
      },
      "source": [
        "##### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZNC2aPuqoKn"
      },
      "outputs": [],
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo/A5/Global-20250403T014737Z-001\" # change as needed\n",
        "dataset_name = \"Global\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/A5/\" # change as needed\n",
        "initial_lr = 0.15\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRfyQKuu2PY"
      },
      "source": [
        "##### Training\n",
        "\n",
        "Create a `train_dataset` object using the `AquacultureData` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRmg0CVHu2PY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29beab5-efb3-4292-b025-7da4b2493d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1188/1188 [01:41<00:00, 11.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------1188 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = AquacultureData(src_dir,\n",
        "                                usage=\"train\",\n",
        "                                dataset_name=dataset_name,\n",
        "                                apply_normalization=False,\n",
        "                                transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYj8wvsnu2Pc"
      },
      "source": [
        "Create a `PyTorch` data loader called `train_loader` that loads data from the `train_dataset`, splits it into batches, convert is to tensor and moves the data to GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8hgJpXyu2Pc"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16,\n",
        "                          shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugOQSrJKu2Pc"
      },
      "source": [
        "##### Validation\n",
        "\n",
        "Create a `validation_dataset` object using the `AquacultureData` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmGHIKlgu2Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06b209f-8740-412c-9aaa-f2ba4901c9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 239/239 [00:09<00:00, 26.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------239 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "validation_dataset = AquacultureData(src_dir,\n",
        "                                     usage=\"validation\",\n",
        "                                     dataset_name=dataset_name,\n",
        "                                     apply_normalization=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM7D9_1_u2Pc"
      },
      "source": [
        "Create a `PyTorch` data loader for the `validation_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Fs9cbhu2Pc"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(validation_dataset,\n",
        "                        batch_size = 1,\n",
        "                        shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5GndESUu2Pc"
      },
      "source": [
        "##### Initialize the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW3IPhzEu2Pc"
      },
      "outputs": [],
      "source": [
        "model = Unet(n_classes,\n",
        "             in_channels,\n",
        "             filter_config,\n",
        "             dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RBZYD41u2Pd"
      },
      "source": [
        "##### Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhI7ZSjMu2Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b06090-5204-4bbe-f6df-11c36f67deee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch [1/10]\n",
            "Training loss: 0.4399\n",
            "validation loss: 0.6085120456986847\n",
            "LR: [0.15]\n",
            "Epoch [2/10]\n",
            "Training loss: 0.3963\n",
            "validation loss: 1.122010616079035\n",
            "LR: [0.15]\n",
            "Epoch [3/10]\n",
            "Training loss: 0.3732\n",
            "validation loss: 0.6129176144071203\n",
            "LR: [0.135]\n",
            "Epoch [4/10]\n",
            "Training loss: 0.3879\n",
            "validation loss: 1.6616404781032308\n",
            "LR: [0.135]\n",
            "Epoch [5/10]\n",
            "Training loss: 0.3728\n",
            "validation loss: 0.6623676385590223\n",
            "LR: [0.135]\n",
            "Epoch [6/10]\n",
            "Training loss: 0.3625\n",
            "validation loss: 0.5172315409492748\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [7/10]\n",
            "Training loss: 0.3645\n",
            "validation loss: 0.6221457207302669\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [8/10]\n",
            "Training loss: 0.3614\n",
            "validation loss: 0.7210131310020031\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [9/10]\n",
            "Training loss: 0.4140\n",
            "validation loss: 0.5231113583472982\n",
            "LR: [0.10935000000000002]\n",
            "Epoch [10/10]\n",
            "Training loss: 0.3504\n",
            "validation loss: 0.6910680451403103\n",
            "LR: [0.10935000000000002]\n",
            "--------------- Training finished in 0:00:33 ---------------\n"
          ]
        }
      ],
      "source": [
        "epochIterater(train_loader,\n",
        "              val_loader,\n",
        "              model,\n",
        "              criterion,\n",
        "              WorkingFolder,\n",
        "              initial_lr,\n",
        "              epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EThvsaEOu2Pd"
      },
      "source": [
        "Save the trained model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hoowpayu2Pd"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\n",
        "           os.path.join(Path(WorkingFolder), \"trained_unet_final_state.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYO5-XDDu2Pd"
      },
      "source": [
        "##### Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MgPgNLu2Pd"
      },
      "outputs": [],
      "source": [
        "test = do_accuracy_evaluation(model.cuda(), val_loader, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buycO80Vo-L0"
      },
      "source": [
        "###  Coding Assignment 2\n",
        "\n",
        "Complete the code in the template below for running predictions on the imagery. See comments in `do_prediction` where code needs to be added to complete the function (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 15/15 points on CA 2</font>**"
      ],
      "metadata": {
        "id": "QHBdvfSKt28i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5dyyiGNpEEe"
      },
      "outputs": [],
      "source": [
        "def do_prediction(testData, model, overlap, device, save_dir):\n",
        "    r\"\"\"\n",
        "    Use train model to predict on unseen data.\n",
        "    Arguments:\n",
        "            testData (custom iterator) -- Batches of image chips from PyTorch\n",
        "                                          custom dataset.\n",
        "            model (ordered Dict) -- trained model.\n",
        "            overlap (int) -- amount of overlap between prediction chips.\n",
        "            device (str) -- Either \"cpu\" or \"cuda\".\n",
        "            save_dir (str) -- Directory to save the prediction output.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create directories to save the predicted output\n",
        "    save_dir_hard = Path(save_dir) / \"HardScore\"\n",
        "    save_dir_soft = Path(save_dir) / \"SoftScore\"\n",
        "\n",
        "    os.makedirs(save_dir_hard, exist_ok=True)\n",
        "    os.makedirs(save_dir_soft, exist_ok=True)\n",
        "\n",
        "    # Start inference on test data\n",
        "    print(\"--------------------- Start Inference(Test) ---------------------\")\n",
        "    start = datetime.now()\n",
        "\n",
        "    # Get the test data, metadata, and tile information\n",
        "    # add your code here\n",
        "    testData, meta, tile = testData\n",
        "\n",
        "    # Define the output file names and metadata for the hard and soft scores\n",
        "    name_prob = \"prob_c{}_r{}\".format(tile[0], tile[1])\n",
        "    name_crisp = \"crisp_c{}_r{}.rst\".format(tile[0], tile[1])\n",
        "\n",
        "    meta_hard = meta.copy()\n",
        "    meta_hard.update({\n",
        "        \"dtype\": \"uint8\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    meta_soft = meta.copy()\n",
        "    meta_soft.update({\n",
        "        \"dtype\": \"float32\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    ##### Add your code to put the model in evaluation mode. (1 line)\n",
        "    model.eval()\n",
        "\n",
        "    canvas_score_ls = []\n",
        "\n",
        "    ##### Create a canvas (call it \"h_canvas\") with the same height, width and\n",
        "    ##### datatype from \"meta_hard\" to hold the score values and initialize it\n",
        "    ##### to zeros. Add your code here. (Expected 1 line)\n",
        "\n",
        "    h_canvas = np.zeros((1, meta_hard['height'], meta_hard['width']), dtype=meta_hard['dtype'])\n",
        "\n",
        "\n",
        "    # Loop over batches of image chips and indices.\n",
        "# Loop over batches of image chips and indices.\n",
        "    for img_chips, index_batch in testData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        img = img_chips.to(device) # size: B X in_channels X W X H\n",
        "\n",
        "        ##### Forward pass through the model to get the predictions and assign\n",
        "        ####  it to a variable called \"pred\".\n",
        "        ##### Add your code here. (Expected 1 line)\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        ##### Normalize the model output using \"softmax\" And assign it to a\n",
        "        ##### variable called \"pred_prob\".\n",
        "        ##### Add your code here (Expected 1 line)\n",
        "\n",
        "        pred_prob = torch.softmax(pred, dim=1)\n",
        "\n",
        "        # Get the dimensions of the prediction\n",
        "        batch, n_class, height, width = pred_prob.size()\n",
        "\n",
        "        # Calculate the score width and score height based on the overlap\n",
        "        # parameter\n",
        "        score_width = (width // 2) - overlap\n",
        "        score_height = (height // 2) - overlap\n",
        "\n",
        "        # Loop over the batch and assign the predicted scores to the canvas\n",
        "        for i in range(batch):\n",
        "\n",
        "            # creating a new tuple index containing the coordinates, which makes\n",
        "            # it easier to index into the \"h_canvas\" and arrays in the\n",
        "            # \"canvas_score_ls\" later on in the code.\n",
        "            index = (index_batch[0][i], index_batch[1][i])\n",
        "\n",
        "            # Get the hard scores by taking the argmax of the prediction\n",
        "            prediction_hard = pred_prob.max(dim=1)[1][\n",
        "                :, overlap:-overlap, overlap:-overlap\n",
        "            ].cpu().numpy()[i, :, :]\n",
        "\n",
        "            # add the batch dimension to the \"prediction_hard\" array and\n",
        "            # convert its data types.\n",
        "            prediction_hard = np.expand_dims(prediction_hard, axis=0).astype(meta_hard[\"dtype\"])\n",
        "\n",
        "            # The \"prediction_hard\" values are assigned to a slice of\n",
        "            # \"h_canvas\", effectively updating the pixels in the original image\n",
        "            # corresponding to the current image chip in the batch with the\n",
        "            # predicted values for that chip.\n",
        "            ##### Add your code here. (Expected 1 line)\n",
        "\n",
        "            h_canvas[:, index[0] - score_width: index[0] + score_width, index[1] - score_height: index[1] + score_height] = prediction_hard\n",
        "\n",
        "\n",
        "            for n in range(1, n_class):\n",
        "                # Extract probability map for class n from predicted\n",
        "                # probabilities tensor\n",
        "                prediction_soft = pred_prob[:, n, :, :].data[i][overlap:-overlap, overlap:-overlap].cpu().numpy() * 100\n",
        "                # Add an extra dimension to the probability map to match the\n",
        "                # expected shape\n",
        "                prediction_soft = np.expand_dims(prediction_soft, axis=0).astype(meta_soft[\"dtype\"])\n",
        "\n",
        "                try:\n",
        "                    # Update existing canvas for class n w/new probability map\n",
        "                    canvas_score_ls[n][\n",
        "                        :, index[0] - score_width: index[0] + score_width,\n",
        "                        index[1] - score_height: index[1] + score_height\n",
        "                    ] = prediction_soft\n",
        "                except:\n",
        "                    # Create a new canvas for class n and initialize it with\n",
        "                    # zeros\n",
        "                    canvas_score_single = np.zeros(\n",
        "                        (1, meta_soft['height'], meta_soft['width']),\n",
        "                        dtype=meta_soft['dtype']\n",
        "                    )\n",
        "\n",
        "                    # Update the new canvas with the new probability map slice\n",
        "                    # by slice\n",
        "                    canvas_score_single[\n",
        "                        :, index[0] - score_width: index[0] + score_width,\n",
        "                        index[1] - score_height: index[1] + score_height\n",
        "                    ] = prediction_soft\n",
        "\n",
        "                    # Add the new canvas to the list of canvases for all classes\n",
        "                    canvas_score_ls.append(canvas_score_single)\n",
        "\n",
        "        # write the hard classification results to an output raster.\n",
        "        # Use \"save_dir_hard\", \"name_crisp\" and \"meta_hard\".\n",
        "        save_raster(h_canvas, Path(save_dir_hard) / name_crisp, meta_hard)\n",
        "\n",
        "        # loop through each class (excluding the background class) and creates a\n",
        "        # new raster file for each class.\n",
        "        # hint: use this code to get a proper name for the prediction output\n",
        "        # for each class: name_prob_updated = f\"{name_prob}_Cat_{n}.tif\"\n",
        "\n",
        "        for n in range(1, len(canvas_score_ls)):\n",
        "            name_prob_updated = f\"{name_prob}_Cat_{n}.tif\"\n",
        "            save_raster(canvas_score_ls[n], Path(save_dir_soft) / name_prob_updated, meta_soft)\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"---------------- Inference finished in {} seconds ----------------\".format(duration_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUQvE6Eux5yS"
      },
      "source": [
        "### Coding Assignment 3\n",
        "\n",
        "In this task, you will fine-tuning an already trained model on a new dataset. We have already trained the provided `U-Net` on the bigger size PondDataset for 40 epochs (see example of train-from-scratch pipeline above). You can find the model in the shared gdrive for the course.\n",
        "\n",
        "We also provide you with a dataset to finetune the pre-trained model for a few more epochs (e.g. 25). You need to decide what layers of the model to freeze (see Practical 5 for different approaches). Normally as we have a small dataset to finetune we only update a few layers at the top of decoder. Your dataset is big enough to experiment with different numbers of layers to freeze.\n",
        "\n",
        "Also, as a rule of thumb we use a smaller learning rate during the finetune process. The initial LR for the first training was 0.1. You can choose LR of 0.01 or smaller.\n",
        "\n",
        "To finetune the model:\n",
        "\n",
        "1. First decide which layers to freeze;\n",
        "2. Then finetune the model\n",
        "\n",
        "That will entail freeze the layers as shown in the code below, then\n",
        "\n",
        "After completion, use this finetuned model to perform the prediction.\n",
        "\n",
        "(20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 20/20 points on CA 3</font>**"
      ],
      "metadata": {
        "id": "YBmtubImtyDh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSO1JHsJu2Pe"
      },
      "source": [
        "#### Fine-tuning pipeline\n",
        "\n",
        "Run through the training pipeline below, where there is a block that has the step for loading weights and then freezing layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPQ8Z94Xu2Pf"
      },
      "source": [
        "##### Initial parameters\n",
        "\n",
        "Change the directory paths as needed to locate the provided datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5etJBDkx6vZ"
      },
      "outputs": [],
      "source": [
        "# src_dir = \"/content/gdrive/MyDrive/adleo/A5/trained_model_parameters-20250403T012520Z-001/trained_model_parameters\"\n",
        "# dataset_name = \"Fine_tune_dataset-20250403T012514Z-001\"\n",
        "# transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "# n_classes = 2\n",
        "# in_channels = 6\n",
        "# filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "# dropout_rate = 0.15\n",
        "\n",
        "# criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "# WorkingFolder = \"/content/gdrive/MyDrive/adleo/A5/\"\n",
        "# initial_lr = 0.01\n",
        "# epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LuNtFf7u2Pf"
      },
      "source": [
        "##### Load train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUxy3PsyyEOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e4e02e-fb23-4587-cead-994394c74e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1188/1188 [01:37<00:00, 12.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------1188 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = AquacultureData(src_dir,\n",
        "                                usage=\"train\",\n",
        "                                dataset_name=dataset_name,\n",
        "                                apply_normalization=False,\n",
        "                                transform=transform)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16,\n",
        "                          shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGbOIlnu2Pf"
      },
      "source": [
        "##### Load validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJX30X0SyHKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5752d7a5-5e4b-41f7-feb2-efaa70b12461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 239/239 [00:09<00:00, 26.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------239 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "validation_dataset = AquacultureData(src_dir,\n",
        "                                     usage=\"validation\",\n",
        "                                     dataset_name=dataset_name,\n",
        "                                     apply_normalization=False)\n",
        "val_loader = DataLoader(validation_dataset,\n",
        "                        batch_size = 1,\n",
        "                        shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TA Comment**\n",
        "\n",
        "You loaded the wrong dataset in here, it should load the finetuning dataset."
      ],
      "metadata": {
        "id": "mmI2AQihtXNE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeaCsX8zyHef"
      },
      "source": [
        "##### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1EGnuBLyUeB"
      },
      "outputs": [],
      "source": [
        "model = Unet(n_classes,\n",
        "             in_channels,\n",
        "             filter_config,\n",
        "             dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aauSwvS8u2Pg"
      },
      "source": [
        "##### Load the saved model parameters, specify freeze layers\n",
        "\n",
        "That means that you load the weights, you will want to use the code we have previously looked at (Practical 5) that show how to find the different model layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOWGhU9su2Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224746b3-7d74-4619-a86f-91af4b96d488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Unet(\n",
            "  (encoder_1): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_2): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_3): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_4): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_5): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_6): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (decoder_1): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv1): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_2): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_3): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_4): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_5): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "encoder_1: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_1.block: Sequential(\n",
            "  (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_1.block.0: Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_1.block.1: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_1.block.2: ReLU(inplace=True)\n",
            "encoder_1.block.3: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_1.block.4: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_1.block.5: ReLU(inplace=True)\n",
            "encoder_1.block.6: Dropout(p=0.1, inplace=False)\n",
            "encoder_2: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_2.block: Sequential(\n",
            "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_2.block.0: Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_2.block.1: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_2.block.2: ReLU(inplace=True)\n",
            "encoder_2.block.3: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_2.block.4: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_2.block.5: ReLU(inplace=True)\n",
            "encoder_2.block.6: Dropout(p=0.1, inplace=False)\n",
            "encoder_3: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_3.block: Sequential(\n",
            "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_3.block.0: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_3.block.1: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_3.block.2: ReLU(inplace=True)\n",
            "encoder_3.block.3: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_3.block.4: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_3.block.5: ReLU(inplace=True)\n",
            "encoder_3.block.6: Dropout(p=0.1, inplace=False)\n",
            "encoder_4: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_4.block: Sequential(\n",
            "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_4.block.0: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_4.block.1: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_4.block.2: ReLU(inplace=True)\n",
            "encoder_4.block.3: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_4.block.4: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_4.block.5: ReLU(inplace=True)\n",
            "encoder_4.block.6: Dropout(p=0.1, inplace=False)\n",
            "encoder_5: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_5.block: Sequential(\n",
            "  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_5.block.0: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_5.block.1: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_5.block.2: ReLU(inplace=True)\n",
            "encoder_5.block.3: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_5.block.4: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_5.block.5: ReLU(inplace=True)\n",
            "encoder_5.block.6: Dropout(p=0.1, inplace=False)\n",
            "encoder_6: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "encoder_6.block: Sequential(\n",
            "  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "encoder_6.block.0: Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_6.block.1: BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_6.block.2: ReLU(inplace=True)\n",
            "encoder_6.block.3: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "encoder_6.block.4: BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "encoder_6.block.5: ReLU(inplace=True)\n",
            "encoder_6.block.6: Dropout(p=0.1, inplace=False)\n",
            "pool: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "decoder_1: UpconvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "decoder_1.block: Sequential(\n",
            "  (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "decoder_1.block.0: ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "conv1: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "conv1.block: Sequential(\n",
            "  (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "conv1.block.0: Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv1.block.1: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv1.block.2: ReLU(inplace=True)\n",
            "conv1.block.3: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv1.block.4: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv1.block.5: ReLU(inplace=True)\n",
            "conv1.block.6: Dropout(p=0.1, inplace=False)\n",
            "decoder_2: UpconvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "decoder_2.block: Sequential(\n",
            "  (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "decoder_2.block.0: ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "conv2: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "conv2.block: Sequential(\n",
            "  (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "conv2.block.0: Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv2.block.1: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv2.block.2: ReLU(inplace=True)\n",
            "conv2.block.3: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv2.block.4: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv2.block.5: ReLU(inplace=True)\n",
            "conv2.block.6: Dropout(p=0.1, inplace=False)\n",
            "decoder_3: UpconvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "decoder_3.block: Sequential(\n",
            "  (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "decoder_3.block.0: ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "conv3: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "conv3.block: Sequential(\n",
            "  (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "conv3.block.0: Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv3.block.1: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv3.block.2: ReLU(inplace=True)\n",
            "conv3.block.3: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv3.block.4: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv3.block.5: ReLU(inplace=True)\n",
            "conv3.block.6: Dropout(p=0.1, inplace=False)\n",
            "decoder_4: UpconvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "decoder_4.block: Sequential(\n",
            "  (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "decoder_4.block.0: ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "conv4: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "conv4.block: Sequential(\n",
            "  (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "conv4.block.0: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv4.block.1: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv4.block.2: ReLU(inplace=True)\n",
            "conv4.block.3: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv4.block.4: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv4.block.5: ReLU(inplace=True)\n",
            "conv4.block.6: Dropout(p=0.1, inplace=False)\n",
            "decoder_5: UpconvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "decoder_5.block: Sequential(\n",
            "  (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "decoder_5.block.0: ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "conv5: ConvBlock(\n",
            "  (block): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "conv5.block: Sequential(\n",
            "  (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "conv5.block.0: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv5.block.1: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv5.block.2: ReLU(inplace=True)\n",
            "conv5.block.3: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "conv5.block.4: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv5.block.5: ReLU(inplace=True)\n",
            "conv5.block.6: Dropout(p=0.1, inplace=False)\n",
            "classifier: Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n"
          ]
        }
      ],
      "source": [
        "# Use this block to add code to help you query the model layers.\n",
        "for name, module in model.named_modules():\n",
        "    print(f\"{name}: {module}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_params = [\"encoder_2\", \"encoder_3\", \"decoder_5\", \"conv5\"]"
      ],
      "metadata": {
        "id": "oCt8pJZLBB3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-tmirARyUm_"
      },
      "outputs": [],
      "source": [
        "# Load trained model parameters\n",
        "params_dir = \"/content/gdrive/MyDrive/adleo/A5/trained_model_parameters-20250403T012520Z-001/trained_model_parameters/trained_unet_final_state.pth\"\n",
        "model = load_params(params_dir,\n",
        "                    model,\n",
        "                    freeze_params=freeze_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e25_G89u2Pg"
      },
      "source": [
        "Now run the fine-tuning. You will need to add the `epochIterator` to do that, and then save the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPXjB5cazAq2"
      },
      "source": [
        "#### Doing prediction (Inference)\n",
        "\n",
        "This is the final step--making predictions on images with the fine-tuned model.\n",
        "\n",
        "First define the parameters, changes paths as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkBWYiIfXN-p"
      },
      "outputs": [],
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo/A5/prediction_scenes-20250403T012518Z-001/prediction_scenes/\"\n",
        "csv_name = \"pond_scenes_inference.csv\"\n",
        "patch_size = 256\n",
        "overlap = 28\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41v06c76XYq3"
      },
      "outputs": [],
      "source": [
        "# # If you are running the prediction at a different time with a new session\n",
        "# # then you need to initialize the model and load the parameters.\n",
        "# # OTHERWISE IGNORE THIS CELL\n",
        "# params_dir = \"/content/gdrive/MyDrive/sam/GEOG315/A5_resources/\"\\\n",
        "#     \"trained_model_parameters/trained_unet_final_state.pth\"\n",
        "\n",
        "# model = Unet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# model = load_params(params_dir, model, freeze_params=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVc00otOu2Ph"
      },
      "source": [
        "##### Prediction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiGxCr_UzCYi"
      },
      "outputs": [],
      "source": [
        "def load_data_pred(usage, csv_name, patch_size, overlap, catalog_row):\n",
        "    pred_dataset = AquacultureData(src_dir,\n",
        "                                   usage = usage,\n",
        "                                   apply_normalization=False,\n",
        "                                   csv_name = csv_name,\n",
        "                                   patch_size = patch_size,\n",
        "                                   overlap = overlap,\n",
        "                                   catalog_index=catalog_row)\n",
        "\n",
        "    data_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)\n",
        "    meta = pred_dataset.meta\n",
        "    tile = pred_dataset.tile\n",
        "\n",
        "    return data_loader, meta, tile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2gFFbW_u2Ph"
      },
      "source": [
        "##### Count tile to pass those to `predict`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.transform import from_origin\n",
        "save_dir = os.path.join(WorkingFolder, \"predictions\")\n",
        "os.makedirs(save_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
        "def save_raster(data, save_path, meta):\n",
        "    \"\"\"Saves a NumPy array as a GeoTIFF raster.\"\"\"\n",
        "    height, width = data.shape[-2:]\n",
        "    transform = from_origin(meta['transform'][2], meta['transform'][5], meta['transform'][0], meta['transform'][4]) # Assuming Affine transform\n",
        "\n",
        "    new_meta = meta.copy()\n",
        "    new_meta.update({\n",
        "        'driver': 'GTiff',\n",
        "        'height': height,\n",
        "        'width': width,\n",
        "        'transform': transform\n",
        "    })\n",
        "\n",
        "    with rasterio.open(save_path, 'w', **new_meta) as dst:\n",
        "        dst.write(data)"
      ],
      "metadata": {
        "id": "Y9qa9Ak0E7JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IAcfG34zM0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59a3452-1acc-4c3b-c834-4a0b21c57225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 88\n",
            "Patched from:\n",
            "[[128, 128], [328, 128], [528, 128], [728, 128], [928, 128], [1128, 128], [1328, 128], [1528, 128], [1728, 128], [128, 328], [328, 328], [528, 328], [728, 328], [928, 328], [1128, 328], [1328, 328], [1528, 328], [1728, 328], [128, 528], [328, 528], [528, 528], [728, 528], [928, 528], [1128, 528], [1328, 528], [1528, 528], [1728, 528], [128, 728], [328, 728], [528, 728], [728, 728], [928, 728], [1128, 728], [1328, 728], [1528, 728], [1728, 728], [128, 928], [328, 928], [528, 928], [728, 928], [928, 928], [1128, 928], [1328, 928], [1528, 928], [1728, 928], [128, 1128], [328, 1128], [528, 1128], [728, 1128], [928, 1128], [1128, 1128], [1328, 1128], [1528, 1128], [1728, 1128], [128, 1328], [328, 1328], [528, 1328], [728, 1328], [928, 1328], [1128, 1328], [1328, 1328], [1528, 1328], [1728, 1328], [328, 1528], [528, 1528], [728, 1528], [928, 1528], [1128, 1528], [1328, 1528], [1528, 1528], [1728, 1528], [328, 1728], [528, 1728], [728, 1728], [928, 1728], [1128, 1728], [1328, 1728], [1528, 1728], [1728, 1728], [928, 1928], [1128, 1928], [1328, 1928], [1528, 1928], [1728, 1928], [928, 2128], [1128, 2128], [1328, 2128], [1528, 2128]]\n",
            "--------------88 patches cropped--------------\n",
            "--------------------- Start Inference(Test) ---------------------\n",
            "---------------- Inference finished in 0:00:14 seconds ----------------\n",
            "Number of patches: 356\n",
            "Patched from:\n",
            "[[128, 128], [328, 128], [528, 128], [728, 128], [928, 128], [1128, 128], [1328, 128], [1528, 128], [1728, 128], [1928, 128], [2128, 128], [2328, 128], [2528, 128], [2728, 128], [2928, 128], [3128, 128], [3328, 128], [3528, 128], [3728, 128], [128, 328], [328, 328], [528, 328], [728, 328], [928, 328], [1128, 328], [1328, 328], [1528, 328], [1728, 328], [1928, 328], [2128, 328], [2328, 328], [2528, 328], [2728, 328], [2928, 328], [3128, 328], [3328, 328], [3528, 328], [3728, 328], [128, 528], [328, 528], [528, 528], [728, 528], [928, 528], [1128, 528], [1328, 528], [1528, 528], [1728, 528], [1928, 528], [2128, 528], [2328, 528], [2528, 528], [2728, 528], [2928, 528], [3128, 528], [3328, 528], [3528, 528], [3728, 528], [128, 728], [328, 728], [528, 728], [728, 728], [928, 728], [1128, 728], [1328, 728], [1528, 728], [1728, 728], [1928, 728], [2128, 728], [2328, 728], [2528, 728], [2728, 728], [2928, 728], [3128, 728], [3328, 728], [3528, 728], [3728, 728], [928, 928], [1128, 928], [1328, 928], [1528, 928], [1728, 928], [1928, 928], [2128, 928], [2328, 928], [2528, 928], [2728, 928], [2928, 928], [3128, 928], [3328, 928], [3528, 928], [3728, 928], [128, 1128], [328, 1128], [528, 1128], [728, 1128], [928, 1128], [1128, 1128], [1328, 1128], [1528, 1128], [1728, 1128], [1928, 1128], [2128, 1128], [2328, 1128], [2528, 1128], [2728, 1128], [2928, 1128], [3128, 1128], [3328, 1128], [3528, 1128], [3728, 1128], [128, 1328], [328, 1328], [528, 1328], [728, 1328], [928, 1328], [1128, 1328], [1328, 1328], [1528, 1328], [1728, 1328], [1928, 1328], [2128, 1328], [2328, 1328], [2528, 1328], [2728, 1328], [2928, 1328], [3128, 1328], [3328, 1328], [3528, 1328], [3728, 1328], [128, 1528], [328, 1528], [528, 1528], [728, 1528], [928, 1528], [1128, 1528], [1328, 1528], [1528, 1528], [1728, 1528], [1928, 1528], [2128, 1528], [2328, 1528], [2528, 1528], [2728, 1528], [2928, 1528], [3128, 1528], [3328, 1528], [3528, 1528], [3728, 1528], [128, 1728], [328, 1728], [528, 1728], [728, 1728], [928, 1728], [1128, 1728], [1328, 1728], [1528, 1728], [1728, 1728], [1928, 1728], [2128, 1728], [2328, 1728], [2528, 1728], [2728, 1728], [2928, 1728], [3128, 1728], [3328, 1728], [3528, 1728], [3728, 1728], [128, 1928], [328, 1928], [528, 1928], [728, 1928], [928, 1928], [1128, 1928], [1328, 1928], [1528, 1928], [1728, 1928], [1928, 1928], [2128, 1928], [2328, 1928], [2528, 1928], [2728, 1928], [2928, 1928], [3128, 1928], [3328, 1928], [3528, 1928], [3728, 1928], [128, 2128], [328, 2128], [528, 2128], [728, 2128], [928, 2128], [1128, 2128], [1328, 2128], [1528, 2128], [1728, 2128], [1928, 2128], [2128, 2128], [2328, 2128], [2528, 2128], [2728, 2128], [2928, 2128], [3128, 2128], [3328, 2128], [3528, 2128], [3728, 2128], [128, 2328], [328, 2328], [528, 2328], [728, 2328], [928, 2328], [1128, 2328], [1328, 2328], [1528, 2328], [1728, 2328], [1928, 2328], [2128, 2328], [2328, 2328], [2528, 2328], [2728, 2328], [2928, 2328], [3128, 2328], [3328, 2328], [3528, 2328], [128, 2528], [328, 2528], [528, 2528], [728, 2528], [928, 2528], [1128, 2528], [1328, 2528], [1528, 2528], [1728, 2528], [1928, 2528], [2128, 2528], [2328, 2528], [2528, 2528], [2728, 2528], [2928, 2528], [3128, 2528], [3328, 2528], [3528, 2528], [3728, 2528], [128, 2728], [328, 2728], [528, 2728], [728, 2728], [928, 2728], [1128, 2728], [1328, 2728], [1528, 2728], [1728, 2728], [1928, 2728], [2128, 2728], [2328, 2728], [2528, 2728], [2728, 2728], [2928, 2728], [3128, 2728], [3328, 2728], [3528, 2728], [3728, 2728], [128, 2928], [328, 2928], [528, 2928], [728, 2928], [928, 2928], [1128, 2928], [1328, 2928], [1528, 2928], [1728, 2928], [1928, 2928], [2128, 2928], [2328, 2928], [2528, 2928], [2728, 2928], [2928, 2928], [3128, 2928], [3328, 2928], [3528, 2928], [3728, 2928], [128, 3128], [328, 3128], [528, 3128], [728, 3128], [928, 3128], [1128, 3128], [1328, 3128], [1528, 3128], [1728, 3128], [1928, 3128], [2128, 3128], [2328, 3128], [2528, 3128], [2728, 3128], [2928, 3128], [3128, 3128], [3328, 3128], [3528, 3128], [3728, 3128], [128, 3328], [328, 3328], [528, 3328], [728, 3328], [928, 3328], [1128, 3328], [1328, 3328], [1528, 3328], [1728, 3328], [1928, 3328], [2128, 3328], [2328, 3328], [2528, 3328], [2728, 3328], [2928, 3328], [3128, 3328], [3328, 3328], [3528, 3328], [3728, 3328], [128, 3528], [328, 3528], [528, 3528], [728, 3528], [928, 3528], [1128, 3528], [1328, 3528], [1528, 3528], [1728, 3528], [1928, 3528], [2128, 3528], [2328, 3528], [2528, 3528], [2728, 3528], [2928, 3528], [3128, 3528], [3328, 3528], [3528, 3528], [3728, 3528], [128, 3728], [328, 3728], [528, 3728], [728, 3728], [928, 3728], [1128, 3728], [1328, 3728], [1528, 3728], [1728, 3728], [1928, 3728], [2128, 3728], [2328, 3728], [2528, 3728], [2728, 3728], [2928, 3728], [3128, 3728], [3328, 3728], [3528, 3728], [3728, 3728]]\n",
            "--------------356 patches cropped--------------\n",
            "--------------------- Start Inference(Test) ---------------------\n"
          ]
        }
      ],
      "source": [
        "tile_count = len(pd.read_csv(os.path.join(src_dir, csv_name)))\n",
        "for i in range(tile_count):\n",
        "    pred_data = load_data_pred(\"inference\", csv_name, patch_size, overlap, i)\n",
        "    do_prediction(pred_data, model, overlap, device, save_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "35Ks0Gqcu2PS",
        "gO0ooH42u2PU",
        "woNQhJitu2PU",
        "fVx9Lxvuu2PV",
        "-FBL7jIZu2PV",
        "EJ9MB_rru2PV",
        "IMl_cclvu2PW",
        "pwf8TVKwu2PX",
        "ReNQoRIwu2PX"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}