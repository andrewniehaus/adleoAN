{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6FkL2SjM6t"
      },
      "source": [
        "# Assignment 4 - More Semantic Segmentation"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BYkq6uiMYYOa"
      },
      "source": [
        "---\n",
        "toc: true\n",
        "toc-depth: 6\n",
        "number-sections: true\n",
        "number-depth: 6\n",
        "execute:\n",
        "  eval: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63sSTy-BjbS_"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Be aware that the code cell for `customDataset` in this template contains complete instructions.\n",
        "\n",
        "Please follow the assignment instructions in the class material for setting up, completing, and submitting your assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2OmIrBlceE"
      },
      "source": [
        "## Continue with our pipeline implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiRzE4uliyfk",
        "outputId": "ec95be27-7d67-41a5-fb22-6b745323f4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2L199Zd1mQmE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G8bnj2T6jZsM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtRCLfXlb10"
      },
      "source": [
        "### Pre-process the input dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sxDF0xfnjh2"
      },
      "source": [
        "Example code is provided below, which will allow you to get up and running with this assignment. However, you will learn best if you use the code you developed/modified from previous assignments to do the work, as you will start to see how it all fits together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRTJmfS9YYOd"
      },
      "source": [
        "#### Input normalization\n",
        "\n",
        "Add your own code for input normalization, or use the existing one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BffGjf8pKDbz"
      },
      "outputs": [],
      "source": [
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is\n",
        "        \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Gh8nIsYYOd"
      },
      "source": [
        "#### Image augmentation functions\n",
        "\n",
        "Add the functions of of your choice here, or use the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O2lQwmu7kWP_"
      },
      "outputs": [],
      "source": [
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either\n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "\n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "\n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "\n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "\n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle\n",
        "            (in degrees) respectively. If number of elements are bigger than 2,\n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "\n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2],\n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2],\n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUiTzWqYYOe"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7nb42nmkoDA"
      },
      "source": [
        "For assignment 4, you are working with a dataset called \"PondDataset\" which consists pairs of already chipped image and labels of size: `256x256` and pixel values are already in the range of `[0, 1]`.\n",
        "\n",
        "Structure of the pondDataset\n",
        "\n",
        "```\n",
        "└── PondDataset/\n",
        "    ├── train/\n",
        "    │   ├── images/\n",
        "    │   └── labels/\n",
        "    └── validate/\n",
        "        ├── images/\n",
        "        └── labels/\n",
        "```\n",
        "\n",
        "You can find the dataset in the shared drive, which is [here](https://drive.google.com/drive/folders/1hJKRa1tNQmglErELsIEk8hXEykJadmKh?usp=share_link). Please download the entire \"PondDataset\" folder and place it in a convenient locations in your own Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fj6IwWhnPOc"
      },
      "source": [
        "## Coding Assignment Part 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAvI5FGuzw42"
      },
      "source": [
        "This time you are provided with a pond dataset that is already chipped into tiles of `256x256` and the image values are already in the range `[0, 1]`.\n",
        "\n",
        "However there is no CSV file to read-in and load the files. You need to modify the `ActiveLoadingDataset` you have developed in assignment 3 so that it gets the list of required \"image\" and \"label\" files directly from the stored directory. Instead of reading from a \"csv file\", you will walk through the folder structure and grab all the \"tiff\" files for \"image\" and \"label\" folders.\n",
        "\n",
        "Further instruction is provided in the corresponding answer template.\n",
        "\n",
        "(15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBS1s04oFE1"
      },
      "source": [
        "As stated above, you can adapt the code provided below, or you can use your own loader and adapt it as needed for this assignment. In this case, you need to modify the loader so that it can read chips from a directory, rather than just reading a CSV.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB75eCtOYYOe"
      },
      "source": [
        "### Custom dataloader\n",
        "\n",
        "Add the custom dataloader from previous assignment and modify to fit the requirements of assignment 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n1SNASS_kD2u"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class ActiveLoadingDataset(Dataset):\n",
        "    def __init__(self, src_dir, dataset_name, usage, apply_normalization=False,\n",
        "                 transform=None, **kargs):\n",
        "        \"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing structured folders for image, label.\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is a string name of the transformation to be used.\n",
        "        \"\"\"\n",
        "\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "\n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "\n",
        "        # Define image and label directories\n",
        "        img_dir = Path(src_dir) / self.dataset_name / self.usage / \"images\"\n",
        "        lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "\n",
        "        # Retrieve and sort all .tif files for images and labels\n",
        "        self.img_paths = sorted([str(p) for p in img_dir.glob(\"*.tif\")])\n",
        "        self.lbl_paths = sorted([str(p) for p in lbl_dir.glob(\"*.tif\")])\n",
        "\n",
        "        self.img_chips = []\n",
        "        self.lbl_chips = []\n",
        "\n",
        "        # Load image and label data\n",
        "        for img_path, lbl_path in zip(self.img_paths, self.lbl_paths):\n",
        "            # Read image and label files\n",
        "            with rasterio.open(img_path) as img_file:\n",
        "                img = img_file.read()  # CHW format\n",
        "                if apply_normalization:\n",
        "                    img = self.min_max_normalize(img)\n",
        "\n",
        "            with rasterio.open(lbl_path) as lbl_file:\n",
        "                lbl = lbl_file.read(1)  # Assume single-channel label (HW format)\n",
        "\n",
        "            # Append to the respective lists\n",
        "            self.img_chips.append(img)  # Images remain in CHW format\n",
        "            self.lbl_chips.append(lbl)  # Labels remain in HW format\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_chips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.tensor(self.img_chips[idx], dtype=torch.float32)\n",
        "        lbl = torch.tensor(self.lbl_chips[idx], dtype=torch.long)\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            img, lbl = self.apply_transforms(img, lbl)\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "    def min_max_normalize(self, image):\n",
        "        # Normalize each channel independently\n",
        "        normalized_image = np.zeros_like(image, dtype=np.float32)\n",
        "        for i in range(image.shape[0]):\n",
        "            min_val = np.min(image[i])\n",
        "            max_val = np.max(image[i])\n",
        "            if max_val - min_val != 0:\n",
        "                normalized_image[i] = (image[i] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                normalized_image[i] = 0  # Handle cases with constant values\n",
        "        return normalized_image\n",
        "\n",
        "    def apply_transforms(self, img, lbl):\n",
        "        # Implement your transformations here based on self.transform\n",
        "        # Example transformations (you'll need to adapt these):\n",
        "        if \"hflip\" in self.transform:\n",
        "            if torch.rand(1) > 0.5:\n",
        "                img = torch.flip(img, dims=[2])\n",
        "                lbl = torch.flip(lbl, dims=[1])\n",
        "        if \"vflip\" in self.transform:\n",
        "            if torch.rand(1) > 0.5:\n",
        "                img = torch.flip(img, dims=[1])\n",
        "                lbl = torch.flip(lbl, dims=[0])\n",
        "        if \"rotate\" in self.transform:\n",
        "            angle = torch.randint(0, 4, (1,)) * 90\n",
        "            img = torch.rot90(img, k=angle.item(), dims=[1, 2])\n",
        "            lbl = torch.rot90(lbl, k=angle.item(), dims=[0, 1])\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img = torch.tensor(self.img_chips[idx], dtype=torch.float32)\n",
        "      lbl = torch.tensor(self.lbl_chips[idx], dtype=torch.long)\n",
        "\n",
        "      # Apply transformations if specified\n",
        "      if self.transform:\n",
        "          img, lbl = self.apply_transforms(img, lbl)\n",
        "\n",
        "      print(f\"Image shape: {img.shape}, Label shape: {lbl.shape}\") # <-- Add this line\n",
        "      return img, lbl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xVbJEqy2KM"
      },
      "source": [
        "#### Loading your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OKYZ7Cb9y9e4"
      },
      "outputs": [],
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo/data/\"\n",
        "dataset_name = \"PondDataset\"\n",
        "\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5dGykQgXzDl6"
      },
      "outputs": [],
      "source": [
        "train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\",\n",
        "                                     apply_normalization=False,\n",
        "                                     transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5j3ElFJWzKw8"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 4,\n",
        "                          shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WFJLUAfmzOlI"
      },
      "outputs": [],
      "source": [
        "validation_dataset = ActiveLoadingDataset(src_dir, dataset_name,\n",
        "                                          usage=\"validation\",\n",
        "                                          apply_normalization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pUMP12gmzVkT"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(validation_dataset, batch_size = 1, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Zo8L59l6y7"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP5vII5nYYOg"
      },
      "source": [
        "#### Model architecture\n",
        "Provide the U-Net architecture you developed for Assignment 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "syq2_DKrmBoL"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class UNet_AN(nn.Module):\n",
        "#     def __init__(self, n_classes, in_channels, filter_config=(64, 128, 256, 512, 1024), dropout_rate=0):\n",
        "#         \"\"\"\n",
        "#         Pytorch implementation of UNet matching the given architecture.\n",
        "\n",
        "#         Args:\n",
        "#             n_classes (int): Number of output classes.\n",
        "#             in_channels (int): Number of input image channels.\n",
        "#             filter_config (list): Feature map sizes for each level.\n",
        "#             dropout_rate (float): Dropout probability.\n",
        "\n",
        "#         Returns:\n",
        "#             Initialized pytorch model.\n",
        "#         \"\"\"\n",
        "#         super(UNet_AN, self).__init__()\n",
        "\n",
        "#         # Encoder (Downsampling)\n",
        "#         self.enc1 = self.conv_block(in_channels, filter_config[0], num_convs=2)  # 2 conv layers\n",
        "#         self.enc2 = self.conv_block(filter_config[0], filter_config[1], num_convs=2)  # 2 conv layers\n",
        "#         self.enc3 = self.conv_block(filter_config[1], filter_config[2], num_convs=2)  # 2 conv layers\n",
        "#         self.enc4 = self.conv_block(filter_config[2], filter_config[3], num_convs=3)  # 3 conv layers\n",
        "#         self.enc5 = self.conv_block(filter_config[3], filter_config[4], num_convs=3)  # 3 conv layers\n",
        "\n",
        "#         # Bottleneck\n",
        "#         self.bottleneck = self.conv_block(filter_config[4], filter_config[4], num_convs=2)  # 2 conv layers\n",
        "\n",
        "#         # Decoder (Upsampling)\n",
        "#         self.up5 = self.upconv_block(filter_config[4], filter_config[3])\n",
        "#         self.dec5 = self.conv_block(filter_config[4], filter_config[3], num_convs=3)  # 3 conv layers\n",
        "#         self.up4 = self.upconv_block(filter_config[3], filter_config[2])\n",
        "#         self.dec4 = self.conv_block(filter_config[3], filter_config[2], num_convs=3)  # 3 conv layers\n",
        "#         self.up3 = self.upconv_block(filter_config[2], filter_config[1])\n",
        "#         self.dec3 = self.conv_block(filter_config[2], filter_config[1], num_convs=2)  # 2 conv layers\n",
        "#         self.up2 = self.upconv_block(filter_config[1], filter_config[0])\n",
        "#         self.dec2 = self.conv_block(filter_config[1], filter_config[0], num_convs=2)  # 2 conv layers\n",
        "#         self.up1 = self.upconv_block(filter_config[0], filter_config[0])\n",
        "#         self.dec1 = self.conv_block(filter_config[0], filter_config[0], num_convs=2)  # 2 conv layers\n",
        "\n",
        "#         # Final Classifier\n",
        "#         self.final = nn.Conv2d(filter_config[0], n_classes, kernel_size=1)\n",
        "\n",
        "#         # Dropout\n",
        "#         self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
        "\n",
        "#     def conv_block(self, in_channels, out_channels, num_convs=2):\n",
        "#         \"\"\"Defines a convolutional block with num_convs Conv2D layers and ReLU activations.\"\"\"\n",
        "#         layers = []\n",
        "#         for _ in range(num_convs):\n",
        "#             layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
        "#             layers.append(nn.ReLU(inplace=True))\n",
        "#             in_channels = out_channels  # Maintain consistency in channels\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def upconv_block(self, in_channels, out_channels):\n",
        "#         \"\"\"Defines an upsampling layer using transposed convolution.\"\"\"\n",
        "#         return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, inputs):\n",
        "#         \"\"\"\n",
        "#         Forward pass of U-Net.\n",
        "\n",
        "#         Args:\n",
        "#             inputs (Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
        "\n",
        "#         Returns:\n",
        "#             Tensor: Output logits (batch_size, n_classes, height, width).\n",
        "#         \"\"\"\n",
        "#         # Encoder\n",
        "#         e1 = self.enc1(inputs)\n",
        "#         e2 = self.enc2(nn.MaxPool2d(kernel_size=2)(e1))\n",
        "#         e3 = self.enc3(nn.MaxPool2d(kernel_size=2)(e2))\n",
        "#         e4 = self.enc4(nn.MaxPool2d(kernel_size=2)(e3))\n",
        "#         e5 = self.enc5(nn.MaxPool2d(kernel_size=2)(e4))\n",
        "\n",
        "#         # Bottleneck\n",
        "#         b = self.bottleneck(nn.MaxPool2d(kernel_size=2)(e5))\n",
        "\n",
        "#         # Decoder\n",
        "#         d5 = self.dec5(torch.cat((self.up5(b), e5), dim=1))\n",
        "#         d4 = self.dec4(torch.cat((self.up4(d5), e4), dim=1))\n",
        "#         d3 = self.dec3(torch.cat((self.up3(d4), e3), dim=1))\n",
        "#         d2 = self.dec2(torch.cat((self.up2(d3), e2), dim=1))\n",
        "#         d1 = self.dec1(torch.cat((self.up1(d2), e1), dim=1))\n",
        "\n",
        "#         # Apply dropout and final layer\n",
        "#         output_logits = self.dropout(self.final(d1))\n",
        "\n",
        "#         return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, dropout_rate=0.0):\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    ]\n",
        "    if dropout_rate > 0:\n",
        "        layers.append(nn.Dropout2d(dropout_rate))\n",
        "    layers.extend([\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    ])\n",
        "    if dropout_rate > 0:\n",
        "        layers.append(nn.Dropout2d(dropout_rate))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_conv_blocks=2, dropout_rate=0.0):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential()\n",
        "        self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels, dropout_rate=dropout_rate))\n",
        "\n",
        "        if num_conv_blocks == 3:\n",
        "            self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "            self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "        elif num_conv_blocks == 2:\n",
        "            self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "        else:\n",
        "            raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_blocks(x)\n",
        "        pooled = self.max_pool(x)\n",
        "        return x, pooled\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        # The in_channels of conv should be in_channels + out_channels\n",
        "        # to account for the concatenation\n",
        "        self.conv = conv_block(in_channels + out_channels, out_channels, dropout_rate=dropout_rate)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config, dropout_rate=0.0):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        filters = filter_config\n",
        "\n",
        "        # Encoder Layers (Specify num_conv_blocks)\n",
        "        self.enc1 = EncoderLayer(in_channels, filters[0], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "        self.enc2 = EncoderLayer(filters[0], filters[1], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "        self.enc3 = EncoderLayer(filters[1], filters[2], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "        self.enc4 = EncoderLayer(filters[2], filters[3], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "        self.enc5 = EncoderLayer(filters[3], filters[4], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "\n",
        "        # Bottleneck (Two convolutional blocks)\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            conv_block(filters[4], filters[4], dropout_rate=dropout_rate),\n",
        "            conv_block(filters[4], filters[4], dropout_rate=dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.dec5 = DecoderLayer(filters[4], filters[3], dropout_rate=dropout_rate)\n",
        "        self.dec4 = DecoderLayer(filters[3], filters[2], dropout_rate=dropout_rate)\n",
        "        self.dec3 = DecoderLayer(filters[2], filters[1], dropout_rate=dropout_rate)\n",
        "        self.dec2 = DecoderLayer(filters[1], filters[0], dropout_rate=dropout_rate)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.final_conv = nn.Conv2d(filters[0], n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Encoder\n",
        "      enc1, pool1 = self.enc1(x)\n",
        "      enc2, pool2 = self.enc2(pool1)\n",
        "      enc3, pool3 = self.enc3(pool2)\n",
        "      enc4, pool4 = self.enc4(pool3)\n",
        "      enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "      # Bottleneck\n",
        "      bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "      # Decoder\n",
        "      dec5 = self.dec5(bottleneck, enc5)\n",
        "      dec4 = self.dec4(dec5, enc4)\n",
        "      dec3 = self.dec3(dec4, enc3)\n",
        "      dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "      # Final Convolution\n",
        "  # Final Convolution\n",
        "      out = self.final_conv(dec2)\n",
        "\n",
        "      # Add upsampling to match target size (256x256)\n",
        "      out = F.interpolate(out, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "      print(f\"Model output shape: {out.shape}\") # <-- Add this line\n",
        "      return out\n",
        "\n",
        "# # Example Usage\n",
        "# n_classes = 2\n",
        "# in_channels = 1\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)"
      ],
      "metadata": {
        "id": "Rm0spkazutah"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming you have the conv_block, EncoderLayer, DecoderLayer, and UNet classes defined as before\n",
        "\n",
        "def validate_unet(model, filter_config):\n",
        "    \"\"\"\n",
        "    Validates the structure of the UNet model.\n",
        "    \"\"\"\n",
        "    encoder_layers = []\n",
        "    decoder_layers = []\n",
        "\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, EncoderLayer):\n",
        "            encoder_layers.append(module)\n",
        "        elif isinstance(module, DecoderLayer):\n",
        "            decoder_layers.append(module)\n",
        "\n",
        "    # 1. Check the number of encoder and decoder layers\n",
        "    assert len(encoder_layers) == 5, f\"Expected 5 encoder layers, but found {len(encoder_layers)}\"\n",
        "    assert len(decoder_layers) == 4, f\"Expected 4 decoder layers, but found {len(decoder_layers)}\"\n",
        "\n",
        "    print(\"Number of encoder layers: 5 (Correct)\")\n",
        "    print(\"Number of decoder layers: 4 (Correct)\")\n",
        "\n",
        "    # 2. Check the number of convolutional blocks in each encoder layer\n",
        "    expected_conv_blocks = [2, 2, 2, 3, 3]  # Based on your specifications\n",
        "    for i, enc_layer in enumerate(encoder_layers):\n",
        "        num_conv_blocks = 0\n",
        "        for block in enc_layer.conv_blocks:\n",
        "            if isinstance(block, nn.Sequential):\n",
        "                num_conv_blocks += 1\n",
        "        assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\"\n",
        "        print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\")\n",
        "\n",
        "    # 3. Check the kernel size, stride, and padding in the convolutional blocks\n",
        "    for enc_layer in encoder_layers:\n",
        "        for conv_block_layer in enc_layer.conv_blocks:\n",
        "            if isinstance(conv_block_layer, nn.Sequential):\n",
        "                for layer in conv_block_layer:\n",
        "                    if isinstance(layer, nn.Conv2d):\n",
        "                        assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "                        assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "                        assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "    for dec_layer in decoder_layers:\n",
        "        for conv_block_layer in dec_layer.conv:\n",
        "            if isinstance(conv_block_layer, nn.Sequential):\n",
        "                for layer in conv_block_layer:\n",
        "                    if isinstance(layer, nn.Conv2d):\n",
        "                        assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "                        assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "                        assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "    print(\"Conv kernel size, stride, and padding are correct.\")\n",
        "\n",
        "    # 4. Check the max pooling kernel size and stride\n",
        "    for enc_layer in encoder_layers:\n",
        "        assert enc_layer.max_pool.kernel_size == 2, \"Max pool kernel size should be 2\"\n",
        "        assert enc_layer.max_pool.stride == 2, \"Max pool stride should be 2\"\n",
        "\n",
        "    print(\"Max pooling kernel size and stride are correct.\")\n",
        "\n",
        "    # 5. Check the number of convolutional blocks in the bottleneck\n",
        "    bottleneck_conv_blocks = 0\n",
        "    for block in model.bottleneck:\n",
        "        if isinstance(block, nn.Sequential):\n",
        "            bottleneck_conv_blocks += 1\n",
        "    assert bottleneck_conv_blocks == 2, f\"Bottleneck expected 2 conv blocks, but found {bottleneck_conv_blocks}\"\n",
        "    print(\"Bottleneck has 2 conv blocks (Correct)\")\n",
        "\n",
        "    # 6. Check the upsampling layer\n",
        "    for dec_layer in decoder_layers:\n",
        "        assert dec_layer.up.kernel_size == (2, 2), \"Upsampling kernel size should be 2x2\"\n",
        "        assert dec_layer.up.stride == (2, 2), \"Upsampling stride should be 2x2\"\n",
        "\n",
        "    print(\"Upsampling kernel size and stride are correct.\")\n",
        "\n",
        "    # 7. Check the filter configuration\n",
        "    expected_filters = filter_config\n",
        "    actual_filters = [enc_layer.conv_blocks[0][0].out_channels for enc_layer in encoder_layers]\n",
        "    assert actual_filters == expected_filters, f\"Filter configuration mismatch. Expected {expected_filters}, but got {actual_filters}\"\n",
        "    print(\"Filter configuration is correct.\")\n",
        "\n",
        "    print(\"UNet model validation successful!\")\n",
        "\n",
        "# Example Usage\n",
        "n_classes = 2\n",
        "in_channels = 1\n",
        "filter_config = [64, 128, 256, 512, 1024]\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "validate_unet(model, filter_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z6tVo7ipfsS",
        "outputId": "473ccdb5-edb3-456c-fe98-36b4b76ffe0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of encoder layers: 5 (Correct)\n",
            "Number of decoder layers: 4 (Correct)\n",
            "Encoder layer 1 has 2 conv blocks (Correct)\n",
            "Encoder layer 2 has 2 conv blocks (Correct)\n",
            "Encoder layer 3 has 2 conv blocks (Correct)\n",
            "Encoder layer 4 has 3 conv blocks (Correct)\n",
            "Encoder layer 5 has 3 conv blocks (Correct)\n",
            "Conv kernel size, stride, and padding are correct.\n",
            "Max pooling kernel size and stride are correct.\n",
            "Bottleneck has 2 conv blocks (Correct)\n",
            "Upsampling kernel size and stride are correct.\n",
            "Filter configuration is correct.\n",
            "UNet model validation successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWdmgmPIzeE-"
      },
      "source": [
        "#### Initialize your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "91DbioGYzluC"
      },
      "outputs": [],
      "source": [
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Qtiayi6AzoA-"
      },
      "outputs": [],
      "source": [
        "model = UNet(n_classes, in_channels, filter_config, dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4hMpDQ1ma0D"
      },
      "source": [
        "### Customized loss function\n",
        "\n",
        "You will want to add two here, which you can copy from the semseg_practical2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "H8fe9coXmgOh"
      },
      "outputs": [],
      "source": [
        "class BalancedCrossEntropyLoss(nn.Module):\n",
        "    '''\n",
        "    Balanced cross entropy loss by weighting of inverse class ratio\n",
        "    Params:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    '''\n",
        "\n",
        "    def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "        super(BalancedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        class_counts = torch.bincount(target.view(-1),\n",
        "                                      minlength=predict.shape[1])\n",
        "        class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "        # set weight of ignore index to 0\n",
        "        if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "            class_weights[self.ignore_index] = 0\n",
        "\n",
        "        # normalize weights\n",
        "        class_weights /= torch.sum(class_weights)\n",
        "\n",
        "        # apply class weights to loss function\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=class_weights,\n",
        "                                      ignore_index=self.ignore_index,\n",
        "                                      reduction=self.reduction)\n",
        "\n",
        "        return loss_fn(predict, target)\n",
        "\n",
        "\n",
        "class BinaryTverskyFocalLoss(nn.Module):\n",
        "    '''\n",
        "    Pytorch version of tversky focal loss proposed in paper\n",
        "    'A novel focal Tversky loss function and improved Attention U-Net for lesion\n",
        "    segmentation'\n",
        "    (https://arxiv.org/abs/1810.07842)\n",
        "    Params:\n",
        "        smooth (float): A float number to smooth loss, and avoid NaN error,\n",
        "            default: 1\n",
        "        alpha (float): Hyperparameters alpha, paired with (1 - alpha) to shift\n",
        "            emphasis to improve recall\n",
        "        gamma (float): Tversky index, default: 1.33\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same\n",
        "            shape with predict\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    '''\n",
        "\n",
        "    def __init__(self, smooth=1, alpha=0.7, gamma=1.33):\n",
        "        super(BinaryTverskyFocalLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.alpha = alpha\n",
        "        self.beta = 1 - self.alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \\\n",
        "            \"predict & target batch size do not match.\"\n",
        "\n",
        "        # Instead of interpolating individual channels, interpolate the whole prediction\n",
        "        # predict has shape (batch_size, n_classes, height, width)\n",
        "        # target has shape (batch_size, height, width)\n",
        "        # Ensure predict has at least 2 spatial dimensions before interpolation\n",
        "        if len(predict.shape) < 4:  # Check if spatial dimensions are missing\n",
        "            predict = predict.unsqueeze(1) # Add a spatial dimension\n",
        "\n",
        "        predict = F.interpolate(predict, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Reshape predict and target for element-wise operations\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        # Compute the numerator and denominator for the Tversky index\n",
        "        num = (predict * target).sum() + self.smooth\n",
        "        den = (predict * target).sum() + self.alpha * ((1 - predict) * target)\\\n",
        "            .sum() + self.beta * (predict * (1 - target)).sum() + self.smooth\n",
        "\n",
        "        # Compute the Tversky index and the loss\n",
        "        tversky = 1 - (num / den)  # Modified calculation\n",
        "        loss = torch.pow(tversky, 1 / self.gamma)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TverskyFocalLoss(nn.Module):\n",
        "    '''\n",
        "    Tversky focal loss\n",
        "    Params:\n",
        "        weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same\n",
        "            shape with predict\n",
        "        other args pass to BinaryTverskyFocalLoss\n",
        "    Returns:\n",
        "        same as BinaryTverskyFocalLoss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
        "        super(TverskyFocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        nclass = predict.shape[1]  # Number of classes in the prediction\n",
        "\n",
        "        # Instead of one-hot encoding the target, we ensure that the target\n",
        "        # has the same shape as the prediction's spatial dimensions but only\n",
        "        # one channel\n",
        "        if len(predict.shape) == 4:\n",
        "            # Ensure target has the same spatial dimensions as prediction but only 1 channel\n",
        "            target = target.unsqueeze(1)  # Add a channel dimension\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        tversky = BinaryTverskyFocalLoss(**self.kwargs)\n",
        "        total_loss = 0\n",
        "        weight = torch.Tensor([1. / nclass] * nclass).cuda() if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)  # Apply softmax along the channel dimension\n",
        "\n",
        "        for i in range(nclass):\n",
        "            if i != self.ignore_index:\n",
        "                # Select the corresponding channel from prediction and target\n",
        "                # predict[:, i] will have shape (batch_size, height, width)\n",
        "                # target[:, 0] will also have shape (batch_size, height, width) because we added a channel dim and select the first\n",
        "\n",
        "                # Check if predict[:, i] has fewer than 3 dimensions (batch size, height, width).\n",
        "                # If so, it indicates missing spatial dimensions.\n",
        "                if len(predict[:, i].shape) < 3:\n",
        "                  # Add a spatial dimension\n",
        "                  predict_channel = predict[:, i].unsqueeze(1)\n",
        "                else:\n",
        "                  predict_channel = predict[:, i]\n",
        "\n",
        "                tversky_loss = tversky(predict_channel, target[:, 0])\n",
        "                assert weight.shape[0] == nclass, \\\n",
        "                    'Expect weight shape [{}], get[{}]'.format(nclass, weight.shape[0])\n",
        "                tversky_loss *= weight[i]\n",
        "                total_loss += tversky_loss\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M9vkOj7AT86"
      },
      "source": [
        "## Coding assignment part 2: Training the network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh13sfNJvDJy"
      },
      "source": [
        "\n",
        "In the sections below you need to complete functions that you need to train and validate the network over a specified number of epochs\n",
        "\n",
        "Complete the training process. To do that you need to complete three functions.\n",
        "\n",
        "1. A function to perform one epoch on the training dataset.\n",
        "\n",
        "2. A function to perform one epoch on the validation dataset.\n",
        "\n",
        "3. A function to iterate over the user-defined number of epochs\n",
        "\n",
        "Develop this code and train/validate the model two times, each with a different loss function.\n",
        "\n",
        "**Note:**\n",
        "Detailed information is provided in the assignment practical.\n",
        "\n",
        "(25 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWRrySwHBBT4"
      },
      "source": [
        "**Tip:**\n",
        "\n",
        "Q. How do you properly use the \"criterion\" argument inside the \"train\" and \"validation\" functions?\n",
        "\n",
        "A. Pass the argument to the function as a string with `()` like: \"BalancedCrossEntropyLoss()\". Then, when it comes to using the argument inside both the `train` and `validation` functions, use `eval()` like this:\n",
        "\n",
        "`loss = eval(criterion)(tensor A, tensor B)`\n",
        "\n",
        "`eval()` is a built-in Python function that allows you to evaluate a string expression as a Python code. It takes a string as an argument and evaluates the expression contained in it. The result of the evaluation is then returned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o68VvDTFYYOr"
      },
      "source": [
        "### Complete the `train` function\n",
        "\n",
        "Complete the function to optimize over a batch of training images and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "g-AEldksAikB"
      },
      "outputs": [],
      "source": [
        "def train(trainData, model, optimizer, criterion, gpu=True, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset (AquacultureData).\n",
        "            model - Choice of segmentation model.\n",
        "            optimizer - Chosen optimization algorithm to update model parameters.\n",
        "            criterion - Chosen function to calculate loss over training samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- List to store training loss values.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu') # Define device\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        # Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "        img_chips = img_chips.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        outputs = model(img_chips)\n",
        "\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        # do the backward pass to calculate gradients with respect to the loss (1 line)\n",
        "        loss.backward()\n",
        "\n",
        "        # update model weights by invoking the proper method on 'optimizer' (1 line)\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQWD_wTs6lT"
      },
      "source": [
        "### Complete the `validation` function\n",
        "Besides training the network, it's important to evaluate its performance on a separate \"validation dataset\" to ensure that it's not overfitting to the training data. The validation process is similar to the training process, but the network is set to evaluation mode using `model.eval()` and the gradients are not computed.\n",
        "\n",
        "Complete the function to process validation images and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "l2VxqLHiBDFi"
      },
      "outputs": [],
      "source": [
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation\n",
        "                samples.\n",
        "            device -- The device to use (e.g., 'cuda' or 'cpu').\n",
        "            val_loss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for img_chips, labels in valData:\n",
        "\n",
        "            img = Variable(img_chips, requires_grad=False)\n",
        "            label = Variable(labels, requires_grad=False)\n",
        "\n",
        "            # Add code to put image and label on the 'device'.\n",
        "            # one line for each.\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Pass image through the model to obtain prediction (1 line)\n",
        "            outputs = model(img)\n",
        "\n",
        "            # calculate loss based on 'model prediction' and label (1 line)\n",
        "            loss = criterion(outputs, label)\n",
        "\n",
        "            # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "            eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {:.4f}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss is not None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niW-SEDiYYOs"
      },
      "source": [
        "### Complete the epochIterator\n",
        "\n",
        "Complete the function that iterate over the desired number of epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "TQFVUtNtD2s_"
      },
      "outputs": [],
      "source": [
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "                  initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "\n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        gpu = True\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        gpu = False\n",
        "        model = model\n",
        "\n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                                    lr=initial_lr,\n",
        "                                    betas=(0.9, 0.999),\n",
        "                                    eps=1e-08,\n",
        "                                    weight_decay=5e-4,\n",
        "                                    amsgrad=False)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                            step_size=3,\n",
        "                                            gamma=0.98)\n",
        "\n",
        "    # Add your code here\n",
        "    # you need to loop through the epochs and perform the following:\n",
        "    # print the current epoch number out of the total epochs\n",
        "    # (e.g. \"epoch: 2/10\")(1 line)\n",
        "    # start the timer (1 line)\n",
        "    # do model fit on the training data for single epoch (1 line)\n",
        "    # do model validation on the validation dataset for one epoch (1 line)\n",
        "    # take a step to update the 'scheduler'. (1 line)\n",
        "    # Print the updated learning rate.\n",
        "    # use \"add_scalars\" method with your writer to save the train and validation\n",
        "    # loss to graph\n",
        "    # using tensorboard package later.\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
        "        start_epoch = datetime.now()\n",
        "\n",
        "        # Training\n",
        "        train(trainData, model, optimizer, criterion, gpu=gpu, train_loss=train_loss)\n",
        "\n",
        "        # Validation\n",
        "        validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "        writer.add_scalars('Loss', {'train': train_loss[-1], 'val': val_loss[-1]}, epoch)\n",
        "\n",
        "        duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "        duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "        print(\"Epoch finished in {}\".format(duration_format))\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\".format(duration_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pG9Ni2kwKC5"
      },
      "source": [
        "### Demonstrate the code\n",
        "Run the model training and validation for a specified number of epochs (e.g. 15), and then save the results. Train / validate twice, once using your first loss function, and again using your second loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l95gsFgyYYOs"
      },
      "source": [
        "#### Train/validate model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "cellView": "form",
        "id": "wq1-0zAKyLQz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# # Train/validate 1\n",
        "# n_classes = 2  # Number of segmentation classes (adjust as needed)\n",
        "# in_channels = 6  # Number of input channels (adjust as needed)\n",
        "# filter_config = [64, 128, 256, 512, 1024]  # Adjust filter sizes as needed\n",
        "# dropout_rate = 0.5  # Adjust dropout rate as needed\n",
        "# initial_lr = 0.001  # Adjust learning rate as needed\n",
        "# num_epochs = 10  # Adjust number of epochs as needed\n",
        "# working_folder = \"./runs\" # Adjust working folder for TensorBoard logs\n",
        "\n",
        "# model1 = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# criterion = BalancedCrossEntropyLoss()\n",
        "\n",
        "# epochIterater(train_loader, val_loader, model, criterion, working_folder, initial_lr, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rzFpzoYYOt"
      },
      "source": [
        "Save model 1 in a directory of choice in your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "bLd8KNWID22z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dc9ff3-775a-4578-cd07-377da5504996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model results saved to: /content/gdrive/MyDrive/adleo/data/model1results\n"
          ]
        }
      ],
      "source": [
        "# Save model results 1\n",
        "results_folder = Path(src_dir) / \"model1results\"  # Create a Path object for the results folder\n",
        "results_folder.mkdir(parents=True, exist_ok=True)\n",
        "model_save_path = results_folder / \"model_weights.pth\"  # Define the save path\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model results saved to: {results_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm_6dELFYYOt"
      },
      "source": [
        "#### Train/validate model 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train/validate 2\n",
        "# n_classes = 2  # Number of segmentation classes (adjust as needed)\n",
        "# in_channels = 6  # Number of input channels (adjust as needed)\n",
        "# filter_config = [64, 128, 256, 512, 1024]  # Adjust filter sizes as needed\n",
        "# dropout_rate = 0.5  # Adjust dropout rate as needed\n",
        "# initial_lr = 0.001  # Adjust learning rate as needed\n",
        "# num_epochs = 10  # Adjust number of epochs as needed\n",
        "# working_folder = \"./runs\" # Adjust working folder for TensorBoard logs\n",
        "\n",
        "# model2 = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# criterion = TverskyFocalLoss()\n",
        "\n",
        "# epochIterater(train_loader, val_loader, model, criterion, working_folder, initial_lr, num_epochs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pT2kGUXxoLRk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYboOH_4YYOt"
      },
      "source": [
        "Save model results 2 in a directory of choice in your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "id": "kyrDM0hqyeMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a957c7fc-fb41-4f32-f1a9-ca46ecd06f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model results saved to: /content/gdrive/MyDrive/adleo/data/model2results\n"
          ]
        }
      ],
      "source": [
        "# Save model results 2\n",
        "results_folder = Path(src_dir) / \"model2results\"  # Create a Path object for the results folder\n",
        "results_folder.mkdir(parents=True, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "model_save_path = results_folder / \"model_weights.pth\"  # Define the save path\n",
        "torch.save(model.state_dict(), model_save_path)  # Save the model's state dictionary\n",
        "print(f\"Model results saved to: {results_folder}\")  # Print the save location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCaNlvonXp4"
      },
      "source": [
        "## Evaluation and accuracy metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey2Ep2LAoRCX"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "If you have disconnected from the Colab session or restarted the kernel, then before doing the evaluation on the validation dataset you must initialize your model once more and load the trained weights onto your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "MLovzg7NngXX"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) /\\\n",
        "            np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kux73Vkuy6bn"
      },
      "source": [
        "## Coding Assignment Part 3\n",
        "\n",
        "Modify `do_accuracy_evaluation` to work with the `Evaluator` class to calculate the overal metrics for a validation dataset. More info on the specification of the function can be found in the template. Complete the code to undertake model evaluation below. Evaluate twice: once for each model trained with a different loss function.\n",
        "\n",
        "(10 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBO97e1_YYOu"
      },
      "source": [
        "### Add the code for evaluation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QYZ4CIs5no54"
      },
      "outputs": [],
      "source": [
        "def do_accuracy_evaluation(model, dataloader, num_classes, filename):\n",
        "    \"\"\"\n",
        "    Evaluates the model's accuracy metrics on the given dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: The trained PyTorch model.\n",
        "        dataloader: The DataLoader for the evaluation dataset.\n",
        "        num_classes: The number of segmentation classes.\n",
        "        filename: The filename to save the evaluation results.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)  # Move the model to the appropriate device\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    evaluator = Evaluator(num_classes)  # Initialize the Evaluator\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for img_chips, labels in dataloader:\n",
        "            img_chips = img_chips.to(device)  # Move data to device\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(img_chips)  # Get model predictions\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
        "\n",
        "            # Convert tensors to NumPy arrays and move to CPU\n",
        "            predicted = predicted.cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            evaluator.add_batch(labels, predicted)  # Update the confusion matrix\n",
        "\n",
        "    # Calculate accuracy metrics\n",
        "    acc = evaluator.Pixel_Accuracy()\n",
        "    acc_class = evaluator.Pixel_Accuracy_Class()\n",
        "    mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "    FWIoU = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    # Print and save the results\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Accuracy Class:\", acc_class)\n",
        "    print(\"Mean IoU:\", mIoU)\n",
        "    print(\"Frequency Weighted IoU:\", FWIoU)\n",
        "\n",
        "    with open(filename, 'w') as f:  # Save results to a file\n",
        "        f.write(f\"Accuracy: {acc}\\n\")\n",
        "        f.write(f\"Accuracy Class: {acc_class}\\n\")\n",
        "        f.write(f\"Mean IoU: {mIoU}\\n\")\n",
        "        f.write(f\"Frequency Weighted IoU: {FWIoU}\\n\")\n",
        "\n",
        "    print(f\"Evaluation results saved to: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_folder1 = Path(src_dir) / \"model1results\"\n",
        "results_folder2 = Path(src_dir) / \"model2results\"\n",
        "model_save_path1 = results_folder1 / \"model_weights.pth\"\n",
        "model_save_path2 = results_folder2 / \"model_weights.pth\""
      ],
      "metadata": {
        "id": "Okf3SoVDvqVt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zDq6fPKZvZ-U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGXm-O8YYOu"
      },
      "source": [
        "### Evaluate model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "W4WK7HeazNVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e5d8b52f-e407-44f2-c36f-70835b40b865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Accuracy: 0.8001712375217014\n",
            "Accuracy Class: 0.5\n",
            "Mean IoU: 0.4000856187608507\n",
            "Frequency Weighted IoU: 0.6402740093570111\n",
            "Evaluation results saved to: evaluation_results_model1.txt\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate evaluation of model 1\n",
        "do_accuracy_evaluation(model1, val_loader, num_classes=n_classes, filename=\"evaluation_results_model1.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz-_jyK7YYOu"
      },
      "source": [
        "### Evaluate model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "cellView": "form",
        "id": "7wKsdVoXzWdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8af765e3-5859-4701-c5be-7ea75524fbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Image shape: torch.Size([6, 256, 256]), Label shape: torch.Size([256, 256])\n",
            "Model output shape: torch.Size([1, 2, 256, 256])\n",
            "Accuracy: 0.20241902669270834\n",
            "Accuracy Class: 0.5003117219848026\n",
            "Mean IoU: 0.10194638331263293\n",
            "Frequency Weighted IoU: 0.04320717815967741\n",
            "Evaluation results saved to: evaluation_results_model2.txt\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate evaluation of model 2\n",
        "do_accuracy_evaluation(model2, val_loader, num_classes=n_classes, filename=\"evaluation_results_model2.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI Conversations"
      ],
      "metadata": {
        "id": "AdTI0FZ-yL_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ###### My conversations with Gemini are pasted below. I had a bit of trouble with the UNet architecture to get input/output dimensions to match up through the whole workflow and don't have particularly great accuracy metrics as I am writing this. I will be trying to fine tune my implementations to run better but will upload an early version of this assignment as is and replace it if I manage to get better results.\n",
        "# please consider the UNET architecture diagram I attach below and explain to me each aspect of the model I would need to make sure I have right in coding it.\n",
        "\n",
        "\n",
        "\n",
        "# Absolutely! Let's break down the UNet architecture diagram you provided and discuss the key aspects you need to get right when coding it.\n",
        "\n",
        "# Understanding the UNet Architecture\n",
        "\n",
        "# The UNet architecture is a popular convolutional neural network (CNN) designed for biomedical image segmentation. It's known for its U-shaped structure, consisting of a contracting (encoder) path and an expansive (decoder) path. The key to its success lies in the skip connections that bridge these two paths, allowing the network to combine high-level semantic information with low-level detailed information.\n",
        "\n",
        "# Key Aspects to Focus On When Coding UNet\n",
        "\n",
        "# Input and Output Shapes:\n",
        "\n",
        "# Input: The diagram shows the input shape as BxC1xHxW, where:\n",
        "\n",
        "# B: Batch size (number of images processed together)\n",
        "# C1: Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
        "# H: Height of the input image\n",
        "# W: Width of the input image\n",
        "# Output: The output shape is BxC2xHxW, where:\n",
        "\n",
        "# C2: Number of output channels (number of segmentation classes). For binary segmentation (object vs. background), C2 would be 2 (or 1 with sigmoid activation). For multi-class segmentation, C2 would be the number of classes.\n",
        "# Coding Implication: Ensure your input data and output predictions match these shapes.\n",
        "\n",
        "# Contracting (Encoder) Path:\n",
        "\n",
        "# Conv Blocks: The encoder path consists of a series of convolutional blocks. Each block typically includes:\n",
        "\n",
        "# Convolutional Layers (Conv3x3): Apply filters to extract features.\n",
        "# Batch Normalization (BN): Normalize activations for stable training.\n",
        "# ReLU Activation: Introduce non-linearity.\n",
        "# Max Pooling: Max pooling layers (MaxPool 2x2) reduce the spatial dimensions (height and width) of the feature maps, capturing more abstract features.\n",
        "\n",
        "# K = 2 and s = 2 indicate a kernel size of 2x2 and a stride of 2, effectively halving the spatial dimensions.\n",
        "# Coding Implication:\n",
        "\n",
        "# Implement the convolutional blocks using the specified layers (Conv2D, BatchNorm2d, ReLU in PyTorch, or similar in other frameworks).\n",
        "# Use MaxPool2D with the correct kernel size and stride for downsampling.\n",
        "# Bottleneck:\n",
        "\n",
        "# The bottleneck is the lowest point in the architecture, where the feature maps have the smallest spatial dimensions and the highest number of channels. It's crucial for capturing the most abstract representations of the input.\n",
        "# Coding Implication:\n",
        "\n",
        "# Ensure the bottleneck layer has a sufficient number of channels to capture the complexity of the data.\n",
        "# Expansive (Decoder) Path:\n",
        "\n",
        "# Upsampling: The decoder path upsamples the feature maps to restore the original spatial dimensions. The diagram shows Upsampling layers. You can use:\n",
        "\n",
        "# Transpose Convolution (ConvTranspose2d): Learns to upsample the feature maps.\n",
        "# Bilinear Upsampling: Simple interpolation to increase spatial dimensions.\n",
        "# Conv Blocks: Similar to the encoder, the decoder also uses convolutional blocks to refine the upsampled features.\n",
        "\n",
        "# Coding Implication:\n",
        "\n",
        "# Choose an appropriate upsampling method (ConvTranspose2d is often preferred).\n",
        "# Implement the convolutional blocks with the same structure as in the encoder.\n",
        "# Skip Connections (Concatenation):\n",
        "\n",
        "# Concatenation: The skip connections are crucial for combining high-level features from the encoder with low-level features from the decoder. This is done through concatenation.\n",
        "# Coding Implication:\n",
        "\n",
        "# Before each upsampling step in the decoder, concatenate the corresponding feature map from the encoder with the upsampled feature map.\n",
        "# Ensure the feature maps have the same height and width before concatenation.\n",
        "# Final Convolution (Conv 1x1):\n",
        "\n",
        "# Conv 1x1: A 1x1 convolution is used at the end to map the feature maps to the desired number of output channels (segmentation classes).\n",
        "# Coding Implication:\n",
        "\n",
        "# Use a 1x1 convolution to produce the final segmentation map.\n",
        "# Classifier:\n",
        "\n",
        "# The diagram shows a \"Classifier\" block after the final convolution. This usually refers to the activation function applied to the output.\n",
        "# Coding Implication:\n",
        "\n",
        "# Sigmoid Activation: For binary segmentation (one object class), use a sigmoid activation to produce a probability map.\n",
        "# Softmax Activation: For multi-class segmentation, use a softmax activation to produce a probability distribution over the classes.\n",
        "# Number of Filters (Channels):\n",
        "\n",
        "# The diagram shows D1, D2, D3, etc., representing the number of filters (channels) in each layer. You'll need to decide on the specific values for these based on your dataset and computational resources.\n",
        "# Coding Implication:\n",
        "\n",
        "# Experiment with different filter sizes to find the best configuration for your task.\n",
        "# Loss Function:\n",
        "\n",
        "# Choose an appropriate loss function for segmentation. Common choices include:\n",
        "# Binary Cross-Entropy Loss: For binary segmentation.\n",
        "# Categorical Cross-Entropy Loss: For multi-class segmentation.\n",
        "# Dice Loss: Measures the overlap between the predicted and ground truth segmentation masks.\n",
        "# Coding Implication:\n",
        "\n",
        "# Implement the chosen loss function correctly.\n",
        "# Optimizer and Learning Rate:\n",
        "\n",
        "# Select an optimizer (e.g., Adam, SGD) and a learning rate for training the model.\n",
        "# Coding Implication:\n",
        "\n",
        "# Experiment with different optimizers and learning rates to find the best configuration for your task.\n",
        "# Example Code Snippet (PyTorch)\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=2):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         # Encoder\n",
        "#         self.enc1 = self.conv_block(in_channels, 64)\n",
        "#         self.enc2 = self.conv_block(64, 128)\n",
        "#         self.enc3 = self.conv_block(128, 256)\n",
        "#         self.enc4 = self.conv_block(256, 512)\n",
        "#         self.enc5 = self.conv_block(512, 1024)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         self.bottleneck = self.conv_block(1024, 1024)\n",
        "\n",
        "#         # Decoder\n",
        "#         self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "#         self.dec4 = self.conv_block(1024, 512)\n",
        "#         self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "#         self.dec3 = self.conv_block(512, 256)\n",
        "#         self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "#         self.dec2 = self.conv_block(256, 128)\n",
        "#         self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "#         self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "#     def conv_block(self, in_channels, out_channels):\n",
        "#         return nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         enc1 = self.enc1(x)\n",
        "#         enc2 = self.enc2(nn.MaxPool2d(2)(enc1))\n",
        "#         enc3 = self.enc3(nn.MaxPool2d(2)(enc2))\n",
        "#         enc4 = self.enc4(nn.MaxPool2d(2)(enc3))\n",
        "#         enc5 = self.enc5(nn.MaxPool2d(2)(enc4))\n",
        "\n",
        "#         # Bottleneck\n",
        "#         bottleneck = self.bottleneck(nn.MaxPool2d(2)(enc5))\n",
        "\n",
        "#         # Decoder\n",
        "#         dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))\n",
        "#         dec3 = self.dec3(torch.cat([self.up3(dec4), enc3],\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# once I have a Unet structure created, how can I test it to make sure all of the relevant parameters and structures match the diagram i gave you\n",
        "\n",
        "\n",
        "# Excellent question! Thorough testing is crucial to ensure your UNet implementation matches the diagram and functions correctly. Here's a systematic approach to testing your UNet structure:\n",
        "\n",
        "# 1. Shape Verification at Each Layer:\n",
        "\n",
        "# Forward Pass with Dummy Data: Create a dummy input tensor with the expected shape (BxC1xHxW). Pass it through your UNet model and track the shape of the output tensor at each layer.\n",
        "# Compare with Diagram: Compare the shapes you obtain during the forward pass with the output shapes specified in the diagram (BxD1xHxW, BxD1xH/2xW/2, etc.).\n",
        "# Automated Assertions: Use assertions in your testing code to automatically check if the shapes match the expected values. This will catch errors early.\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "\n",
        "# # Assuming 'model' is your UNet instance\n",
        "# model = UNet(in_channels=1, out_channels=2)  # Adjust in_channels and out_channels as needed\n",
        "\n",
        "# # Dummy input\n",
        "# batch_size = 2\n",
        "# in_channels = 1\n",
        "# height = 256\n",
        "# width = 256\n",
        "# dummy_input = torch.randn(batch_size, in_channels, height, width)\n",
        "\n",
        "# # Forward pass\n",
        "# outputs = model(dummy_input)\n",
        "\n",
        "# # Shape verification (example)\n",
        "# assert outputs.shape == (batch_size, 2, height, width), \"Output shape mismatch\"\n",
        "\n",
        "# # Add assertions for other layers (e.g., encoder, bottleneck, decoder)\n",
        "# # You can add print statements to check intermediate shapes if needed\n",
        "# 2. Parameter Count Verification:\n",
        "\n",
        "# Calculate Expected Parameters: Manually calculate the number of parameters for each layer based on the kernel sizes, number of input and output channels, and whether there's a bias term.\n",
        "# Retrieve Actual Parameters: Use model.parameters() to retrieve the parameters of your model and count them.\n",
        "# Compare and Assert: Compare the calculated and actual parameter counts. Use assertions to ensure they match.\n",
        "# Python\n",
        "\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# # Calculate expected parameters (example for a Conv2d layer)\n",
        "# kernel_size = 3\n",
        "# in_channels = 64\n",
        "# out_channels = 128\n",
        "# expected_conv_params = kernel_size * kernel_size * in_channels * out_channels + out_channels  # + out_channels for bias\n",
        "\n",
        "# # Get actual parameters\n",
        "# actual_params = count_parameters(model)\n",
        "\n",
        "# # Assertions (you'll need to calculate expected parameters for each layer)\n",
        "# # assert actual_params == expected_total_params, \"Total parameter count mismatch\"\n",
        "# 3. Layer Type and Configuration Verification:\n",
        "\n",
        "# Inspect Layers: Iterate through the layers of your model and check their types and configurations (e.g., kernel size, stride, padding).\n",
        "# Compare with Diagram: Compare the layer types and configurations with those specified in the diagram.\n",
        "# Assertions: Use assertions to ensure the layer types and configurations are correct.\n",
        "# Python\n",
        "\n",
        "# # Example: Checking the first Conv2d layer\n",
        "# first_conv_layer = model.enc1[0]  # Assuming enc1 is a Sequential container\n",
        "\n",
        "# assert isinstance(first_conv_layer, nn.Conv2d), \"First layer should be Conv2d\"\n",
        "# assert first_conv_layer.kernel_size == (3, 3), \"Kernel size should be 3x3\"\n",
        "# assert first_conv_layer.padding == (1, 1), \"Padding should be 1\"\n",
        "# 4. Skip Connection Verification:\n",
        "\n",
        "# Track Tensor Flow: Carefully track the tensors during the forward pass, especially at the concatenation points.\n",
        "# Verify Concatenation: Ensure that the tensors being concatenated have the same spatial dimensions and that the concatenation is performed along the correct axis (usually the channel axis).\n",
        "# Assertions: Use assertions to check the shapes of the tensors before and after concatenation.\n",
        "# Python\n",
        "\n",
        "# # Example: Checking skip connection in the decoder\n",
        "# # ... (forward pass code)\n",
        "# assert dec4_input.shape[2:] == enc4.shape[2:], \"Skip connection shapes mismatch\"\n",
        "# 5. Activation Function Verification:\n",
        "\n",
        "# Inspect Activation Layers: Check the activation functions used in each layer (e.g., ReLU, Sigmoid, Softmax).\n",
        "# Compare with Diagram: Verify that the activation functions match those specified in the diagram or your intended design.\n",
        "# Assertions: Use assertions to check the types of activation layers.\n",
        "# Python\n",
        "\n",
        "# # Example: Checking ReLU activation in a Conv block\n",
        "# relu_layer = model.enc1[2]  # Assuming ReLU is the third layer in enc1\n",
        "# assert isinstance(relu_layer, nn.ReLU), \"Activation should be ReLU\"\n",
        "# 6. Upsampling Method Verification:\n",
        "\n",
        "# Inspect Upsampling Layers: Check the type of upsampling layer used (e.g., ConvTranspose2d, nn.Upsample).\n",
        "# Compare with Diagram: Verify that the upsampling method matches the diagram or your intended design.\n",
        "# Assertions: Use assertions to check the type of upsampling layer.\n",
        "# Python\n",
        "\n",
        "# # Example: Checking Upsample layer\n",
        "# upsample_layer = model.up4\n",
        "# assert isinstance(upsample_layer, nn.ConvTranspose2d), \"Upsampling should be ConvTranspose2d\"\n",
        "# 7. Bottleneck Verification:\n",
        "\n",
        "# Inspect Bottleneck Layer: Check the number of channels in the bottleneck layer.\n",
        "# Compare with Diagram: Verify that the number of channels matches the diagram or your intended design.\n",
        "# Assertions: Use assertions to check the number of channels.\n",
        "# Python\n",
        "\n",
        "# # Example: Checking bottleneck channels\n",
        "# bottleneck_channels = model.bottleneck[0].out_channels  # Assuming the first layer in bottleneck is Conv2d\n",
        "# assert bottleneck_channels == 1024, \"Bottleneck channels mismatch\"\n",
        "# 8. Gradient Flow Verification (Optional):\n",
        "\n",
        "# Backpropagation Check: Perform a forward pass and backpropagation with a dummy loss to ensure that gradients are flowing through the network.\n",
        "# Inspect Gradients: Check the gradients of the parameters to ensure they are not zero or NaN.\n",
        "# Debugging: Use tools like torch.autograd.gradcheck to verify the correctness of gradients.\n",
        "# Testing Frameworks:\n",
        "\n",
        "# PyTest or Unitest: Use testing frameworks like PyTest or Unittest to organize your tests and run them automatically.\n",
        "# Coverage Reports: Generate coverage reports to ensure that all parts of your UNet implementation are being tested.\n",
        "# By systematically applying these testing techniques, you can gain confidence that your UNet implementation matches the diagram and functions correctly. Remember to adapt the assertions and checks to match the specific details of your UNet architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# okay id like to go a bit more slowly and methodically through coding this architecture. Ill ask you for specific aspects of the model and we'll fill it out until its complete\n",
        "\n",
        "\n",
        "# Great! Let's take it step by step. To start, let's focus on the input and output structure of the UNet based on the diagram.\n",
        "\n",
        "# Question 1: Input and Output Shapes\n",
        "\n",
        "# Based on the diagram, what are the following:\n",
        "\n",
        "# Input Shape (Symbolically): What is the general shape of the input to the network, and what do the symbols represent?\n",
        "# Output Shape (Symbolically): What is the general shape of the output from the network, and what do the symbols represent?\n",
        "# Let's start there. Once we clarify the input and output, we can move on to the internal layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# the model should be able to handle different image dimensions. lets start by making the convolutional block. Please give me the code for a convolutional block making sure it has two convolutions, uses batch normalization, uses the relu activation function, has a 3x3 convolution window, a padding of 1, and a stride of 1\n",
        "\n",
        "\n",
        "\n",
        "# All right, let's create the convolutional block. We'll use PyTorch for this example. Here's the code for a convolutional block with the specifications you provided:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels):\n",
        "#     \"\"\"\n",
        "#     Creates a convolutional block with two 3x3 convolutions,\n",
        "#     batch normalization, and ReLU activation.\n",
        "\n",
        "#     Args:\n",
        "#         in_channels (int): Number of input channels.\n",
        "#         out_channels (int): Number of output channels.\n",
        "\n",
        "#     Returns:\n",
        "#         nn.Sequential: A sequential container of layers representing the conv block.\n",
        "#     \"\"\"\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# # Example usage:\n",
        "# # Create a convolutional block that takes 3 input channels and produces 64 output channels\n",
        "# example_block = conv_block(3, 64)\n",
        "# print(example_block)\n",
        "# Explanation:\n",
        "\n",
        "# nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1):\n",
        "\n",
        "# This creates a 2D convolutional layer.\n",
        "# in_channels: Number of input channels.\n",
        "# out_channels: Number of output channels (number of filters).\n",
        "# kernel_size=3: 3x3 convolution window.\n",
        "# padding=1: Adds padding of 1 pixel to the input, ensuring that the output feature map has the same spatial dimensions as the input.\n",
        "# stride=1: The filter moves one pixel at a time.\n",
        "# nn.BatchNorm2d(out_channels):\n",
        "\n",
        "# Applies batch normalization to the output of the convolutional layer.\n",
        "# out_channels: The number of features expected in the input.\n",
        "# nn.ReLU(inplace=True):\n",
        "\n",
        "# Applies the ReLU activation function.\n",
        "# inplace=True: Modifies the input directly, saving memory.\n",
        "# nn.Sequential(...):\n",
        "\n",
        "# Creates a sequential container that applies the layers in the order they are passed.\n",
        "# This makes it easy to create a reusable convolutional block.\n",
        "# To use this block in your UNet, you would simply call it with the input tensor:\n",
        "\n",
        "# Python\n",
        "\n",
        "# # Example Usage within a UNet\n",
        "# class SimpleUNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(SimpleUNet, self).__init__()\n",
        "#         self.conv1 = conv_block(1, 64) # example of using the conv_block\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         return x\n",
        "\n",
        "# # Example Usage\n",
        "# model = SimpleUNet()\n",
        "# input_tensor = torch.randn(1, 1, 256, 256) # Example input, adjust dimensions as needed\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# Let me know if you would like any modifications or further explanation! We can now move on to the max pooling layers if you are ready.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# okay great. now lets make one encoder layer. It should start with the convolutional block we just created and have the structure to include either 2 or 3 of the convolutional blocks. it should then do max pooling with a kernel size 2 and stride of 2\n",
        "\n",
        "\n",
        "\n",
        "# Perfect, let's build an encoder layer with the flexibility to include 2 or 3 convolutional blocks, followed by max pooling.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels):\n",
        "#     # (Same conv_block function as before)\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2):\n",
        "#         \"\"\"\n",
        "#         Creates an encoder layer with convolutional blocks and max pooling.\n",
        "\n",
        "#         Args:\n",
        "#             in_channels (int): Number of input channels.\n",
        "#             out_channels (int): Number of output channels.\n",
        "#             num_conv_blocks (int): Number of convolutional blocks (2 or 3).\n",
        "#         \"\"\"\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()  # Initialize as an empty Sequential container\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels))\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels))\n",
        "#         elif num_conv_blocks == 2:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#         else:\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_blocks(x)\n",
        "#         pooled = self.max_pool(x)\n",
        "#         return pooled\n",
        "\n",
        "# # Example Usage\n",
        "# encoder = EncoderLayer(1, 64, num_conv_blocks=2)  # Example with 2 conv blocks\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = encoder(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "\n",
        "# encoder3 = EncoderLayer(64, 128, num_conv_blocks=3) # Example with 3 conv blocks\n",
        "# input_tensor2 = torch.randn(1, 64, 128, 128)\n",
        "# output_tensor2 = encoder3(input_tensor2)\n",
        "# print(output_tensor2.shape)\n",
        "# Explanation:\n",
        "\n",
        "# EncoderLayer Class:\n",
        "\n",
        "# Takes in_channels, out_channels, and num_conv_blocks as arguments.\n",
        "# num_conv_blocks determines whether to include 2 or 3 convolutional blocks.\n",
        "# Uses nn.Sequential to create a container for the convolutional blocks.\n",
        "# nn.MaxPool2d is used for max pooling.\n",
        "# Conditional Conv Block Addition:\n",
        "\n",
        "# The code adds either 2 or 3 convolutional blocks to the self.conv_blocks container based on the num_conv_blocks argument.\n",
        "# This ensures that the encoder layer can have the desired number of convolutional blocks.\n",
        "# Forward Pass:\n",
        "\n",
        "# The forward method applies the convolutional blocks and then max pooling to the input tensor.\n",
        "# How to Use:\n",
        "\n",
        "# Create an EncoderLayer instance with the desired input and output channels and the number of convolutional blocks.\n",
        "# Pass the input tensor through the EncoderLayer.\n",
        "# The output will be the downsampled feature map after max pooling.\n",
        "# Let me know if you are ready to move on to the next layer!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# okay good. now lets put it together by creating 5 encoder decoder layer pairs with concatenation connecting each layer level and a bottleneck step at the bottom\n",
        "\n",
        "\n",
        "\n",
        "# Got it, let's build the complete encoder-decoder structure with 5 layers, concatenation, and a bottleneck. We'll use the EncoderLayer and conv_block functions we defined earlier.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels):\n",
        "#     # (Same conv_block function as before)\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2):\n",
        "#         # (Same EncoderLayer class as before)\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels))\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels))\n",
        "#         elif num_conv_blocks == 2:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#         else:\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_blocks(x)\n",
        "#         pooled = self.max_pool(x)\n",
        "#         return x, pooled  # Return both the conv output and the pooled output\n",
        "\n",
        "# class DecoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(DecoderLayer, self).__init__()\n",
        "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "#         self.conv = conv_block(in_channels, out_channels)\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "#         x = self.up(x)\n",
        "#         x = torch.cat([x, skip], dim=1)\n",
        "#         x = self.conv(x)\n",
        "#         return x\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=2):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         # Encoder Layers\n",
        "#         self.enc1 = EncoderLayer(in_channels, 64, num_conv_blocks=2)\n",
        "#         self.enc2 = EncoderLayer(64, 128, num_conv_blocks=2)\n",
        "#         self.enc3 = EncoderLayer(128, 256, num_conv_blocks=2)\n",
        "#         self.enc4 = EncoderLayer(256, 512, num_conv_blocks=3)\n",
        "#         self.enc5 = EncoderLayer(512, 1024, num_conv_blocks=3)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         self.bottleneck = conv_block(1024, 1024)\n",
        "\n",
        "#         # Decoder Layers\n",
        "#         self.dec5 = DecoderLayer(1024, 512)\n",
        "#         self.dec4 = DecoderLayer(512, 256)\n",
        "#         self.dec3 = DecoderLayer(256, 128)\n",
        "#         self.dec2 = DecoderLayer(128, 64)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         enc1, pool1 = self.enc1(x)\n",
        "#         enc2, pool2 = self.enc2(pool1)\n",
        "#         enc3, pool3 = self.enc3(pool2)\n",
        "#         enc4, pool4 = self.enc4(pool3)\n",
        "#         enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "#         # Decoder\n",
        "#         dec5 = self.dec5(bottleneck, enc5)\n",
        "#         dec4 = self.dec4(dec5, enc4)\n",
        "#         dec3 = self.dec3(dec4, enc3)\n",
        "#         dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         out = self.final_conv(dec2)\n",
        "#         return out\n",
        "\n",
        "# # Example Usage\n",
        "# model = UNet(in_channels=1, out_channels=2)\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# Explanation:\n",
        "\n",
        "# DecoderLayer Class:\n",
        "\n",
        "# Takes in_channels and out_channels as arguments.\n",
        "# nn.ConvTranspose2d for upsampling.\n",
        "# Concatenates the upsampled feature map with the skip connection from the encoder.\n",
        "# Applies a conv_block to refine the concatenated features.\n",
        "# UNet Class:\n",
        "\n",
        "# Creates 5 encoder layers with increasing numbers of channels.\n",
        "# Creates a bottleneck layer.\n",
        "# Creates 4 decoder layers with decreasing numbers of channels.\n",
        "# Uses nn.Conv2d for the final 1x1 convolution.\n",
        "# Forward Pass:\n",
        "\n",
        "# Passes the input through the encoder layers, storing both the output of the convolutional blocks and the output of the max pooling layers.\n",
        "# Passes the pooled output from the last encoder layer through the bottleneck.\n",
        "# Passes the bottleneck output and the skip connections from the encoder layers through the decoder layers.\n",
        "# Passes the output of the last decoder layer through the final 1x1 convolution.\n",
        "# Key Points:\n",
        "\n",
        "# Skip Connections: The skip connections are implemented using torch.cat to concatenate the upsampled feature maps with the corresponding feature maps from the encoder.\n",
        "# Upsampling: nn.ConvTranspose2d is used for upsampling, which learns to upsample the feature maps.\n",
        "# Bottleneck: The bottleneck layer is a convolutional block that captures the most abstract features.\n",
        "# Number of Channels: The number of channels is doubled in each encoder layer and halved in each decoder layer.\n",
        "# This implementation should match the general structure you described. Let me know if you would like to test this implementation or make any further adjustments.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# okay now please specify for the first 3 layers to have two convolutional blocks and the 4th and 5th layers (not the bottleneck) to have three convolutions. then also add for the bottleneck to have two convolutional blocks\n",
        "\n",
        "\n",
        "\n",
        "# Got it! Let's modify the UNet class to specify the number of convolutional blocks for each layer as requested, and also add two convolutional blocks to the bottleneck.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels):\n",
        "#     # (Same conv_block function as before)\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2):\n",
        "#         # (Same EncoderLayer class as before)\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels))\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels))\n",
        "#         elif num_conv_blocks == 2:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "#         else:\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_blocks(x)\n",
        "#         pooled = self.max_pool(x)\n",
        "#         return x, pooled  # Return both the conv output and the pooled output\n",
        "\n",
        "# class DecoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(DecoderLayer, self).__init__()\n",
        "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "#         self.conv = conv_block(in_channels, out_channels)\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "#         x = self.up(x)\n",
        "#         x = torch.cat([x, skip], dim=1)\n",
        "#         x = self.conv(x)\n",
        "#         return x\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=2):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         # Encoder Layers (Specify num_conv_blocks)\n",
        "#         self.enc1 = EncoderLayer(in_channels, 64, num_conv_blocks=2)\n",
        "#         self.enc2 = EncoderLayer(64, 128, num_conv_blocks=2)\n",
        "#         self.enc3 = EncoderLayer(128, 256, num_conv_blocks=2)\n",
        "#         self.enc4 = EncoderLayer(256, 512, num_conv_blocks=3)\n",
        "#         self.enc5 = EncoderLayer(512, 1024, num_conv_blocks=3)\n",
        "\n",
        "#         # Bottleneck (Two convolutional blocks)\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             conv_block(1024, 1024),\n",
        "#             conv_block(1024, 1024)\n",
        "#         )\n",
        "\n",
        "#         # Decoder Layers\n",
        "#         self.dec5 = DecoderLayer(1024, 512)\n",
        "#         self.dec4 = DecoderLayer(512, 256)\n",
        "#         self.dec3 = DecoderLayer(256, 128)\n",
        "#         self.dec2 = DecoderLayer(128, 64)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         enc1, pool1 = self.enc1(x)\n",
        "#         enc2, pool2 = self.enc2(pool1)\n",
        "#         enc3, pool3 = self.enc3(pool2)\n",
        "#         enc4, pool4 = self.enc4(pool3)\n",
        "#         enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "#         # Decoder\n",
        "#         dec5 = self.dec5(bottleneck, enc5)\n",
        "#         dec4 = self.dec4(dec5, enc4)\n",
        "#         dec3 = self.dec3(dec4, enc3)\n",
        "#         dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         out = self.final_conv(dec2)\n",
        "#         return out\n",
        "\n",
        "# # Example Usage\n",
        "# model = UNet(in_channels=1, out_channels=2)\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# Changes:\n",
        "\n",
        "# Encoder Layers:\n",
        "\n",
        "# self.enc1, self.enc2, and self.enc3 are created with num_conv_blocks=2.\n",
        "# self.enc4 and self.enc5 are created with num_conv_blocks=3.\n",
        "# Bottleneck:\n",
        "\n",
        "# self.bottleneck is now a nn.Sequential containing two conv_block instances, effectively creating a bottleneck with two convolutional blocks.\n",
        "# This updated code should now match the specific configuration you requested for the number of convolutional blocks in each layer and the bottleneck. Let me know if you would like any further modifications or have more questions!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# okay now i would like some help validating that my UNet model turned out how I intend it to. Please give me some code that I can run to confirm that the number of encoder decoder layer pairs is correct, that I have the right stride and padding and kernel sizes, and how many convolutional blocks are in each encoder layer and each decoder layer\n",
        "\n",
        "\n",
        "\n",
        "# Great! Let's build a validation script to check the structure of your UNet model. We'll verify the number of layers, kernel sizes, strides, padding, and the number of convolutional blocks.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer, and UNet classes defined as before\n",
        "\n",
        "# def validate_unet(model):\n",
        "#     \"\"\"\n",
        "#     Validates the structure of the UNet model.\n",
        "#     \"\"\"\n",
        "#     encoder_layers = []\n",
        "#     decoder_layers = []\n",
        "\n",
        "#     for name, module in model.named_children():\n",
        "#         if isinstance(module, EncoderLayer):\n",
        "#             encoder_layers.append(module)\n",
        "#         elif isinstance(module, DecoderLayer):\n",
        "#             decoder_layers.append(module)\n",
        "\n",
        "#     # 1. Check the number of encoder and decoder layers\n",
        "#     assert len(encoder_layers) == 5, f\"Expected 5 encoder layers, but found {len(encoder_layers)}\"\n",
        "#     assert len(decoder_layers) == 4, f\"Expected 4 decoder layers, but found {len(decoder_layers)}\"\n",
        "\n",
        "#     print(\"Number of encoder layers: 5 (Correct)\")\n",
        "#     print(\"Number of decoder layers: 4 (Correct)\")\n",
        "\n",
        "#     # 2. Check the number of convolutional blocks in each encoder layer\n",
        "#     expected_conv_blocks = [2, 2, 2, 3, 3]  # Based on your specifications\n",
        "#     for i, enc_layer in enumerate(encoder_layers):\n",
        "#         num_conv_blocks = len(enc_layer.conv_blocks) // 2  # Each conv_block has 2 layers\n",
        "#         assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\"\n",
        "#         print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\")\n",
        "\n",
        "#     # 3. Check the kernel size, stride, and padding in the convolutional blocks\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         for conv_block_layer in enc_layer.conv_blocks:\n",
        "#             if isinstance(conv_block_layer, nn.Conv2d):\n",
        "#                 assert conv_block_layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                 assert conv_block_layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                 assert conv_block_layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         for conv_block_layer in dec_layer.conv:\n",
        "#             if isinstance(conv_block_layer, nn.Conv2d):\n",
        "#                 assert conv_block_layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                 assert conv_block_layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                 assert conv_block_layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     print(\"Conv kernel size, stride, and padding are correct.\")\n",
        "\n",
        "#     # 4. Check the max pooling kernel size and stride\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         assert enc_layer.max_pool.kernel_size == 2, \"Max pool kernel size should be 2\"\n",
        "#         assert enc_layer.max_pool.stride == 2, \"Max pool stride should be 2\"\n",
        "\n",
        "#     print(\"Max pooling kernel size and stride are correct.\")\n",
        "\n",
        "#     # 5. Check the number of convolutional blocks in the bottleneck\n",
        "#     bottleneck_conv_blocks = len(model.bottleneck) // 2\n",
        "#     assert bottleneck_conv_blocks == 2, f\"Bottleneck expected 2 conv blocks, but found {bottleneck_conv_blocks}\"\n",
        "#     print(\"Bottleneck has 2 conv blocks (Correct)\")\n",
        "\n",
        "#     # 6. Check the upsampling layer\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         assert dec_layer.up.kernel_size == (2, 2), \"Upsampling kernel size should be 2x2\"\n",
        "#         assert dec_layer.up.stride == (2, 2), \"Upsampling stride should be 2x2\"\n",
        "\n",
        "#     print(\"Upsampling kernel size and stride are correct.\")\n",
        "\n",
        "#     print(\"UNet model validation successful!\")\n",
        "\n",
        "# # Example Usage\n",
        "# model = UNet(in_channels=1, out_channels=2)\n",
        "# validate_unet(model)\n",
        "# Explanation:\n",
        "\n",
        "# Layer Count Verification:\n",
        "\n",
        "# Iterates through the model's children and counts the number of EncoderLayer and DecoderLayer instances.\n",
        "# Asserts that the counts match the expected values (5 encoder, 4 decoder).\n",
        "# Convolutional Block Count Verification:\n",
        "\n",
        "# Checks the num_conv_blocks attribute of each EncoderLayer to ensure it matches the expected number of convolutional blocks (2 for the first 3 layers, 3 for the last 2).\n",
        "# Checks the number of convolutional blocks within the bottleneck.\n",
        "# Convolutional Kernel, Stride, and Padding Verification:\n",
        "\n",
        "# Iterates through the convolutional layers within each EncoderLayer and DecoderLayer and checks the kernel_size, stride, and padding attributes.\n",
        "# Max Pooling Verification:\n",
        "\n",
        "# Checks the kernel_size and stride attributes of the MaxPool2d layers in each EncoderLayer.\n",
        "# Upsampling Verification:\n",
        "\n",
        "# Checks the kernel_size and stride attributes of the ConvTranspose2d layers in each DecoderLayer.\n",
        "# How to Use:\n",
        "\n",
        "# Make sure your UNet model is instantiated.\n",
        "# Call the validate_unet(model) function, passing your model instance as an argument.\n",
        "# This script will provide detailed feedback, confirming that your UNet model is constructed as intended. Let me know if you have any questions or need further assistance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# i got this error:\n",
        "\n",
        "# Number of encoder layers: 5 (Correct)\n",
        "\n",
        "# Number of decoder layers: 4 (Correct)\n",
        "\n",
        "# ---------------------------------------------------------------------------AssertionError Traceback (most recent call last)<ipython-input-42-1ef6c1660f82> in <cell line: 0>() 66 # Example Usage 67 model = UNet(in_channels=1, out_channels=2)---> 68 validate_unet(model)<ipython-input-42-1ef6c1660f82> in validate_unet(model) 23 for i, enc_layer in enumerate(encoder_layers): 24 num_conv_blocks = len(enc_layer.conv_blocks) // 2 # Each conv_block has 2 layers---> 25 assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\" 26 print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\") 27 AssertionError: Encoder 1 expected 2 conv blocks, but found 1\n",
        "\n",
        "\n",
        "\n",
        "# You are absolutely right! My apologies, there's a subtle error in the way we're counting the convolutional blocks in the EncoderLayer.\n",
        "\n",
        "# The issue is that we are accessing the first convolutional block, but the EncoderLayer has a Sequential container named conv_blocks that contains a single conv_block instance. Each conv_block instance, in turn, contains two convolutional layers.\n",
        "\n",
        "# Let's correct the validate_unet function to accurately count the convolutional blocks:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer, and UNet classes defined as before\n",
        "\n",
        "# def validate_unet(model):\n",
        "#     \"\"\"\n",
        "#     Validates the structure of the UNet model.\n",
        "#     \"\"\"\n",
        "#     encoder_layers = []\n",
        "#     decoder_layers = []\n",
        "\n",
        "#     for name, module in model.named_children():\n",
        "#         if isinstance(module, EncoderLayer):\n",
        "#             encoder_layers.append(module)\n",
        "#         elif isinstance(module, DecoderLayer):\n",
        "#             decoder_layers.append(module)\n",
        "\n",
        "#     # 1. Check the number of encoder and decoder layers\n",
        "#     assert len(encoder_layers) == 5, f\"Expected 5 encoder layers, but found {len(encoder_layers)}\"\n",
        "#     assert len(decoder_layers) == 4, f\"Expected 4 decoder layers, but found {len(decoder_layers)}\"\n",
        "\n",
        "#     print(\"Number of encoder layers: 5 (Correct)\")\n",
        "#     print(\"Number of decoder layers: 4 (Correct)\")\n",
        "\n",
        "#     # 2. Check the number of convolutional blocks in each encoder layer\n",
        "#     expected_conv_blocks = [2, 2, 2, 3, 3]  # Based on your specifications\n",
        "#     for i, enc_layer in enumerate(encoder_layers):\n",
        "#         num_conv_blocks = 0\n",
        "#         for block in enc_layer.conv_blocks:\n",
        "#             if isinstance(block, nn.Sequential):\n",
        "#                 num_conv_blocks += 1\n",
        "#         assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\"\n",
        "#         print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\")\n",
        "\n",
        "#     # 3. Check the kernel size, stride, and padding in the convolutional blocks\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         for conv_block_layer in enc_layer.conv_blocks:\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "#                 for layer in conv_block_layer:\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         for conv_block_layer in dec_layer.conv:\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "#                 for layer in conv_block_layer:\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     print(\"Conv kernel size, stride, and padding are correct.\")\n",
        "\n",
        "#     # 4. Check the max pooling kernel size and stride\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         assert enc_layer.max_pool.kernel_size == 2, \"Max pool kernel size should be 2\"\n",
        "#         assert enc_layer.max_pool.stride == 2, \"Max pool stride should be 2\"\n",
        "\n",
        "#     print(\"Max pooling kernel size and stride are correct.\")\n",
        "\n",
        "#     # 5. Check the number of convolutional blocks in the bottleneck\n",
        "#     bottleneck_conv_blocks = 0\n",
        "#     for block in model.bottleneck:\n",
        "#         if isinstance(block, nn.Sequential):\n",
        "#             bottleneck_conv_blocks += 1\n",
        "#     assert bottleneck_conv_blocks == 2, f\"Bottleneck expected 2 conv blocks, but found {bottleneck_conv_blocks}\"\n",
        "#     print(\"Bottleneck has 2 conv blocks (Correct)\")\n",
        "\n",
        "#     # 6. Check the upsampling layer\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         assert dec_layer.up.kernel_size == (2, 2), \"Upsampling kernel size should be 2x2\"\n",
        "#         assert dec_layer.up.stride == (2, 2), \"Upsampling stride should be 2x2\"\n",
        "\n",
        "#     print(\"Upsampling kernel size and stride are correct.\")\n",
        "\n",
        "#     print(\"UNet model validation successful!\")\n",
        "\n",
        "# # Example Usage\n",
        "# model = UNet(in_channels=1, out_channels=2)\n",
        "# validate_unet(model)\n",
        "# Key Change:\n",
        "\n",
        "# In the part that counts convolutional blocks, we now iterate through the conv_blocks of each EncoderLayer and check if each item is an nn.Sequential (which represents a conv_block). We then count these nn.Sequential blocks.\n",
        "# We also make a similar change to counting the number of convolutional blocks in the bottleneck.\n",
        "# We also iterate through each layer in the convolutional blocks to check the convolutional layer configurations.\n",
        "# This corrected version should now accurately count the convolutional blocks and pass the validation.\n",
        "\n",
        "# Please try running this updated code, and let me know if you encounter any further issues. My apologies again for the initial mistake!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# i got this error:\n",
        "\n",
        "# ---------------------------------------------------------------------------TypeError Traceback (most recent call last)<ipython-input-47-8847ec42b6bf> in <cell line: 0>()----> 1 model = UNet(n_classes, in_channels, filter_config, dropout_rate)TypeError: UNet.__init__() takes from 1 to 3 positional arguments but 5 were given\n",
        "\n",
        "\n",
        "\n",
        "# on this command:\n",
        "\n",
        "\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "\n",
        "\n",
        "\n",
        "# You are absolutely right! My apologies, there's a discrepancy between the way we defined the UNet class's __init__ method and the way you're calling it.\n",
        "\n",
        "# The Error:\n",
        "\n",
        "# The error message TypeError: UNet.__init__() takes from 1 to 3 positional arguments but 5 were given indicates that your UNet class's __init__ method is expecting a different number of arguments than you are providing when you instantiate the model.\n",
        "\n",
        "# The Solution:\n",
        "\n",
        "# Based on the code we've been working with, the UNet class's __init__ method is defined as:\n",
        "\n",
        "# Python\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=2):\n",
        "#         super(UNet, self).__init__()\n",
        "#         # ... rest of the code\n",
        "# This means it expects only in_channels and out_channels as arguments (with default values).\n",
        "\n",
        "# You're trying to call it with:\n",
        "\n",
        "# Python\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# To fix this, you have two main options:\n",
        "\n",
        "# Modify the UNet Class:\n",
        "\n",
        "# Update the __init__ method to accept the arguments you're providing (i.e., n_classes, in_channels, filter_config, dropout_rate).\n",
        "# Use these arguments to configure your model accordingly.\n",
        "# Python\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, n_classes, in_channels, filter_config, dropout_rate):\n",
        "#         super(UNet, self).__init__()\n",
        "#         # Use filter_config to define the number of filters in each layer\n",
        "#         # Use dropout_rate to add dropout layers if needed\n",
        "#         # Use n_classes for the output channels\n",
        "#         # Use in_channels for the input channels\n",
        "\n",
        "#         # Example:\n",
        "#         filters = filter_config  # Assuming filter_config is a list of filter sizes\n",
        "#         self.enc1 = EncoderLayer(in_channels, filters[0], num_conv_blocks=2)\n",
        "#         # ... rest of your UNet layers\n",
        "#         self.final_conv = nn.Conv2d(filters[-1], n_classes, kernel_size=1)\n",
        "# Modify the Call to UNet:\n",
        "\n",
        "# Adjust the call to UNet to match the arguments expected by the __init__ method.\n",
        "# This means only passing in_channels and out_channels (or n_classes as output).\n",
        "# Python\n",
        "\n",
        "# model = UNet(in_channels=in_channels, out_channels=n_classes)\n",
        "# Recommendation:\n",
        "\n",
        "# If you need to customize your UNet with filter_config and dropout_rate, option 1 is the correct approach. You'll need to update the UNet class to utilize these parameters to configure the network.\n",
        "\n",
        "# Example (Option 1):\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # ... (conv_block, EncoderLayer, DecoderLayer definitions)\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, n_classes, in_channels, filter_config, dropout_rate=0.0):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         filters = filter_config\n",
        "\n",
        "#         self.enc1 = EncoderLayer(in_channels, filters[0], num_conv_blocks=2)\n",
        "#         self.enc2 = EncoderLayer(filters[0], filters[1], num_conv_blocks=2)\n",
        "#         self.enc3 = EncoderLayer(filters[1], filters[2], num_conv_blocks=2)\n",
        "#         self.enc4 = EncoderLayer(filters[2], filters[3], num_conv_blocks=3)\n",
        "#         self.enc5 = EncoderLayer(filters[3], filters[4], num_conv_blocks=3)\n",
        "\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             conv_block(filters[4], filters[4]),\n",
        "#             conv_block(filters[4], filters[4])\n",
        "#         )\n",
        "\n",
        "#         self.dec5 = DecoderLayer(filters[4], filters[3])\n",
        "#         self.dec4 = DecoderLayer(filters[3], filters[2])\n",
        "#         self.dec3 = DecoderLayer(filters[2], filters[1])\n",
        "#         self.dec2 = DecoderLayer(filters[1], filters[0])\n",
        "\n",
        "#         self.final_conv = nn.Conv2d(filters[0], n_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # ... (rest of the forward method)\n",
        "\n",
        "# # Example Usage:\n",
        "# n_classes = 2\n",
        "# in_channels = 1\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# Important: Make sure your filter_config list has the correct number of filter sizes for your UNet layers.\n",
        "\n",
        "# Please adjust your code accordingly, and let me know if you have any more questions!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# can you give m ecode to add filter_config and dropout rate into the model architecture\n",
        "\n",
        "\n",
        "# All right, let's incorporate filter_config and dropout_rate into your UNet architecture.\n",
        "\n",
        "# 1. Update the UNet Class:\n",
        "\n",
        "# We'll modify the UNet class to accept filter_config and dropout_rate as arguments in the __init__ method. We'll also use filter_config to define the number of filters in each layer and add dropout layers where needed.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer classes\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, n_classes, in_channels, filter_config, dropout_rate=0.0):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         filters = filter_config\n",
        "\n",
        "#         # Encoder Layers\n",
        "#         self.enc1 = EncoderLayer(in_channels, filters[0], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc2 = EncoderLayer(filters[0], filters[1], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc3 = EncoderLayer(filters[1], filters[2], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc4 = EncoderLayer(filters[2], filters[3], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "#         self.enc5 = EncoderLayer(filters[3], filters[4], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             conv_block(filters[4], filters[4], dropout_rate=dropout_rate),\n",
        "#             conv_block(filters[4], filters[4], dropout_rate=dropout_rate)\n",
        "#         )\n",
        "\n",
        "#         # Decoder Layers\n",
        "#         self.dec5 = DecoderLayer(filters[4], filters[3], dropout_rate=dropout_rate)\n",
        "#         self.dec4 = DecoderLayer(filters[3], filters[2], dropout_rate=dropout_rate)\n",
        "#         self.dec3 = DecoderLayer(filters[2], filters[1], dropout_rate=dropout_rate)\n",
        "#         self.dec2 = DecoderLayer(filters[1], filters[0], dropout_rate=dropout_rate)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         self.final_conv = nn.Conv2d(filters[0], n_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         enc1, pool1 = self.enc1(x)\n",
        "#         enc2, pool2 = self.enc2(pool1)\n",
        "#         enc3, pool3 = self.enc3(pool2)\n",
        "#         enc4, pool4 = self.enc4(pool3)\n",
        "#         enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "#         # Decoder\n",
        "#         dec5 = self.dec5(bottleneck, enc5)\n",
        "#         dec4 = self.dec4(dec5, enc4)\n",
        "#         dec3 = self.dec3(dec4, enc3)\n",
        "#         dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         out = self.final_conv(dec2)\n",
        "#         return out\n",
        "# 2. Update conv_block, EncoderLayer, and DecoderLayer:\n",
        "\n",
        "# We'll add dropout_rate as an argument to these classes and use it to add dropout layers after each convolutional layer.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels, dropout_rate=0.0):\n",
        "#     layers = [\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     ]\n",
        "#     if dropout_rate > 0:\n",
        "#         layers.append(nn.Dropout2d(dropout_rate))\n",
        "#     layers.extend([\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     ])\n",
        "#     if dropout_rate > 0:\n",
        "#         layers.append(nn.Dropout2d(dropout_rate))\n",
        "#     return nn.Sequential(*layers)\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2, dropout_rate=0.0):\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels, dropout_rate=dropout_rate))\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#         elif num_conv_blocks == 2:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#         else:\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_blocks(x)\n",
        "#         pooled = self.max_pool(x)\n",
        "#         return x, pooled\n",
        "\n",
        "# class DecoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
        "#         super(DecoderLayer, self).__init__()\n",
        "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "#         self.conv = conv_block(in_channels, out_channels, dropout_rate=dropout_rate)\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "#         x = self.up(x)\n",
        "#         x = torch.cat([x, skip], dim=1)\n",
        "#         x = self.conv(x)\n",
        "#         return x\n",
        "# 3. Example Usage:\n",
        "\n",
        "# Python\n",
        "\n",
        "# n_classes = 2\n",
        "# in_channels = 1\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# Key Points:\n",
        "\n",
        "# Dropout: We've added nn.Dropout2d layers after each convolutional layer if dropout_rate is greater than 0.\n",
        "# Filter Configuration: The filter_config list is used to determine the number of filters in each layer.\n",
        "# Flexibility: This approach makes your UNet more flexible, allowing you to easily experiment with different filter configurations and dropout rates.\n",
        "# Important:\n",
        "\n",
        "# Make sure your filter_config list contains the correct number of filter sizes for your UNet layers.\n",
        "# The dropout_rate should be a value between 0 and 1.\n",
        "# This implementation now incorporates filter_config and dropout_rate into your UNet model. Let me know if you have any more questions!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# please add those changes specifically to this model:\n",
        "\n",
        "\n",
        "\n",
        "# ## Working through better UNet\n",
        "\n",
        "# def conv_block(in_channels, out_channels):\n",
        "\n",
        "#     # (Same conv_block function as before)\n",
        "\n",
        "#     return nn.Sequential(\n",
        "\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "\n",
        "#         nn.ReLU(inplace=True),\n",
        "\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "\n",
        "#         nn.ReLU(inplace=True)\n",
        "\n",
        "#     )\n",
        "\n",
        "\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2):\n",
        "\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()\n",
        "\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels))\n",
        "\n",
        "\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels))\n",
        "\n",
        "#         elif num_conv_blocks == 2:\n",
        "\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels))\n",
        "\n",
        "#         else:\n",
        "\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = self.conv_blocks(x)\n",
        "\n",
        "#         pooled = self.max_pool(x)\n",
        "\n",
        "#         return x, pooled  # Return both the conv output and the pooled output\n",
        "\n",
        "\n",
        "\n",
        "# class DecoderLayer(nn.Module):\n",
        "\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "\n",
        "#         super(DecoderLayer, self).__init__()\n",
        "\n",
        "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "#         self.conv = conv_block(in_channels, out_channels)\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "\n",
        "#         x = self.up(x)\n",
        "\n",
        "#         x = torch.cat([x, skip], dim=1)\n",
        "\n",
        "#         x = self.conv(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "\n",
        "#     def __init__(self, in_channels=1, out_channels=2):\n",
        "\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "#         # Encoder Layers (Specify num_conv_blocks)\n",
        "\n",
        "#         self.enc1 = EncoderLayer(in_channels, 64, num_conv_blocks=2)\n",
        "\n",
        "#         self.enc2 = EncoderLayer(64, 128, num_conv_blocks=2)\n",
        "\n",
        "#         self.enc3 = EncoderLayer(128, 256, num_conv_blocks=2)\n",
        "\n",
        "#         self.enc4 = EncoderLayer(256, 512, num_conv_blocks=3)\n",
        "\n",
        "#         self.enc5 = EncoderLayer(512, 1024, num_conv_blocks=3)\n",
        "\n",
        "\n",
        "\n",
        "#         # Bottleneck (Two convolutional blocks)\n",
        "\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "\n",
        "#             conv_block(1024, 1024),\n",
        "\n",
        "#             conv_block(1024, 1024)\n",
        "\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "#         # Decoder Layers\n",
        "\n",
        "#         self.dec5 = DecoderLayer(1024, 512)\n",
        "\n",
        "#         self.dec4 = DecoderLayer(512, 256)\n",
        "\n",
        "#         self.dec3 = DecoderLayer(256, 128)\n",
        "\n",
        "#         self.dec2 = DecoderLayer(128, 64)\n",
        "\n",
        "\n",
        "\n",
        "#         # Final Convolution\n",
        "\n",
        "#         self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         # Encoder\n",
        "\n",
        "#         enc1, pool1 = self.enc1(x)\n",
        "\n",
        "#         enc2, pool2 = self.enc2(pool1)\n",
        "\n",
        "#         enc3, pool3 = self.enc3(pool2)\n",
        "\n",
        "#         enc4, pool4 = self.enc4(pool3)\n",
        "\n",
        "#         enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "\n",
        "\n",
        "#         # Bottleneck\n",
        "\n",
        "#         bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "\n",
        "\n",
        "#         # Decoder\n",
        "\n",
        "#         dec5 = self.dec5(bottleneck, enc5)\n",
        "\n",
        "#         dec4 = self.dec4(dec5, enc4)\n",
        "\n",
        "#         dec3 = self.dec3(dec4, enc3)\n",
        "\n",
        "#         dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "\n",
        "\n",
        "#         # Final Convolution\n",
        "\n",
        "#         out = self.final_conv(dec2)\n",
        "\n",
        "#         return out\n",
        "\n",
        "\n",
        "\n",
        "# Got it! Let's modify the provided UNet code to incorporate filter_config and dropout_rate as you requested.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def conv_block(in_channels, out_channels, dropout_rate=0.0):\n",
        "#     layers = [\n",
        "#         nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     ]\n",
        "#     if dropout_rate > 0:\n",
        "#         layers.append(nn.Dropout2d(dropout_rate))\n",
        "#     layers.extend([\n",
        "#         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     ])\n",
        "#     if dropout_rate > 0:\n",
        "#         layers.append(nn.Dropout2d(dropout_rate))\n",
        "#     return nn.Sequential(*layers)\n",
        "\n",
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_conv_blocks=2, dropout_rate=0.0):\n",
        "#         super(EncoderLayer, self).__init__()\n",
        "\n",
        "#         self.conv_blocks = nn.Sequential()\n",
        "#         self.conv_blocks.add_module('conv_block_1', conv_block(in_channels, out_channels, dropout_rate=dropout_rate))\n",
        "\n",
        "#         if num_conv_blocks == 3:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#             self.conv_blocks.add_module('conv_block_3', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#         elif num_conv_blocks == 2:\n",
        "#             self.conv_blocks.add_module('conv_block_2', conv_block(out_channels, out_channels, dropout_rate=dropout_rate))\n",
        "#         else:\n",
        "#             raise ValueError(\"num_conv_blocks must be 2 or 3\")\n",
        "\n",
        "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_blocks(x)\n",
        "#         pooled = self.max_pool(x)\n",
        "#         return x, pooled\n",
        "\n",
        "# class DecoderLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
        "#         super(DecoderLayer, self).__init__()\n",
        "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "#         self.conv = conv_block(in_channels, out_channels, dropout_rate=dropout_rate)\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "#         x = self.up(x)\n",
        "#         x = torch.cat([x, skip], dim=1)\n",
        "#         x = self.conv(x)\n",
        "#         return x\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, n_classes, in_channels, filter_config, dropout_rate=0.0):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#         filters = filter_config\n",
        "\n",
        "#         # Encoder Layers (Specify num_conv_blocks)\n",
        "#         self.enc1 = EncoderLayer(in_channels, filters[0], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc2 = EncoderLayer(filters[0], filters[1], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc3 = EncoderLayer(filters[1], filters[2], num_conv_blocks=2, dropout_rate=dropout_rate)\n",
        "#         self.enc4 = EncoderLayer(filters[2], filters[3], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "#         self.enc5 = EncoderLayer(filters[3], filters[4], num_conv_blocks=3, dropout_rate=dropout_rate)\n",
        "\n",
        "#         # Bottleneck (Two convolutional blocks)\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             conv_block(filters[4], filters[4], dropout_rate=dropout_rate),\n",
        "#             conv_block(filters[4], filters[4], dropout_rate=dropout_rate)\n",
        "#         )\n",
        "\n",
        "#         # Decoder Layers\n",
        "#         self.dec5 = DecoderLayer(filters[4], filters[3], dropout_rate=dropout_rate)\n",
        "#         self.dec4 = DecoderLayer(filters[3], filters[2], dropout_rate=dropout_rate)\n",
        "#         self.dec3 = DecoderLayer(filters[2], filters[1], dropout_rate=dropout_rate)\n",
        "#         self.dec2 = DecoderLayer(filters[1], filters[0], dropout_rate=dropout_rate)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         self.final_conv = nn.Conv2d(filters[0], n_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         enc1, pool1 = self.enc1(x)\n",
        "#         enc2, pool2 = self.enc2(pool1)\n",
        "#         enc3, pool3 = self.enc3(pool2)\n",
        "#         enc4, pool4 = self.enc4(pool3)\n",
        "#         enc5, pool5 = self.enc5(pool4)\n",
        "\n",
        "#         # Bottleneck\n",
        "#         bottleneck = self.bottleneck(pool5)\n",
        "\n",
        "#         # Decoder\n",
        "#         dec5 = self.dec5(bottleneck, enc5)\n",
        "#         dec4 = self.dec4(dec5, enc4)\n",
        "#         dec3 = self.dec3(dec4, enc3)\n",
        "#         dec2 = self.dec2(dec3, enc2)\n",
        "\n",
        "#         # Final Convolution\n",
        "#         out = self.final_conv(dec2)\n",
        "#         return out\n",
        "\n",
        "# # Example Usage\n",
        "# n_classes = 2\n",
        "# in_channels = 1\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# input_tensor = torch.randn(1, 1, 256, 256)\n",
        "# output_tensor = model(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# Key Modifications:\n",
        "\n",
        "# conv_block:\n",
        "\n",
        "# Added dropout_rate as an argument.\n",
        "# Added nn.Dropout2d layers after each nn.Conv2d layer if dropout_rate is greater than 0.\n",
        "# EncoderLayer:\n",
        "\n",
        "# Added dropout_rate as an argument.\n",
        "# Passed dropout_rate to the conv_block calls.\n",
        "# DecoderLayer:\n",
        "\n",
        "# Added dropout_rate as an argument.\n",
        "# Passed dropout_rate to the conv_block call.\n",
        "# UNet:\n",
        "\n",
        "# Added n_classes, in_channels, filter_config, and dropout_rate as arguments to the __init__ method.\n",
        "# Used filter_config to define the number of filters in each layer.\n",
        "# Passed dropout_rate to the EncoderLayer, DecoderLayer, and conv_block calls.\n",
        "# Used n_classes to define the number of output channels in the final nn.Conv2d layer.\n",
        "# Now, your UNet model incorporates filter_config and dropout_rate as requested. Remember to adjust the filter_config list to match the desired number of filters in each layer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# now reframe this code to verify the model you just made:\n",
        "\n",
        "\n",
        "\n",
        "# def validate_unet(model):\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     Validates the structure of the UNet model.\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     encoder_layers = []\n",
        "\n",
        "#     decoder_layers = []\n",
        "\n",
        "\n",
        "\n",
        "#     for name, module in model.named_children():\n",
        "\n",
        "#         if isinstance(module, EncoderLayer):\n",
        "\n",
        "#             encoder_layers.append(module)\n",
        "\n",
        "#         elif isinstance(module, DecoderLayer):\n",
        "\n",
        "#             decoder_layers.append(module)\n",
        "\n",
        "\n",
        "\n",
        "#     # 1. Check the number of encoder and decoder layers\n",
        "\n",
        "#     assert len(encoder_layers) == 5, f\"Expected 5 encoder layers, but found {len(encoder_layers)}\"\n",
        "\n",
        "#     assert len(decoder_layers) == 4, f\"Expected 4 decoder layers, but found {len(decoder_layers)}\"\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"Number of encoder layers: 5 (Correct)\")\n",
        "\n",
        "#     print(\"Number of decoder layers: 4 (Correct)\")\n",
        "\n",
        "\n",
        "\n",
        "#     # 2. Check the number of convolutional blocks in each encoder layer\n",
        "\n",
        "#     expected_conv_blocks = [2, 2, 2, 3, 3]  # Based on your specifications\n",
        "\n",
        "#     for i, enc_layer in enumerate(encoder_layers):\n",
        "\n",
        "#         num_conv_blocks = 0\n",
        "\n",
        "#         for block in enc_layer.conv_blocks:\n",
        "\n",
        "#             if isinstance(block, nn.Sequential):\n",
        "\n",
        "#                 num_conv_blocks += 1\n",
        "\n",
        "#         assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\"\n",
        "\n",
        "#         print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\")\n",
        "\n",
        "\n",
        "\n",
        "#     # 3. Check the kernel size, stride, and padding in the convolutional blocks\n",
        "\n",
        "#     for enc_layer in encoder_layers:\n",
        "\n",
        "#         for conv_block_layer in enc_layer.conv_blocks:\n",
        "\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "\n",
        "#                 for layer in conv_block_layer:\n",
        "\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "\n",
        "\n",
        "#     for dec_layer in decoder_layers:\n",
        "\n",
        "#         for conv_block_layer in dec_layer.conv:\n",
        "\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "\n",
        "#                 for layer in conv_block_layer:\n",
        "\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"Conv kernel size, stride, and padding are correct.\")\n",
        "\n",
        "\n",
        "\n",
        "#     # 4. Check the max pooling kernel size and stride\n",
        "\n",
        "#     for enc_layer in encoder_layers:\n",
        "\n",
        "#         assert enc_layer.max_pool.kernel_size == 2, \"Max pool kernel size should be 2\"\n",
        "\n",
        "#         assert enc_layer.max_pool.stride == 2, \"Max pool stride should be 2\"\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"Max pooling kernel size and stride are correct.\")\n",
        "\n",
        "\n",
        "\n",
        "#     # 5. Check the number of convolutional blocks in the bottleneck\n",
        "\n",
        "#     bottleneck_conv_blocks = 0\n",
        "\n",
        "#     for block in model.bottleneck:\n",
        "\n",
        "#         if isinstance(block, nn.Sequential):\n",
        "\n",
        "#             bottleneck_conv_blocks += 1\n",
        "\n",
        "#     assert bottleneck_conv_blocks == 2, f\"Bottleneck expected 2 conv blocks, but found {bottleneck_conv_blocks}\"\n",
        "\n",
        "#     print(\"Bottleneck has 2 conv blocks (Correct)\")\n",
        "\n",
        "\n",
        "\n",
        "#     # 6. Check the upsampling layer\n",
        "\n",
        "#     for dec_layer in decoder_layers:\n",
        "\n",
        "#         assert dec_layer.up.kernel_size == (2, 2), \"Upsampling kernel size should be 2x2\"\n",
        "\n",
        "#         assert dec_layer.up.stride == (2, 2), \"Upsampling stride should be 2x2\"\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"Upsampling kernel size and stride are correct.\")\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"UNet model validation successful!\")\n",
        "\n",
        "\n",
        "\n",
        "# # Example Usage\n",
        "\n",
        "# model = UNet(in_channels=1, out_channels=2)\n",
        "\n",
        "# validate_unet(model)\n",
        "\n",
        "\n",
        "\n",
        "# Got it! Let's adapt the validate_unet function to work with the UNet model you just created, which now includes filter_config and dropout_rate in its initialization.\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer, and UNet classes defined as before\n",
        "\n",
        "# def validate_unet(model, filter_config):\n",
        "#     \"\"\"\n",
        "#     Validates the structure of the UNet model.\n",
        "#     \"\"\"\n",
        "#     encoder_layers = []\n",
        "#     decoder_layers = []\n",
        "\n",
        "#     for name, module in model.named_children():\n",
        "#         if isinstance(module, EncoderLayer):\n",
        "#             encoder_layers.append(module)\n",
        "#         elif isinstance(module, DecoderLayer):\n",
        "#             decoder_layers.append(module)\n",
        "\n",
        "#     # 1. Check the number of encoder and decoder layers\n",
        "#     assert len(encoder_layers) == 5, f\"Expected 5 encoder layers, but found {len(encoder_layers)}\"\n",
        "#     assert len(decoder_layers) == 4, f\"Expected 4 decoder layers, but found {len(decoder_layers)}\"\n",
        "\n",
        "#     print(\"Number of encoder layers: 5 (Correct)\")\n",
        "#     print(\"Number of decoder layers: 4 (Correct)\")\n",
        "\n",
        "#     # 2. Check the number of convolutional blocks in each encoder layer\n",
        "#     expected_conv_blocks = [2, 2, 2, 3, 3]  # Based on your specifications\n",
        "#     for i, enc_layer in enumerate(encoder_layers):\n",
        "#         num_conv_blocks = 0\n",
        "#         for block in enc_layer.conv_blocks:\n",
        "#             if isinstance(block, nn.Sequential):\n",
        "#                 num_conv_blocks += 1\n",
        "#         assert num_conv_blocks == expected_conv_blocks[i], f\"Encoder {i + 1} expected {expected_conv_blocks[i]} conv blocks, but found {num_conv_blocks}\"\n",
        "#         print(f\"Encoder layer {i + 1} has {num_conv_blocks} conv blocks (Correct)\")\n",
        "\n",
        "#     # 3. Check the kernel size, stride, and padding in the convolutional blocks\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         for conv_block_layer in enc_layer.conv_blocks:\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "#                 for layer in conv_block_layer:\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         for conv_block_layer in dec_layer.conv:\n",
        "#             if isinstance(conv_block_layer, nn.Sequential):\n",
        "#                 for layer in conv_block_layer:\n",
        "#                     if isinstance(layer, nn.Conv2d):\n",
        "#                         assert layer.kernel_size == (3, 3), \"Conv kernel size should be 3x3\"\n",
        "#                         assert layer.stride == (1, 1), \"Conv stride should be 1\"\n",
        "#                         assert layer.padding == (1, 1), \"Conv padding should be 1\"\n",
        "\n",
        "#     print(\"Conv kernel size, stride, and padding are correct.\")\n",
        "\n",
        "#     # 4. Check the max pooling kernel size and stride\n",
        "#     for enc_layer in encoder_layers:\n",
        "#         assert enc_layer.max_pool.kernel_size == 2, \"Max pool kernel size should be 2\"\n",
        "#         assert enc_layer.max_pool.stride == 2, \"Max pool stride should be 2\"\n",
        "\n",
        "#     print(\"Max pooling kernel size and stride are correct.\")\n",
        "\n",
        "#     # 5. Check the number of convolutional blocks in the bottleneck\n",
        "#     bottleneck_conv_blocks = 0\n",
        "#     for block in model.bottleneck:\n",
        "#         if isinstance(block, nn.Sequential):\n",
        "#             bottleneck_conv_blocks += 1\n",
        "#     assert bottleneck_conv_blocks == 2, f\"Bottleneck expected 2 conv blocks, but found {bottleneck_conv_blocks}\"\n",
        "#     print(\"Bottleneck has 2 conv blocks (Correct)\")\n",
        "\n",
        "#     # 6. Check the upsampling layer\n",
        "#     for dec_layer in decoder_layers:\n",
        "#         assert dec_layer.up.kernel_size == (2, 2), \"Upsampling kernel size should be 2x2\"\n",
        "#         assert dec_layer.up.stride == (2, 2), \"Upsampling stride should be 2x2\"\n",
        "\n",
        "#     print(\"Upsampling kernel size and stride are correct.\")\n",
        "\n",
        "#     # 7. Check the filter configuration\n",
        "#     expected_filters = filter_config\n",
        "#     actual_filters = [enc_layer.conv_blocks[0][0].out_channels for enc_layer in encoder_layers]\n",
        "#     assert actual_filters == expected_filters, f\"Filter configuration mismatch. Expected {expected_filters}, but got {actual_filters}\"\n",
        "#     print(\"Filter configuration is correct.\")\n",
        "\n",
        "#     print(\"UNet model validation successful!\")\n",
        "\n",
        "# # Example Usage\n",
        "# n_classes = 2\n",
        "# in_channels = 1\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# validate_unet(model, filter_config)\n",
        "# Key Modifications:\n",
        "\n",
        "# validate_unet Arguments:\n",
        "\n",
        "# Added filter_config as an argument to the validate_unet function.\n",
        "# Filter Configuration Verification:\n",
        "\n",
        "# Added a check to verify that the filter configuration in the model matches the filter_config list passed to the function.\n",
        "# We extract the out_channels from the first convolutional layer of each encoder layer and compare them with the expected filter sizes.\n",
        "# Now, the validate_unet function will also check the filter configuration of your UNet model, ensuring that it matches the intended design. Please try running this updated code.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# tell me what this code does and what i need to do to complete it:\n",
        "\n",
        "\n",
        "\n",
        "# def train(trainData, model, optimizer, criterion, gpu=True, train_loss=[]):\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#         Train the model using provided training dataset.\n",
        "\n",
        "#         Params:\n",
        "\n",
        "#             trainData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "\n",
        "#                 custom dataset (AquacultureData).\n",
        "\n",
        "#             model - Choice of segmentation model.\n",
        "\n",
        "#             optimizer - Chosen optimization algorithm to update model parameters.\n",
        "\n",
        "#             criterion - Chosen function to calculate loss over training samples.\n",
        "\n",
        "#             gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "\n",
        "#             train_loss (empty list, optional) -- ???????????????????????????\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#     model.train()\n",
        "\n",
        "\n",
        "\n",
        "#     # Mini batch iteration\n",
        "\n",
        "#     train_epoch_loss = 0\n",
        "\n",
        "#     train_batches = len(trainData)\n",
        "\n",
        "\n",
        "\n",
        "#     for img_chips, labels in trainData:\n",
        "\n",
        "\n",
        "\n",
        "#         #Add code to put image and label on the 'device'.\n",
        "\n",
        "#         # one line for each.\n",
        "\n",
        "\n",
        "\n",
        "#         # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "\n",
        "#         # Pass image through the model to obtain prediction (1 line)\n",
        "\n",
        "#         # calculate loss based on 'model prediction' and label (1 line)\n",
        "\n",
        "#         # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "\n",
        "#         # do the backward pass to calculate gradients with respect to the loss\n",
        "\n",
        "#         # (1 line) update model weights by invoking the proper method on\n",
        "\n",
        "#         # 'optimizer'\n",
        "\n",
        "\n",
        "\n",
        "#     train_loss.append(train_epoch_loss / train_batches)\n",
        "\n",
        "#     print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))\n",
        "\n",
        "\n",
        "\n",
        "# Let's break down this train function and fill in the missing code to complete it.\n",
        "\n",
        "# Understanding the Code\n",
        "\n",
        "# This code defines a training loop for a segmentation model (likely your UNet). It iterates through batches of image chips and their corresponding labels from a DataLoader. Here's a step-by-step explanation:\n",
        "\n",
        "# model.train(): Sets the model to training mode, enabling features like dropout and batch normalization.\n",
        "\n",
        "# train_epoch_loss = 0: Initializes a variable to accumulate the loss over the entire training epoch.\n",
        "\n",
        "# train_batches = len(trainData): Gets the total number of batches in the training data.\n",
        "\n",
        "# for img_chips, labels in trainData:: Iterates through each batch of image chips and labels.\n",
        "\n",
        "# Missing Code: This is where you need to add the code for moving data to the device (GPU if available), clearing gradients, making predictions, calculating loss, performing backpropagation, and updating model weights.\n",
        "\n",
        "# train_loss.append(train_epoch_loss / train_batches): Appends the average loss for the epoch to the train_loss list.\n",
        "\n",
        "# print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches)): Prints the average training loss for the epoch.\n",
        "\n",
        "# Completing the Code\n",
        "\n",
        "# Here's the completed code with the missing parts filled in:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "\n",
        "# def train(trainData, model, optimizer, criterion, gpu=True, train_loss=[]):\n",
        "#     \"\"\"\n",
        "#         Train the model using provided training dataset.\n",
        "#         Params:\n",
        "#             trainData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "#                 custom dataset (AquacultureData).\n",
        "#             model - Choice of segmentation model.\n",
        "#             optimizer - Chosen optimization algorithm to update model parameters.\n",
        "#             criterion - Chosen function to calculate loss over training samples.\n",
        "#             gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "#             train_loss (empty list, optional) -- List to store training loss values.\n",
        "#     \"\"\"\n",
        "\n",
        "#     model.train()\n",
        "\n",
        "#     # Mini batch iteration\n",
        "#     train_epoch_loss = 0\n",
        "#     train_batches = len(trainData)\n",
        "\n",
        "#     device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu') # Define device\n",
        "\n",
        "#     for img_chips, labels in trainData:\n",
        "\n",
        "#         # Add code to put image and label on the 'device'.\n",
        "#         # one line for each.\n",
        "#         img_chips = img_chips.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Pass image through the model to obtain prediction (1 line)\n",
        "#         outputs = model(img_chips)\n",
        "\n",
        "#         # calculate loss based on 'model prediction' and label (1 line)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "#         train_epoch_loss += loss.item()\n",
        "\n",
        "#         # do the backward pass to calculate gradients with respect to the loss (1 line)\n",
        "#         loss.backward()\n",
        "\n",
        "#         # update model weights by invoking the proper method on 'optimizer' (1 line)\n",
        "#         optimizer.step()\n",
        "\n",
        "#     train_loss.append(train_epoch_loss / train_batches)\n",
        "#     print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))\n",
        "# Explanation of Added Code:\n",
        "\n",
        "# device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu'):\n",
        "\n",
        "# Determines whether to use GPU (if available and gpu=True) or CPU.\n",
        "# img_chips = img_chips.to(device) and labels = labels.to(device):\n",
        "\n",
        "# Moves the image chips and labels to the selected device (GPU or CPU).\n",
        "# optimizer.zero_grad():\n",
        "\n",
        "# Clears the gradients from the previous iteration.\n",
        "# outputs = model(img_chips):\n",
        "\n",
        "# Passes the image chips through the model to get the predictions.\n",
        "# loss = criterion(outputs, labels):\n",
        "\n",
        "# Calculates the loss using the specified criterion (e.g., cross-entropy loss).\n",
        "# train_epoch_loss += loss.item():\n",
        "\n",
        "# Adds the current batch's loss to the train_epoch_loss counter.\n",
        "# loss.backward():\n",
        "\n",
        "# Performs backpropagation to calculate the gradients of the loss with respect to the model's parameters.\n",
        "# optimizer.step():\n",
        "\n",
        "# Updates the model's parameters using the calculated gradients and the specified optimizer.\n",
        "# Key Points:\n",
        "\n",
        "# Device Handling: Ensure your data and model are on the same device (GPU or CPU).\n",
        "# Gradient Clearing: Always clear gradients before computing new ones.\n",
        "# Loss Calculation: Choose an appropriate loss function for your task (e.g., cross-entropy for segmentation).\n",
        "# Optimizer: Select an optimizer (e.g., Adam, SGD) and learning rate.\n",
        "# Let me know if you have any further questions or need additional clarification!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# now do the same thing for this\n",
        "\n",
        "\n",
        "\n",
        "# def validate(valData, model, criterion, device, val_loss=[]):\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#         Evaluate the model on separate Landsat scenes.\n",
        "\n",
        "#         Params:\n",
        "\n",
        "#             valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "\n",
        "#                 custom dataset(AquacultureData)\n",
        "\n",
        "#             model -- Choice of segmentation Model.\n",
        "\n",
        "#             criterion -- Chosen function to calculate loss over validation\n",
        "\n",
        "#                 samples.\n",
        "\n",
        "#             buffer: Buffer added to the targeted grid when creating dataset.\n",
        "\n",
        "#                 This allows loss to calculate at non-buffered region.\n",
        "\n",
        "#             gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "\n",
        "#             valLoss (empty list): To record average loss for each epoch\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "\n",
        "\n",
        "#     # mini batch iteration\n",
        "\n",
        "#     eval_epoch_loss = 0\n",
        "\n",
        "\n",
        "\n",
        "#     for img_chips, labels in valData:\n",
        "\n",
        "\n",
        "\n",
        "#         img = Variable(img_chips, requires_grad=False)\n",
        "\n",
        "#         label = Variable(labels, requires_grad=False)\n",
        "\n",
        "\n",
        "\n",
        "#         #Add code to put image and label on the 'device'.\n",
        "\n",
        "#         # one line for each.\n",
        "\n",
        "\n",
        "\n",
        "#         # Pass image through the model to obtain prediction (1 line)\n",
        "\n",
        "#         # calculate loss based on 'model prediction' and label (1 line)\n",
        "\n",
        "#         # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "\n",
        "\n",
        "\n",
        "#     print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "\n",
        "\n",
        "#     if val_loss != None:\n",
        "\n",
        "#         val_loss.append(float(eval_epoch_loss / len(valData)))\n",
        "\n",
        "\n",
        "\n",
        "# Let's complete the validate function, which is used for evaluating the model on a validation dataset.\n",
        "\n",
        "# Understanding the Code\n",
        "\n",
        "# This code defines a validation loop that iterates through batches of image chips and labels from a DataLoader. Here's a breakdown:\n",
        "\n",
        "# model.eval(): Sets the model to evaluation mode, disabling features like dropout and batch normalization.\n",
        "\n",
        "# eval_epoch_loss = 0: Initializes a variable to accumulate the loss over the entire validation epoch.\n",
        "\n",
        "# for img_chips, labels in valData:: Iterates through each batch of image chips and labels.\n",
        "\n",
        "# img = Variable(img_chips, requires_grad=False) and label = Variable(labels, requires_grad=False): Creates Variable objects from the image chips and labels. requires_grad=False indicates that gradients should not be computed for these variables.\n",
        "\n",
        "# Missing Code: This is where you need to add the code for moving data to the device, making predictions, and calculating loss.\n",
        "\n",
        "# print('validation loss: {}'.format(eval_epoch_loss / len(valData))): Prints the average validation loss for the epoch.\n",
        "\n",
        "# if val_loss != None: val_loss.append(float(eval_epoch_loss / len(valData))): Appends the average validation loss to the val_loss list if it's not None.\n",
        "\n",
        "# Completing the Code\n",
        "\n",
        "# Here's the completed code with the missing parts filled in:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "# def validate(valData, model, criterion, device, val_loss=[]):\n",
        "#     \"\"\"\n",
        "#         Evaluate the model on separate Landsat scenes.\n",
        "#         Params:\n",
        "#             valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "#                 custom dataset(AquacultureData)\n",
        "#             model -- Choice of segmentation Model.\n",
        "#             criterion -- Chosen function to calculate loss over validation\n",
        "#                 samples.\n",
        "#             device -- The device to use (e.g., 'cuda' or 'cpu').\n",
        "#             val_loss (empty list): To record average loss for each epoch\n",
        "#     \"\"\"\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "#     # mini batch iteration\n",
        "#     eval_epoch_loss = 0\n",
        "\n",
        "#     with torch.no_grad():  # Disable gradient calculation during validation\n",
        "#         for img_chips, labels in valData:\n",
        "\n",
        "#             img = Variable(img_chips, requires_grad=False)\n",
        "#             label = Variable(labels, requires_grad=False)\n",
        "\n",
        "#             # Add code to put image and label on the 'device'.\n",
        "#             # one line for each.\n",
        "#             img = img.to(device)\n",
        "#             label = label.to(device)\n",
        "\n",
        "#             # Pass image through the model to obtain prediction (1 line)\n",
        "#             outputs = model(img)\n",
        "\n",
        "#             # calculate loss based on 'model prediction' and label (1 line)\n",
        "#             loss = criterion(outputs, label)\n",
        "\n",
        "#             # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "#             eval_epoch_loss += loss.item()\n",
        "\n",
        "#     print('validation loss: {:.4f}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "#     if val_loss is not None:\n",
        "#         val_loss.append(float(eval_epoch_loss / len(valData)))\n",
        "# Explanation of Added Code:\n",
        "\n",
        "# with torch.no_grad()::\n",
        "\n",
        "# Disables gradient calculation during the validation phase. This is important because you don't need to compute gradients during validation, and it can save memory and speed up the process.\n",
        "# img = img.to(device) and label = label.to(device):\n",
        "\n",
        "# Moves the image chips and labels to the specified device (GPU or CPU).\n",
        "# outputs = model(img):\n",
        "\n",
        "# Passes the image chips through the model to get the predictions.\n",
        "# loss = criterion(outputs, label):\n",
        "\n",
        "# Calculates the loss using the specified criterion.\n",
        "# eval_epoch_loss += loss.item():\n",
        "\n",
        "# Adds the current batch's loss to the eval_epoch_loss counter.\n",
        "# Key Points:\n",
        "\n",
        "# torch.no_grad(): Always use torch.no_grad() during validation to disable gradient calculations.\n",
        "# Device Handling: Ensure your data and model are on the same device.\n",
        "# Loss Calculation: Choose an appropriate loss function.\n",
        "# This completed validate function should now correctly evaluate your model on the validation dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# lastly do the same thing for the code below and then add a summary of what you did for each task.\n",
        "\n",
        "\n",
        "\n",
        "# def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "\n",
        "#                   initial_lr, num_epochs):\n",
        "\n",
        "#     r\"\"\"\n",
        "\n",
        "#     Epoch iteration for train and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "#     Arguments:\n",
        "\n",
        "#     trainData (dataloader object): Batch grouped data to train the model.\n",
        "\n",
        "#     evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "\n",
        "#     model (pytorch.nn.module object): initialized model.\n",
        "\n",
        "#     initial_lr(float): The initial learning rate.\n",
        "\n",
        "#     num_epochs (int): User-defined number of epochs to run the model.\n",
        "\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#     train_loss = []\n",
        "\n",
        "#     val_loss = []\n",
        "\n",
        "\n",
        "\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     if device.type == \"cuda\":\n",
        "\n",
        "#         print('----------GPU available----------')\n",
        "\n",
        "#         gpu = True\n",
        "\n",
        "#         model = model.to(device)\n",
        "\n",
        "#     else:\n",
        "\n",
        "#         print('----------No GPU available, using CPU instead----------')\n",
        "\n",
        "#         gpu = False\n",
        "\n",
        "#         model = model\n",
        "\n",
        "\n",
        "\n",
        "#     writer = SummaryWriter(WorkingFolder)\n",
        "\n",
        "#     optimizer = optim.Adam(model.parameters(),\n",
        "\n",
        "#                            lr=initial_lr,\n",
        "\n",
        "#                            betas=(0.9, 0.999),\n",
        "\n",
        "#                            eps=1e-08,\n",
        "\n",
        "#                            weight_decay=5e-4,\n",
        "\n",
        "#                            amsgrad=False)\n",
        "\n",
        "\n",
        "\n",
        "#     scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "\n",
        "#                                           step_size=3,\n",
        "\n",
        "#                                           gamma=0.98)\n",
        "\n",
        "\n",
        "\n",
        "#     # Add your code here\n",
        "\n",
        "#     # you need to loop through the epochs and perform the following:\n",
        "\n",
        "#     # print the current epoch number out of the total epochs\n",
        "\n",
        "#     # (e.g. \"epoch: 2/10\")(1 line)\n",
        "\n",
        "#     # start the timer (1 line)\n",
        "\n",
        "#     # do model fit on the training data for single epoch (1 line)\n",
        "\n",
        "#     # do model validation on the validation dataset for one epoch (1 line)\n",
        "\n",
        "#     # take a step to update the 'scheduler'. (1 line)\n",
        "\n",
        "#     # Print the updated learning rate.\n",
        "\n",
        "#     # use \"add_scalars\" method with your writer to save the train and validation\n",
        "\n",
        "#     # loss to graph\n",
        "\n",
        "#     # using tensorboard package later.\n",
        "\n",
        "\n",
        "\n",
        "#     writer.close()\n",
        "\n",
        "\n",
        "\n",
        "#     duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "\n",
        "#     duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "\n",
        "#     print(\"--------------- Training finished in {} ---------------\"\\\n",
        "\n",
        "#           .format(duration_format))\n",
        "\n",
        "\n",
        "\n",
        "# All right, let's complete the epochIterater function, adding the necessary training and validation loops, and then provide a summary of all the tasks.\n",
        "\n",
        "# Completed Code\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "#                   initial_lr, num_epochs):\n",
        "#     r\"\"\"\n",
        "#     Epoch iteration for train and evaluation.\n",
        "\n",
        "#     Arguments:\n",
        "#     trainData (dataloader object): Batch grouped data to train the model.\n",
        "#     evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "#     model (pytorch.nn.module object): initialized model.\n",
        "#     initial_lr(float): The initial learning rate.\n",
        "#     num_epochs (int): User-defined number of epochs to run the model.\n",
        "#     \"\"\"\n",
        "\n",
        "#     train_loss = []\n",
        "#     val_loss = []\n",
        "\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     if device.type == \"cuda\":\n",
        "#         print('----------GPU available----------')\n",
        "#         gpu = True\n",
        "#         model = model.to(device)\n",
        "#     else:\n",
        "#         print('----------No GPU available, using CPU instead----------')\n",
        "#         gpu = False\n",
        "#         model = model\n",
        "\n",
        "#     writer = SummaryWriter(WorkingFolder)\n",
        "#     optimizer = optim.Adam(model.parameters(),\n",
        "#                                     lr=initial_lr,\n",
        "#                                     betas=(0.9, 0.999),\n",
        "#                                     eps=1e-08,\n",
        "#                                     weight_decay=5e-4,\n",
        "#                                     amsgrad=False)\n",
        "\n",
        "#     scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "#                                             step_size=3,\n",
        "#                                             gamma=0.98)\n",
        "\n",
        "#     # Add your code here\n",
        "#     # you need to loop through the epochs and perform the following:\n",
        "#     # print the current epoch number out of the total epochs\n",
        "#     # (e.g. \"epoch: 2/10\")(1 line)\n",
        "#     # start the timer (1 line)\n",
        "#     # do model fit on the training data for single epoch (1 line)\n",
        "#     # do model validation on the validation dataset for one epoch (1 line)\n",
        "#     # take a step to update the 'scheduler'. (1 line)\n",
        "#     # Print the updated learning rate.\n",
        "#     # use \"add_scalars\" method with your writer to save the train and validation\n",
        "#     # loss to graph\n",
        "#     # using tensorboard package later.\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
        "#         start_epoch = datetime.now()\n",
        "\n",
        "#         # Training\n",
        "#         train(trainData, model, optimizer, criterion, gpu=gpu, train_loss=train_loss)\n",
        "\n",
        "#         # Validation\n",
        "#         validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "#         scheduler.step()\n",
        "#         print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "#         writer.add_scalars('Loss', {'train': train_loss[-1], 'val': val_loss[-1]}, epoch)\n",
        "\n",
        "#         duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "#         duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "#         print(\"Epoch finished in {}\".format(duration_format))\n",
        "\n",
        "#     writer.close()\n",
        "\n",
        "#     duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "#     duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "#     print(\"--------------- Training finished in {} ---------------\".format(duration_format))\n",
        "# Explanation of Added Code\n",
        "\n",
        "# Epoch Loop:\n",
        "\n",
        "# for epoch in range(num_epochs): loops through the specified number of epochs.\n",
        "# print(f\"Epoch: {epoch + 1}/{num_epochs}\") prints the current epoch number.\n",
        "# start_epoch = datetime.now() starts the timer for each epoch.\n",
        "# Training and Validation:\n",
        "\n",
        "# train(trainData, model, optimizer, criterion, gpu=gpu, train_loss=train_loss) calls the train function to train the model for one epoch.\n",
        "# validate(valData, model, criterion, device, val_loss=val_loss) calls the validate function to evaluate the model for one epoch.\n",
        "# Scheduler and Learning Rate:\n",
        "\n",
        "# scheduler.step() updates the learning rate based on the scheduler's policy.\n",
        "# print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\") prints the updated learning rate.\n",
        "# TensorBoard Logging:\n",
        "\n",
        "# writer.add_scalars('Loss', {'train': train_loss[-1], 'val': val_loss[-1]}, epoch) logs the training and validation loss for the epoch to TensorBoard.\n",
        "# Epoch Duration:\n",
        "\n",
        "# Calculates and prints the duration of each epoch.\n",
        "# Training Duration:\n",
        "\n",
        "# Calculates and prints the total training duration.\n",
        "# Summary of Tasks\n",
        "\n",
        "# UNet Model Creation:\n",
        "\n",
        "# We created a UNet architecture based on the provided diagram, including encoder and decoder layers, skip connections, and a bottleneck.\n",
        "# We added parameters for filter_config and dropout_rate to make the model more flexible.\n",
        "# Model Validation:\n",
        "\n",
        "# We created a validate_unet function to verify the model's structure, including the number of layers, kernel sizes, strides, padding, and filter configuration.\n",
        "# Training Loop:\n",
        "\n",
        "# We created a train function to train the model using a DataLoader, optimizer, and loss criterion.\n",
        "# We added code to move data to the device, clear gradients, make predictions, calculate loss, perform backpropagation, and update model weights.\n",
        "# Validation Loop:\n",
        "\n",
        "# We created a validate function to evaluate the model on a validation dataset.\n",
        "# We added code to move data to the device, make predictions, and calculate loss.\n",
        "# Epoch Iteration:\n",
        "\n",
        "# We created an epochIterater function to orchestrate the training and validation loops over multiple epochs.\n",
        "# We added code to print epoch numbers, start timers, call the train and validate functions, update the learning rate scheduler, log losses to TensorBoard, and print training duration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# please consider the training and validation and epoch iterator functions you just gave me and give me code to train and validate the datasets provided in the screenshot I'll give. Id like you to use the balanced cross entropy loss function defined below for the loss function:\n",
        "\n",
        "\n",
        "\n",
        "# class BalancedCrossEntropyLoss(nn.Module):\n",
        "\n",
        "#     '''\n",
        "\n",
        "#     Balanced cross entropy loss by weighting of inverse class ratio\n",
        "\n",
        "#     Params:\n",
        "\n",
        "#         ignore_index (int): Class index to ignore\n",
        "\n",
        "#         reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "\n",
        "#             return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "\n",
        "#     Returns:\n",
        "\n",
        "#         Loss tensor according to arg reduction\n",
        "\n",
        "#     '''\n",
        "\n",
        "\n",
        "\n",
        "#     def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "\n",
        "#         super(BalancedCrossEntropyLoss, self).__init__()\n",
        "\n",
        "#         self.ignore_index = ignore_index\n",
        "\n",
        "#         self.reduction = reduction\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, predict, target):\n",
        "\n",
        "#         # get class weights\n",
        "\n",
        "#         class_counts = torch.bincount(target.view(-1),\n",
        "\n",
        "#                                       minlength=predict.shape[1])\n",
        "\n",
        "#         class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "\n",
        "\n",
        "#         # set weight of ignore index to 0\n",
        "\n",
        "#         if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "\n",
        "#             class_weights[self.ignore_index] = 0\n",
        "\n",
        "\n",
        "\n",
        "#         # normalize weights\n",
        "\n",
        "#         class_weights /= torch.sum(class_weights)\n",
        "\n",
        "\n",
        "\n",
        "#         # apply class weights to loss function\n",
        "\n",
        "#         loss_fn = nn.CrossEntropyLoss(weight=class_weights,\n",
        "\n",
        "#                                       ignore_index=self.ignore_index,\n",
        "\n",
        "#                                       reduction=self.reduction)\n",
        "\n",
        "\n",
        "\n",
        "#         return loss_fn(predict, target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Absolutely! Let's put together the code to train and validate your UNet model using the provided datasets and the BalancedCrossEntropyLoss function.\n",
        "\n",
        "# 1. Import Necessary Libraries and Define Loss Function:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer, UNet classes defined\n",
        "# # and the train, validate, epochIterater functions from the previous response.\n",
        "\n",
        "# class BalancedCrossEntropyLoss(nn.Module):\n",
        "#     '''\n",
        "#     Balanced cross entropy loss by weighting of inverse class ratio\n",
        "#     Params:\n",
        "#         ignore_index (int): Class index to ignore\n",
        "#         reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "#             return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "#     Returns:\n",
        "#         Loss tensor according to arg reduction\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "#         super(BalancedCrossEntropyLoss, self).__init__()\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, predict, target):\n",
        "#         # get class weights\n",
        "#         class_counts = torch.bincount(target.view(-1),\n",
        "#                                         minlength=predict.shape[1])\n",
        "#         class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "#         # set weight of ignore index to 0\n",
        "#         if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "#             class_weights[self.ignore_index] = 0\n",
        "\n",
        "#         # normalize weights\n",
        "#         class_weights /= torch.sum(class_weights)\n",
        "\n",
        "#         # apply class weights to loss function\n",
        "#         loss_fn = nn.CrossEntropyLoss(weight=class_weights,\n",
        "#                                         ignore_index=self.ignore_index,\n",
        "#                                         reduction=self.reduction)\n",
        "\n",
        "#         return loss_fn(predict, target)\n",
        "# 2. Load Datasets and Define DataLoaders:\n",
        "\n",
        "# Python\n",
        "\n",
        "# # Assuming your dataset class is named ActiveLoadingDataset\n",
        "# # and is defined in a separate file (e.g., dataset.py)\n",
        "\n",
        "# from dataset import ActiveLoadingDataset  # Import your dataset class\n",
        "\n",
        "# src_dir = \"/content/gdrive/MyDrive/adleo/data/\"\n",
        "# dataset_name = \"PondDataset\"\n",
        "# transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "# train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\",\n",
        "#                                      apply_normalization=False,\n",
        "#                                      transform=transform)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# validation_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"validation\",\n",
        "#                                           apply_normalization=False)\n",
        "# val_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
        "# 3. Initialize Model, Loss Function, and Training Parameters:\n",
        "\n",
        "# Python\n",
        "\n",
        "# # Assuming your UNet class is defined in a separate file (e.g., model.py)\n",
        "# from model import UNet  # Import your UNet model class\n",
        "\n",
        "# n_classes = 2  # Number of segmentation classes (adjust as needed)\n",
        "# in_channels = 3  # Number of input channels (adjust as needed)\n",
        "# filter_config = [64, 128, 256, 512, 1024]  # Adjust filter sizes as needed\n",
        "# dropout_rate = 0.5  # Adjust dropout rate as needed\n",
        "# initial_lr = 0.001  # Adjust learning rate as needed\n",
        "# num_epochs = 10  # Adjust number of epochs as needed\n",
        "# working_folder = \"./runs\" # Adjust working folder for TensorBoard logs\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# criterion = BalancedCrossEntropyLoss()\n",
        "# 4. Train and Validate the Model:\n",
        "\n",
        "# Python\n",
        "\n",
        "# epochIterater(train_loader, val_loader, model, criterion, working_folder, initial_lr, num_epochs)\n",
        "# Complete Code:\n",
        "\n",
        "# Python\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # Assuming you have the conv_block, EncoderLayer, DecoderLayer, UNet classes defined\n",
        "# # and the train, validate, epochIterater functions from the previous response.\n",
        "\n",
        "# class BalancedCrossEntropyLoss(nn.Module):\n",
        "#     '''\n",
        "#     Balanced cross entropy loss by weighting of inverse class ratio\n",
        "#     Params:\n",
        "#         ignore_index (int): Class index to ignore\n",
        "#         reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "#             return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "#     Returns:\n",
        "#         Loss tensor according to arg reduction\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "#         super(BalancedCrossEntropyLoss, self).__init__()\n",
        "#         self.ignore_index = ignore_index\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, predict, target):\n",
        "#         # get class weights\n",
        "#         class_counts = torch.bincount(target.view(-1),\n",
        "#                                         minlength=predict.shape[1])\n",
        "#         class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "#         # set weight of ignore index to 0\n",
        "#         if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "#             class_weights[self.ignore_index] = 0\n",
        "\n",
        "#         # normalize weights\n",
        "#         class_weights /= torch.sum(class_weights)\n",
        "\n",
        "#         # apply class weights to loss function\n",
        "#         loss_fn = nn.CrossEntropyLoss(weight=class_weights,\n",
        "#                                         ignore_index=self.ignore_index,\n",
        "#                                         reduction=self.reduction)\n",
        "\n",
        "#         return loss_fn(predict, target)\n",
        "\n",
        "\n",
        "# # Assuming your dataset class is named ActiveLoadingDataset\n",
        "# # and is defined in a separate file (e.g., dataset.py)\n",
        "# from dataset import ActiveLoadingDataset  # Import your dataset class\n",
        "\n",
        "# src_dir = \"/content/gdrive/MyDrive/adleo/data/\"\n",
        "# dataset_name = \"PondDataset\"\n",
        "# transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "# train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\",\n",
        "#                                      apply_normalization=False,\n",
        "#                                      transform=transform)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# validation_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"validation\",\n",
        "#                                           apply_normalization=False)\n",
        "# val_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# # Assuming your UNet class is defined in a separate file (e.g., model.py)\n",
        "# from model import UNet  # Import your UNet model class\n",
        "\n",
        "# n_classes = 2  # Number of segmentation classes (adjust as needed)\n",
        "# in_channels = 3  # Number of input channels (adjust as needed)\n",
        "# filter_config = [64, 128, 256, 512, 1024]  # Adjust filter sizes as needed\n",
        "# dropout_rate = 0.5  # Adjust dropout rate as needed\n",
        "# initial_lr = 0.001  # Adjust learning rate as needed\n",
        "# num_epochs = 10  # Adjust number of epochs as needed\n",
        "# working_folder = \"./runs\" # Adjust working folder for TensorBoard logs\n",
        "\n",
        "# model = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# criterion = BalancedCrossEntropyLoss()\n",
        "\n",
        "# epochIterater(train_loader, val_loader, model, criterion, working_folder, initial_lr, num_epochs)\n",
        "# Important Notes:\n",
        "\n",
        "# Dataset and Model Files: Ensure that your dataset.py and model.py files are in the same directory as your training script or in a location where they can be imported correctly.\n",
        "# Adjust Parameters: Adjust the parameters (e.g., n_classes, in_channels, filter_config, dropout_rate, initial_lr, num_epochs) based on your specific dataset and task.\n",
        "# Device Handling: The epochIterater function will automatically use a GPU if available.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# please reprint this code block:\n",
        "\n",
        "# from pathlib import Path\n",
        "\n",
        "# import rasterio\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# import torch\n",
        "\n",
        "\n",
        "\n",
        "# class ActiveLoadingDataset(Dataset):\n",
        "\n",
        "#     def __init__(self, src_dir, dataset_name, usage, apply_normalization=False,\n",
        "\n",
        "#                  transform=None, **kargs):\n",
        "\n",
        "#         \"\"\"\n",
        "\n",
        "#         src_dir (str or path): Root of resource directory.\n",
        "\n",
        "#         dataset_name (str): Name of the training/validation dataset containing structured folders for image, label.\n",
        "\n",
        "#         usage (str): Either 'train' or 'validation'.\n",
        "\n",
        "#         transform (list): Each element is a string name of the transformation to be used.\n",
        "\n",
        "#         \"\"\"\n",
        "\n",
        "#         self.src_dir = src_dir\n",
        "\n",
        "#         self.dataset_name = dataset_name\n",
        "\n",
        "#         self.apply_normalization = apply_normalization\n",
        "\n",
        "#         self.transform = transform\n",
        "\n",
        "\n",
        "\n",
        "#         self.usage = usage\n",
        "\n",
        "#         assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "\n",
        "\n",
        "\n",
        "#         # Define image and label directories\n",
        "\n",
        "#         img_dir = Path(src_dir) / self.dataset_name / self.usage / \"images\"\n",
        "\n",
        "#         lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "\n",
        "\n",
        "\n",
        "#         # Retrieve and sort all .tif files for images and labels\n",
        "\n",
        "#         self.img_paths = sorted([str(p) for p in img_dir.glob(\"*.tif\")])\n",
        "\n",
        "#         self.lbl_paths = sorted([str(p) for p in lbl_dir.glob(\"*.tif\")])\n",
        "\n",
        "\n",
        "\n",
        "#         self.img_chips = []\n",
        "\n",
        "#         self.lbl_chips = []\n",
        "\n",
        "\n",
        "\n",
        "#         # Load image and label data\n",
        "\n",
        "#         for img_path, lbl_path in zip(self.img_paths, self.lbl_paths):\n",
        "\n",
        "#             # Read image and label files\n",
        "\n",
        "#             with rasterio.open(img_path) as img_file:\n",
        "\n",
        "#                 img = img_file.read()  # CHW format\n",
        "\n",
        "#                 if apply_normalization:\n",
        "\n",
        "#                     img = self.min_max_normalize(img)\n",
        "\n",
        "\n",
        "\n",
        "#             with rasterio.open(lbl_path) as lbl_file:\n",
        "\n",
        "#                 lbl = lbl_file.read(1)  # Assume single-channel label (HW format)\n",
        "\n",
        "\n",
        "\n",
        "#             # Append to the respective lists\n",
        "\n",
        "#             self.img_chips.append(img)  # Images remain in CHW format\n",
        "\n",
        "#             self.lbl_chips.append(lbl)  # Labels remain in HW format\n",
        "\n",
        "\n",
        "\n",
        "# to avoid this error:\n",
        "\n",
        "\n",
        "\n",
        "# ----------No GPU available, using CPU instead----------\n",
        "\n",
        "# Epoch: 1/10\n",
        "\n",
        "# ---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)<ipython-input-75-f56ec63c3cee> in <cell line: 0>() 11 criterion = BalancedCrossEntropyLoss() 12 ---> 13 epochIterater(train_loader, val_loader, model, criterion, working_folder, initial_lr, num_epochs)\n",
        "\n",
        "# 9 frames\n",
        "\n",
        "# /usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py in collate_tensor_fn(batch, collate_fn_map) 270 storage = elem._typed_storage()._new_shared(numel, device=elem.device) 271 out = elem.new(storage).resize_(len(batch), *list(elem.size()))--> 272 return torch.stack(batch, 0, out=out) 273 274 RuntimeError: stack expects each tensor to be equal size, but got [6, 256, 256] at entry 0 and [256, 6, 256] at entry 2\n",
        "\n",
        "\n",
        "\n",
        "# Python\n",
        "\n",
        "# from pathlib import Path\n",
        "# import rasterio\n",
        "# import numpy as np\n",
        "# from torch.utils.data import Dataset\n",
        "# import torch\n",
        "\n",
        "# class ActiveLoadingDataset(Dataset):\n",
        "#     def __init__(self, src_dir, dataset_name, usage, apply_normalization=False,\n",
        "#                  transform=None, **kargs):\n",
        "#         \"\"\"\n",
        "#         src_dir (str or path): Root of resource directory.\n",
        "#         dataset_name (str): Name of the training/validation dataset containing structured folders for image, label.\n",
        "#         usage (str): Either 'train' or 'validation'.\n",
        "#         transform (list): Each element is a string name of the transformation to be used.\n",
        "#         \"\"\"\n",
        "#         self.src_dir = src_dir\n",
        "#         self.dataset_name = dataset_name\n",
        "#         self.apply_normalization = apply_normalization\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.usage = usage\n",
        "#         assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "\n",
        "#         # Define image and label directories\n",
        "#         img_dir = Path(src_dir) / self.dataset_name / self.usage / \"images\"\n",
        "#         lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "\n",
        "#         # Retrieve and sort all .tif files for images and labels\n",
        "#         self.img_paths = sorted([str(p) for p in img_dir.glob(\"*.tif\")])\n",
        "#         self.lbl_paths = sorted([str(p) for p in lbl_dir.glob(\"*.tif\")])\n",
        "\n",
        "#         self.img_chips = []\n",
        "#         self.lbl_chips = []\n",
        "\n",
        "#         # Load image and label data\n",
        "#         for img_path, lbl_path in zip(self.img_paths, self.lbl_paths):\n",
        "#             # Read image and label files\n",
        "#             with rasterio.open(img_path) as img_file:\n",
        "#                 img = img_file.read()  # CHW format\n",
        "#                 if apply_normalization:\n",
        "#                     img = self.min_max_normalize(img)\n",
        "\n",
        "#             with rasterio.open(lbl_path) as lbl_file:\n",
        "#                 lbl = lbl_file.read(1)  # Assume single-channel label (HW format)\n",
        "\n",
        "#             # Append to the respective lists\n",
        "#             self.img_chips.append(img)  # Images remain in CHW format\n",
        "#             self.lbl_chips.append(lbl)  # Labels remain in HW format\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_chips)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img = torch.tensor(self.img_chips[idx], dtype=torch.float32)\n",
        "#         lbl = torch.tensor(self.lbl_chips[idx], dtype=torch.long)\n",
        "\n",
        "#         # Apply transformations if specified\n",
        "#         if self.transform:\n",
        "#             img, lbl = self.apply_transforms(img, lbl)\n",
        "\n",
        "#         return img, lbl\n",
        "\n",
        "#     def min_max_normalize(self, image):\n",
        "#         # Normalize each channel independently\n",
        "#         normalized_image = np.zeros_like(image, dtype=np.float32)\n",
        "#         for i in range(image.shape[0]):\n",
        "#             min_val = np.min(image[i])\n",
        "#             max_val = np.max(image[i])\n",
        "#             if max_val - min_val != 0:\n",
        "#                 normalized_image[i] = (image[i] - min_val) / (max_val - min_val)\n",
        "#             else:\n",
        "#                 normalized_image[i] = 0  # Handle cases with constant values\n",
        "#         return normalized_image\n",
        "\n",
        "#     def apply_transforms(self, img, lbl):\n",
        "#         # Implement your transformations here based on self.transform\n",
        "#         # Example transformations (you'll need to adapt these):\n",
        "#         if \"hflip\" in self.transform:\n",
        "#             if torch.rand(1) > 0.5:\n",
        "#                 img = torch.flip(img, dims=[2])\n",
        "#                 lbl = torch.flip(lbl, dims=[1])\n",
        "#         if \"vflip\" in self.transform:\n",
        "#             if torch.rand(1) > 0.5:\n",
        "#                 img = torch.flip(img, dims=[1])\n",
        "#                 lbl = torch.flip(lbl, dims=[0])\n",
        "#         if \"rotate\" in self.transform:\n",
        "#             angle = torch.randint(0, 4, (1,)) * 90\n",
        "#             img = torch.rot90(img, k=angle.item(), dims=[1, 2])\n",
        "#             lbl = torch.rot90(lbl, k=angle.item(), dims=[0, 1])\n",
        "\n",
        "#         return img, lbl\n",
        "# Key Changes to Address the Error:\n",
        "\n",
        "# __len__ and __getitem__ Methods:\n",
        "\n",
        "# The Dataset class requires __len__ and __getitem__ methods to function with DataLoader.\n",
        "# __len__ returns the number of samples in the dataset.\n",
        "# __getitem__ retrieves a sample (image and label) at a given index, converts them to tensors, and applies transformations.\n",
        "# Tensor Conversion:\n",
        "\n",
        "# In __getitem__, self.img_chips[idx] and self.lbl_chips[idx] are converted to PyTorch tensors using torch.tensor(). This is crucial for working with PyTorch models and DataLoader.\n",
        "# Images are converted to torch.float32 and labels to torch.long.\n",
        "# Transformation Application:\n",
        "\n",
        "# The apply_transforms method is implemented to apply the specified transformations (e.g., horizontal flip, vertical flip, rotation) to the images and labels. You'll need to customize this method based on your specific transformation requirements.\n",
        "# Min-Max Normalization:\n",
        "\n",
        "# The min max normalization method was adjusted to handle the case where the max and min are the same value.\n",
        "# By making these changes, the ActiveLoadingDataset class should now work correctly with DataLoader and avoid the \"stack expects each tensor to be equal size\" error.\n",
        "\n",
        "\n",
        "#                                    Please explain the error:\n",
        "\n",
        "# ValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [256] and output size of torch.Size([256, 256]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.\n",
        "# Hypothesis\n",
        "# The error message \"ValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [256] and output size of torch.Size([256, 256]).\" indicates that the input tensor to the F.interpolate function within the BinaryTverskyFocalLoss class has an incorrect shape.\n",
        "\n",
        "# The error suggests that the input tensor expected by the interpolate function must be of shape (N, C, d1, d2, ...,dK) and in your case, the predict tensor is having a shape of (N, d1) which is causing this error. F.interpolate requires at least 2 spatial dimensions (H, W), but the input to F.interpolate only has 1 spatial dimension.\n",
        "\n",
        "# The problem originates from the BinaryTverskyFocalLoss.forward function.\n",
        "\n",
        "\n",
        "# predict = F.interpolate(predict, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
        "# Use code with caution\n",
        "# Here predict is output from your model, which is expected to have a spatial shape. However, it is likely being reshaped to a 1-dimensional tensor, which is then passed to F.interpolate. The error occurs because F.interpolate requires at least 2 spatial dimensions for interpolation, whereas the reshaped predict has only 1.\n",
        "\n",
        "# Suggested Changes\n",
        "\n",
        "# class BinaryTverskyFocalLoss(nn.Module):\n",
        "#     '''\n",
        "#     Pytorch version of tversky focal loss proposed in paper\n",
        "#     'A novel focal Tversky loss function and improved Attention U-Net for lesion\n",
        "#     segmentation'\n",
        "#     (https://arxiv.org/abs/1810.07842)\n",
        "#     Params:\n",
        "#         smooth (float): A float number to smooth loss, and avoid NaN error,\n",
        "#             default: 1\n",
        "#         alpha (float): Hyperparameters alpha, paired with (1 - alpha) to shift\n",
        "#             emphasis to improve recall\n",
        "#         gamma (float): Tversky index, default: 1.33\n",
        "#         predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "#         target (torch.tensor): Target tensor either in shape [N,*] or of same\n",
        "#             shape with predict\n",
        "#     Returns:\n",
        "#         Loss tensor\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, smooth=1, alpha=0.7, gamma=1.33):\n",
        "#         super(BinaryTverskyFocalLoss, self).__init__()\n",
        "#         self.smooth = smooth\n",
        "#         self.alpha = alpha\n",
        "#         self.beta = 1 - self.alpha\n",
        "#         self.gamma = gamma\n",
        "\n",
        "\n",
        "#     def forward(self, predict, target):\n",
        "#         assert predict.shape[0] == target.shape[0], \\\n",
        "#             \"predict & target batch size do not match.\"\n",
        "\n",
        "#         # Instead of interpolating individual channels, interpolate the whole prediction\n",
        "#         # predict has shape (batch_size, n_classes, height, width)\n",
        "#         # target has shape (batch_size, height, width)\n",
        "#         # Ensure predict has at least 2 spatial dimensions before interpolation\n",
        "#         if len(predict.shape) < 4:  # Check if spatial dimensions are missing\n",
        "#             predict = predict.unsqueeze(1) # Add a spatial dimension\n",
        "\n",
        "#         predict = F.interpolate(predict, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "#         # Reshape predict and target for element-wise operations\n",
        "#         predict = predict.contiguous().view(-1)\n",
        "#         target = target.contiguous().view(-1)\n",
        "\n",
        "#         # Compute the numerator and denominator for the Tversky index\n",
        "#         num = (predict * target).sum() + self.smooth\n",
        "#         den = (predict * target).sum() + self.alpha * ((1 - predict) * target)\\\n",
        "#             .sum() + self.beta * (predict * (1 - target)).sum() + self.smooth\n",
        "\n",
        "#         # Compute the Tversky index and the loss\n",
        "#         tversky = 1 - (num / den)  # Modified calculation\n",
        "#         loss = torch.pow(tversky, 1 / self.gamma)\n",
        "\n",
        "#         return loss\n",
        "# Use code with caution\n",
        "# Explanation:\n",
        "\n",
        "# Check for Spatial Dimensions: Added a condition to check if predict has fewer than 4 dimensions (batch size, channels, height, width). If so, it indicates missing spatial dimensions.\n",
        "\n",
        "# Add Spatial Dimension: If spatial dimensions are missing, predict.unsqueeze(1) is used to add a spatial dimension (channels) to the tensor. This changes the shape from (N, d1) to (N, 1, d1). Now, when you pass this reshaped prediction to interpolate it would correctly interpret d1 as a spatial dimension.\n",
        "\n",
        "# With these changes, the input to F.interpolate will have at least 2 spatial dimensions (channels, height), resolving the ValueError.\n",
        "\n",
        "# Sources\n",
        "# discuss.pytorch.org/t/shape-mismatch-error/181785\n",
        "# discuss.pytorch.org/t/functional-spatial-dimensions/175791\n",
        "# ClarkCGA/kenya-multitemporal-crop-segmentation-baseline\n",
        "# agroimpacts/nmeo\n",
        "# Rate this answer\n",
        "# since the binary tversky focal loss relies on the tversky focal loss do i need to change anything in that code as well?\n",
        "\n",
        "# Yes, you're right. Since TverskyFocalLoss uses BinaryTverskyFocalLoss internally, you need to ensure that the predict tensor passed to BinaryTverskyFocalLoss from TverskyFocalLoss has the correct dimensions. Here's the updated TverskyFocalLoss class:\n",
        "\n",
        "\n",
        "# class TverskyFocalLoss(nn.Module):\n",
        "#     '''\n",
        "#     Tversky focal loss\n",
        "#     Params:\n",
        "#         weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "#         ignore_index (int): Class index to ignore\n",
        "#         predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "#         target (torch.tensor): Target tensor either in shape [N,*] or of same\n",
        "#             shape with predict\n",
        "#         other args pass to BinaryTverskyFocalLoss\n",
        "#     Returns:\n",
        "#         same as BinaryTverskyFocalLoss\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
        "#         super(TverskyFocalLoss, self).__init__()\n",
        "#         self.kwargs = kwargs\n",
        "#         self.weight = weight\n",
        "#         self.ignore_index = ignore_index\n",
        "\n",
        "#     def forward(self, predict, target):\n",
        "#         nclass = predict.shape[1]  # Number of classes in the prediction\n",
        "\n",
        "#         # Instead of one-hot encoding the target, we ensure that the target\n",
        "#         # has the same shape as the prediction's spatial dimensions but only\n",
        "#         # one channel\n",
        "#         if len(predict.shape) == 4:\n",
        "#             # Ensure target has the same spatial dimensions as prediction but only 1 channel\n",
        "#             target = target.unsqueeze(1)  # Add a channel dimension\n",
        "#         else:\n",
        "#             assert 'predict shape not applicable'\n",
        "\n",
        "#         tversky = BinaryTverskyFocalLoss(**self.kwargs)\n",
        "#         total_loss = 0\n",
        "#         weight = torch.Tensor([1. / nclass] * nclass).cuda() if self.weight is None else self.weight\n",
        "#         predict = F.softmax(predict, dim=1)  # Apply softmax along the channel dimension\n",
        "\n",
        "#         for i in range(nclass):\n",
        "#             if i != self.ignore_index:\n",
        "#                 # Select the corresponding channel from prediction and target\n",
        "#                 # predict[:, i] will have shape (batch_size, height, width)\n",
        "#                 # target[:, 0] will also have shape (batch_size, height, width) because we added a channel dim and select the first\n",
        "\n",
        "#                 # Check if predict[:, i] has fewer than 3 dimensions (batch size, height, width).\n",
        "#                 # If so, it indicates missing spatial dimensions.\n",
        "#                 if len(predict[:, i].shape) < 3:\n",
        "#                   # Add a spatial dimension\n",
        "#                   predict_channel = predict[:, i].unsqueeze(1)\n",
        "#                 else:\n",
        "#                   predict_channel = predict[:, i]\n",
        "\n",
        "#                 tversky_loss = tversky(predict_channel, target[:, 0])\n",
        "#                 assert weight.shape[0] == nclass, \\\n",
        "#                     'Expect weight shape [{}], get[{}]'.format(nclass, weight.shape[0])\n",
        "#                 tversky_loss *= weight[i]\n",
        "#                 total_loss += tversky_loss\n",
        "\n",
        "#         return total_loss\n",
        "# Use code with caution\n",
        "# Explanation of Changes\n",
        "\n",
        "# Check for Spatial Dimensions: Inside the for loop, a condition checks if the extracted channel predict[:, i] has fewer than 3 dimensions (batch size, height, width). If so, it indicates missing spatial dimensions.\n",
        "# Add Spatial Dimension: If spatial dimensions are missing, predict_channel = predict[:, i].unsqueeze(1) is used to add a spatial dimension to the channel tensor. This transforms the channel tensor's shape from (N, d1) to (N, 1, d1), ensuring compatibility with BinaryTverskyFocalLoss.\n",
        "# Pass to BinaryTverskyFocalLoss: The reshaped predict_channel (with at least 2 spatial dimensions) is then passed to tversky (which is an instance of BinaryTverskyFocalLoss), preventing the ValueError\n",
        "# By incorporating these changes in both BinaryTverskyFocalLoss and TverskyFocalLoss, you ensure that the input to F.interpolate always has the correct number of spatial dimensions, preventing the ValueError.\n",
        "\n",
        "# Sources\n",
        "# Rate this answer\n",
        "# can you alter this code so it runs for my second model run:\n",
        "\n",
        "# Save model results 2\n",
        "# results_folder = Path(src_dir) / \"model2results\" # Create a Path object for the results folder results_folder.mkdir(parents=True, exist_ok=True) model_save_path = results_folder / \"model_weights.pth\" # Define the save path torch.save(model.state_dict(), model_save_path) print(f\"Model results saved to: {results_folder}\")\n",
        "\n",
        "# Okay, here's the altered code to save the results for your second model run:\n",
        "\n",
        "\n",
        "# # Save model results 2\n",
        "# results_folder = Path(src_dir) / \"model2results\"  # Create a Path object for the results folder\n",
        "# results_folder.mkdir(parents=True, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "# model_save_path = results_folder / \"model_weights.pth\"  # Define the save path\n",
        "# torch.save(model.state_dict(), model_save_path)  # Save the model's state dictionary\n",
        "# print(f\"Model results saved to: {results_folder}\")  # Print the save location\n",
        "# Use code with caution\n",
        "# Explanation of changes:\n",
        "\n",
        "# Results Folder: The results_folder variable is now assigned to Path(src_dir) / \"model2results\", creating a dedicated folder for the second model's results within your src_dir.\n",
        "# Folder Creation: The results_folder.mkdir(parents=True, exist_ok=True) line ensures that the \"model2results\" folder is created if it doesn't exist, preventing errors if the directory structure is missing. Additionally, parents=True ensures that intermediate parent directories are created if necessary.\n",
        "# Save Path: The model_save_path variable is updated to results_folder / \"model_weights.pth\", ensuring the model's weights are saved within the \"model2results\" folder.\n",
        "# How to use:\n",
        "\n",
        "# Place the code: Insert this code snippet after the training and validation process for your second model run.\n",
        "# Ensure src_dir is defined: Make sure the src_dir variable is correctly defined and points to the desired location in your file system.\n",
        "# By running this code after training your second model, you'll save the model's state dictionary (containing the learned weights) to a dedicated folder named \"model2results,\" with the file name \"model_weights.pth.\" This helps in organizing and managing the results of different model runs effectively.\n",
        "\n",
        "# Sources\n",
        "# Rate this answer\n",
        "# given this evaluator class: class Evaluator(object): def init(self, num_class): self.num_class = num_class self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "# def Pixel_Accuracy(self):\n",
        "#     Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "#     return Acc\n",
        "\n",
        "# def Pixel_Accuracy_Class(self):\n",
        "#     Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "#     Acc = np.nanmean(Acc)\n",
        "#     return Acc\n",
        "\n",
        "# def Mean_Intersection_over_Union(self):\n",
        "#     MIoU = np.diag(self.confusion_matrix) / (\n",
        "#                 np.sum(self.confusion_matrix, axis=1) +\n",
        "#                 np.sum(self.confusion_matrix, axis=0) -\n",
        "#                 np.diag(self.confusion_matrix))\n",
        "#     MIoU = np.nanmean(MIoU)\n",
        "#     return MIoU\n",
        "\n",
        "# def Frequency_Weighted_Intersection_over_Union(self):\n",
        "#     freq = np.sum(self.confusion_matrix, axis=1) /\\\n",
        "#         np.sum(self.confusion_matrix)\n",
        "#     iu = np.diag(self.confusion_matrix) / (\n",
        "#                 np.sum(self.confusion_matrix, axis=1) +\n",
        "#                 np.sum(self.confusion_matrix, axis=0) -\n",
        "#                 np.diag(self.confusion_matrix))\n",
        "\n",
        "#     FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "#     return FWIoU\n",
        "\n",
        "# def _generate_matrix(self, gt_image, pre_image):\n",
        "#     mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "#     label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "#     count = np.bincount(label, minlength=self.num_class**2)\n",
        "#     confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "#     return confusion_matrix\n",
        "\n",
        "# def add_batch(self, gt_image, pre_image):\n",
        "#     assert gt_image.shape == pre_image.shape\n",
        "#     self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "# def reset(self):\n",
        "#     self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
        "# give me the code to run this:\n",
        "\n",
        "# def do_accuracy_evaluation(model, dataloader, num_classes, filename):\n",
        "\n",
        "# # fill in the re\n",
        "# Okay, here's the code to implement the do_accuracy_evaluation function using the provided Evaluator class:\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "# def do_accuracy_evaluation(model, dataloader, num_classes, filename):\n",
        "#     \"\"\"\n",
        "#     Evaluates the model's accuracy metrics on the given dataloader.\n",
        "\n",
        "#     Args:\n",
        "#         model: The trained PyTorch model.\n",
        "#         dataloader: The DataLoader for the evaluation dataset.\n",
        "#         num_classes: The number of segmentation classes.\n",
        "#         filename: The filename to save the evaluation results.\n",
        "#     \"\"\"\n",
        "\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)  # Move the model to the appropriate device\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     evaluator = Evaluator(num_classes)  # Initialize the Evaluator\n",
        "\n",
        "#     with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "#         for img_chips, labels in dataloader:\n",
        "#             img_chips = img_chips.to(device)  # Move data to device\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(img_chips)  # Get model predictions\n",
        "#             _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
        "\n",
        "#             # Convert tensors to NumPy arrays and move to CPU\n",
        "#             predicted = predicted.cpu().numpy()\n",
        "#             labels = labels.cpu().numpy()\n",
        "\n",
        "#             evaluator.add_batch(labels, predicted)  # Update the confusion matrix\n",
        "\n",
        "#     # Calculate accuracy metrics\n",
        "#     acc = evaluator.Pixel_Accuracy()\n",
        "#     acc_class = evaluator.Pixel_Accuracy_Class()\n",
        "#     mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "#     FWIoU = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "#     # Print and save the results\n",
        "#     print(\"Accuracy:\", acc)\n",
        "#     print(\"Accuracy Class:\", acc_class)\n",
        "#     print(\"Mean IoU:\", mIoU)\n",
        "#     print(\"Frequency Weighted IoU:\", FWIoU)\n",
        "\n",
        "#     with open(filename, 'w') as f:  # Save results to a file\n",
        "#         f.write(f\"Accuracy: {acc}\\n\")\n",
        "#         f.write(f\"Accuracy Class: {acc_class}\\n\")\n",
        "#         f.write(f\"Mean IoU: {mIoU}\\n\")\n",
        "#         f.write(f\"Frequency Weighted IoU: {FWIoU}\\n\")\n",
        "\n",
        "#     print(f\"Evaluation results saved to: {filename}\")\n",
        "# Use code with caution\n",
        "# Explanation\n",
        "\n",
        "# Initialization:\n",
        "# Initializes the Evaluator object with the given num_classes.\n",
        "# Sets the model to evaluation mode using model.eval().\n",
        "# Iteration:\n",
        "# Iterates through the dataloader to get batches of images and labels.\n",
        "# Makes predictions using the model.\n",
        "# Updates the confusion matrix of the Evaluator using add_batch.\n",
        "# Evaluation Metrics:\n",
        "# Calls the respective methods of the Evaluator to calculate the accuracy metrics (Accuracy, Accuracy Class, Mean IoU, Frequency Weighted IoU).\n",
        "# Output:\n",
        "# Prints the calculated accuracy metrics to the console.\n",
        "# Saves the results to a text file with the provided filename.\n",
        "# How to use\n",
        "\n",
        "# Import: Import the Evaluator class and the do_accuracy_evaluation function into your script.\n",
        "# Load the Model: Load your trained PyTorch model.\n",
        "# Create DataLoader: Create a DataLoader for your evaluation dataset.\n",
        "# Call: Call the do_accuracy_evaluation function, providing the model, dataloader, number of classes, and desired filename for saving the results.\n",
        "\n",
        "# # Example usage\n",
        "# do_accuracy_evaluation(model, val_loader, num_classes=2, filename=\"evaluation_results.txt\")\n",
        "# Use code with caution\n",
        "# Replace val_loader with your evaluation dataloader and adjust the number of classes and filename as needed.\n",
        "\n",
        "# Sources\n",
        "# Rate this answer\n",
        "# now give me code to run that accuracy evalutation for both models I ran\n",
        "\n",
        "# Okay, here's the code to run the accuracy evaluation for both of your models:\n",
        "\n",
        "\n",
        "# import torch\n",
        "# from pathlib import Path\n",
        "\n",
        "# # ... (your previous code including model definitions, dataloaders, etc.) ...\n",
        "\n",
        "# # Define paths for loading and saving model results\n",
        "# src_dir = \"/content/gdrive/MyDrive/adleo/data/\"  # Replace with your actual src_dir\n",
        "# results_folder1 = Path(src_dir) / \"results\"\n",
        "# results_folder2 = Path(src_dir) / \"model2results\"\n",
        "# model_save_path1 = results_folder1 / \"model_weights.pth\"\n",
        "# model_save_path2 = results_folder2 / \"model_weights.pth\"\n",
        "\n",
        "# # Assuming you have the following:\n",
        "# # - UNet class definition\n",
        "# # - BalancedCrossEntropyLoss and TverskyFocalLoss class definitions\n",
        "# # - train_loader and val_loader (your dataloaders)\n",
        "\n",
        "# # Define model parameters\n",
        "# n_classes = 2\n",
        "# in_channels = 6\n",
        "# filter_config = [64, 128, 256, 512, 1024]\n",
        "# dropout_rate = 0.5\n",
        "\n",
        "# # Create and load Model 1\n",
        "# model1 = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# model1.load_state_dict(torch.load(model_save_path1))\n",
        "\n",
        "# # Create and load Model 2\n",
        "# model2 = UNet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "# model2.load_state_dict(torch.load(model_save_path2))\n",
        "\n",
        "# # Evaluate Model 1\n",
        "# do_accuracy_evaluation(model1, val_loader, num_classes=n_classes, filename=\"evaluation_results_model1.txt\")\n",
        "\n",
        "# # Evaluate Model 2\n",
        "# do_accuracy_evaluation(model2, val_loader, num_classes=n_classes, filename=\"evaluation_results_model2.txt\")\n",
        "# Use code with caution\n",
        "# Explanation\n",
        "\n",
        "# Load Saved Models:\n",
        "# Loads the saved state dictionaries (weights) for both models using torch.load and model.load_state_dict.\n",
        "# Make sure the paths model_save_path1 and model_save_path2 are correct for your saved models.\n",
        "# Evaluate Each Model:\n",
        "# Calls the do_accuracy_evaluation function for each model separately, passing the model, validation dataloader, number of classes, and a unique filename for saving the results.\n",
        "# The evaluation results for each model will be printed to the console and saved to separate text files (\"evaluation_results_model1.txt\" and \"evaluation_results_model2.txt\").\n",
        "# Before running:\n",
        "\n",
        "# Define Paths: Make sure to replace the placeholder src_dir with the actual path to your data directory where your model results are saved.\n",
        "# Model Parameters: Ensure that the n_classes, in_channels, filter_config, and dropout_rate are consistent with the parameters used to train your models.\n",
        "# Dataloaders: Verify that val_loader is your validation dataloader.\n",
        "# After running this code, you will have the accuracy evaluation results for both models saved in separate files, allowing you to compare their performance."
      ],
      "metadata": {
        "id": "oH3_cZ5hxIu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vz2OmIrBlceE",
        "X4Zo8L59l6y7",
        "nP5vII5nYYOg",
        "o68VvDTFYYOr",
        "HvQWD_wTs6lT",
        "niW-SEDiYYOs",
        "AdTI0FZ-yL_H"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}